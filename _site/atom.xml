<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Pavan Donthireddy</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2023-04-05T11:14:20+01:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Pavan Donthireddy</name>
   <email>quarterpastix@gmail.com</email>
 </author>

 
 <entry>
   <title>Sparse representation of signals</title>
   <link href="http://localhost:4000/2017/04/11/sparse-representation"/>
   <updated>2017-04-11T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/11/sparse-representation</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;Sparse representation is widely employed for expressing signals using very few linear combinations of elementary signals. These elementary signals are called atoms. Since the
number of the atoms is more than the dimension of the signal
space, any signal can be represented by linear combinations
of these atoms and the representations are not unique.&lt;/p&gt;

&lt;p&gt;Sparse representation is to use the minimum number of atoms to express the signals and this is actually an $L_{0}$ norm optimization problem. That is for a given overcomplete dictionary $A\in\Re^{N \times M}$ and a signal $x\in R^{N \times 1}$, where $N&amp;lt;M$ and $R^{a \times b}$ denotes the space of $a \times b$ real valued matrices, the representation problem is to find $z\in\Re^{M \times 1}$ such that $x=Az$ and $\lVert z\lVert_{0}$ is minimized. That is 
\(z_{0}^{\ast} = \mathrm{argmin}_{z} \lVert z\lVert_{0} \text{  subject to  } x=Az \tag{1}\)&lt;/p&gt;

&lt;p&gt;Here $\lVert z\lVert_{0}$ denotes $L_{0}$ norm of $z$, which is equivalent to the number of nonzero elements in $z$.&lt;/p&gt;

&lt;p&gt;The problem defined in (1) is nonconvex, nonsmooth and NP hard, it requires an exhaustive search for finding the solution. An approximate solution can be obrtained by solving the corresponding $L_{1}$ norm optimization problem if the isometry condition is satisfied. The $L_{1}$ norm optimization is as follows&lt;/p&gt;

\[z_{1}^{\ast} = \mathrm{argmin}_{z} \lVert z\lVert_{1} \text{  subject to  } x=Az \tag{2}\]

&lt;p&gt;Although $z_{1}^{\ast}$ is a good approximation of $z_{0}^{\ast}$ when the isometry condition is satisifed, these two solutions will be very different if $x$ contains significant amount of noise. Nevertheless, this is the typical case in practical circumstances. Hence the exact equality constraint is usually related to an inequality constraint as follows.&lt;/p&gt;

\[z^{\ast} = \mathrm{argmin}_{z} \lVert z\lVert_{1} \text{  subject to  } \lVert Az-x\lVert_{\infty} \le \epsilon \tag{3}\]

&lt;p&gt;where $\epsilon$ is the specification on the maximium absolute difference between $Az$ and $x$. This problem can be efficiently solved via reformulating the $L_{1}$ norm optimization problem to a linear programming problem.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Single Spectrum Analysis</title>
   <link href="http://localhost:4000/2017/04/10/ssa"/>
   <updated>2017-04-10T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/10/SSA</id>
   <content type="html">&lt;p&gt;The main steps of SSA can be summarized as follows:&lt;/p&gt;

&lt;p&gt;For a time series $x(n)$ for $n=1,2\dots N$, let the window length be $L$ , where  $1&amp;lt;L&amp;lt;N$. The first step of SSA is to construct a trajectory matrix as follows. Define the $L$ dimensional vectors as&lt;/p&gt;

\[X_{n}=\begin{bmatrix} x(n)\\ \vdots \\ x(n+L-1)
\end{bmatrix}\]

&lt;p&gt;for $n =1,2,\dots, N − L +1$.&lt;/p&gt;

&lt;p&gt;Denote $K = N − L +1$. These $K$ vectors are put into a matrix and the $L \times K$ trajectory matrix is constructed as follows:&lt;/p&gt;

\[X = \begin{bmatrix}
X_{1} &amp;amp; X_{2} &amp;amp; \dots &amp;amp; X_{K}
\end{bmatrix}\]

&lt;p&gt;The second step is to express $X$ as the sum of component matrices. Let $S=XX^T$ and the eigenvalues of $S$ be $\lambda_{1}\ge\lambda_{2}\dots\ge\lambda_{L}\ge 0$. Define $D=\max{j:\lambda_{j}&amp;gt;0}$. Let $U_{1},\dots,U_{D}$ be the corresponding eigenvectors.&lt;/p&gt;

&lt;p&gt;Denote $V_{j}=\frac{X^TU_{j}}{\sqrt{ \lambda_{j} }}$ for $j=1,2,\dots,D$  be the factor vectors.&lt;/p&gt;

&lt;p&gt;Define:&lt;/p&gt;

\[\tilde{X}_{j} =\sqrt{\lambda_{j}}U_{j}V_{j}^T\]

&lt;p&gt;for $j=1,2,\dots,D$. It can be shown that $X$ can be represented as&lt;/p&gt;

\[X = \tilde{X}_{1}+\dots+\tilde{X}_{D}\]

&lt;p&gt;The third step is to represent $X$ as the sum of grouped matrix components as follows. The indices set ${1,\dots,D}$ is partitioned into $M$ disjoint subsets $I_{1}\dots I_{M}$. Let $I_{m} ={i_{m_{1}},\dots,{i_{m_{c}}}}$ for $m=1,\dots, M$ and&lt;/p&gt;

\[\tilde{X}_{I_{m}}=\tilde{X}_{i_{m_{1}}}+\dots+\tilde{X}_{i_{m_{C}}}\]

&lt;p&gt;Hence we have&lt;/p&gt;

\[X=\tilde{X}_{I_{1}}+\dots+\tilde{X}_{I_{M}}\]

&lt;p&gt;The final step is to reconstruct the signal by the diagonal averaging method.&lt;/p&gt;

&lt;p&gt;First, transform  new text $\tilde{X}_{I_m}$ into new one dimensional signals of length $N$ by the hankelization like procedure.&lt;/p&gt;

&lt;p&gt;The vectors and the transform operator are denoted as $\tilde{x}_{I_m}$  for $m=1,\dots,M$ and  $\Im(.)$ respectievely. That is&lt;/p&gt;

\[\tilde{x}_{I_{m}} = \Im(\tilde{X}_{I_{m}}) ; m=1,\dots,M\]

&lt;p&gt;Thus the original time series can be expressed as a sum of $M$ series,&lt;/p&gt;

\[x(n)=\tilde{x}_{I_{1}}(n)+\dots+\tilde{x}_{I_{M}}(n) ; n=1,\dots,N\]
</content>
 </entry>
 
 <entry>
   <title>Trend extraction with SSA and Sparse Binary Programming</title>
   <link href="http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming"/>
   <updated>2017-04-09T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;The underlying trend is approximated by the sum of a part of &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/10/ssa&quot;&gt;SSA&lt;/a&gt; components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.&lt;/p&gt;

&lt;p&gt;As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem can
be approximated by a linear programming problem when the
isometry condition is satisfied, and the solution of the linear
programming problem can be obtained via existing simplex
methods or interior point methods.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;By applying the binary
quantization to the obtained solution of the linear programming
problem, the approximated solution of the original sparse
binary programming problem is obtained. Unlike previously
reported techniques that require a pre-cursor model or
parameter specifications, the proposed method is completely
adaptive.&lt;/p&gt;

&lt;h3 id=&quot;details&quot;&gt;Details&lt;/h3&gt;

&lt;p align=&quot;justify&quot;&gt;The conventional approach for selecting SSA
components for extracting the underlying trend is to employ
only the first several SSA components. However, this
selection rule fails when the underlying trend of a signal has a
complicated structure such as a high order polynomial
structure which cannot be characterized by only the first
several SSA components.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The idea is to formulate the
selection problem as a sparse binary programming problem and proposes an efficient methodology for approximating
the solution of the problem. In particular, the selection
problem is formulated as follows. The number of the
components to be selected is minimized subject to a
specification on the maximum absolute difference between
the approximated underlying trend and the original signal as
well as the binary valued constraint on the selection
coefficients. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Since the sparse binary programming problem is
nonsmooth, nonconvex and NP hard, it requires an exhaustive
search for finding the solution. As a result, the computational
effort for finding the solution is very large and an efficient
algorithm for approximating the solution is very useful and
important. &lt;/p&gt;

&lt;p&gt;To address these issues, the corresponding continuous valued optimization problem (the same
optimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective function
subject to an $L_{\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.&lt;/p&gt;

&lt;p&gt;By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem is
obtained.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;SSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.&lt;/p&gt;

&lt;p&gt;Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.&lt;/p&gt;

&lt;p&gt;However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.&lt;/p&gt;

&lt;p&gt;In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.&lt;/p&gt;

&lt;p&gt;The selection problem is formulated as the following sparse sparse binary programming problem.&lt;/p&gt;

\[z^{*}= \begin{bmatrix}
z_{1}^{*}&amp;amp; \dots, z_{D}^*
\end{bmatrix}^{T}= \mathrm{argmin}_{z} \lVert z\lVert_{0}\]

&lt;p&gt;subject to $\lVert Az-x\lVert_{\infty} \le \epsilon$ and $z_{i}\in{0,1}$ for $i=1,\dots,D$.&lt;/p&gt;

&lt;p&gt;Here&lt;/p&gt;

\[A=[\tilde{x}_1,\dots,\tilde{x}_D]\in R^{N \times D}\]

&lt;p&gt;and&lt;/p&gt;

\[\epsilon=0.5\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\]

&lt;p&gt;where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.&lt;/p&gt;

&lt;p&gt;If $z_{i}^{\ast}=1$ (or $z_{i}^{\ast}=0$), then the corresponding component $\tilde{x}_i$ for $i=1,\dots,D$ is selected (or excluded) for the representation of the underlying trend.&lt;/p&gt;

&lt;p&gt;Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.&lt;/p&gt;

&lt;p&gt;In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimization
problem is considered first. The solution of the $L_{0}$ norm
continuous valued optimization problem is approximated by
the solution of the corresponding $L_{1}$ norm continuous valued
optimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:&lt;/p&gt;

\[y^{*}= [y_{1}^{*},\dots,y_{D}^{*}]= \mathrm{arg}\min_{y}\lVert y\lVert_{1} \text{  subject to } \lVert Ay-x\lVert_{\infty}\le \epsilon\]

&lt;p&gt;By further applying the quantization on $y_{i}^{*}$ for $i=1,\dots,D$ to either $0$ or $1$ via the following operator.&lt;/p&gt;

\[W_{i}^{*}=
\begin{cases}
1 &amp;amp;  y_{i}^{*}\ge 0.5\\
0 &amp;amp;  y_{i}^{*}&amp;lt; 0.5
\end{cases}\]

&lt;p&gt;the corresponding component $\tilde{x}_{i}$&lt;/p&gt;

&lt;p&gt;for is selected (excluded) for the representation of the underlying trend if&lt;/p&gt;

&lt;p&gt;$W_{i}^{\ast}=1( \text{ or } W_{i}^{\ast}=0)$&lt;/p&gt;

&lt;p&gt;Finally the underlying trend of the signal is obtained by&lt;/p&gt;

\[\Gamma = AW^*\]

&lt;p&gt;where $W^{\ast}= [W_1^{\ast},\dots,W_D^{\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/11/sparse-representation&quot;&gt;sparse binary programming problem&lt;/a&gt;. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programming
problem. That is, $W^{\ast}$ in is very close to $z^{\ast}$ above.&lt;/p&gt;

&lt;p&gt;Therefore, the subset $I_{1}$ can be obtained by&lt;/p&gt;

\[I_{1} = \{i\vert W_{i}^{*}= 1, 1\le i 
\le D\}\]

&lt;p&gt;After obtaining the underlying trend $\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.&lt;/p&gt;

&lt;p&gt;Denote $\Gamma =[\gamma(1), \dots, \gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume that
there is an underlying structure in the time series and this
structure is preserved for the time period to be predicted. A
prediction model based on the linear recurrent formulae (LRF)
is employed.&lt;/p&gt;

&lt;p&gt;That is, the points $\gamma(N-L+2), \dots, \gamma(N)$ are employed for the prediction of $\gamma(N+1)$, and so on. In other words,&lt;/p&gt;

\[\gamma(n+1) = \sum_{k=0}^{L-2}a_{k}\gamma(n-k) \text{   for  } n\ge N,\]

&lt;p&gt;where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \dots, a_{0})^T$ is given by&lt;/p&gt;

\[R = \frac{1}{1-\nu^{2}}\sum_{i\in I_{1}}\pi_{i}U_{i}^{\nabla}\]

&lt;p&gt;Here, $\nu^{2=}\sum_{i\in I_{1}}\pi_{i}^2$, $\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\nabla }\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \in I_{1}.$&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ICA with CDWT</title>
   <link href="http://localhost:4000/2017/04/08/ica-with-cdwt"/>
   <updated>2017-04-08T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/08/ICA-with-CDWT</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;It is well known that if the original sounds are mixed in
the real environment (in the time domain) then the observed
sounds are a convolution mixture between the original
sounds with a delay and a reverberation. In order to simplify
this convolution mixture, it is a good idea to convert the
signal from the time domain into the time–frequency
domain and transform the convolution mixture into the
linear mixture by a time–frequency analysis method. By
doing this the drawback of poor performance with unsteady
sounds of the ICA also can be improved.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The time-frequency analysis method is usually a
combination of the &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/06/blind-source-seperation-ica&quot;&gt;ICA&lt;/a&gt;, the Short Time Fourier Transform
(STFT) and the Discrete Wavelet Transform (DWT).&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The STFT is probably the most common approach for
time–frequency analysis. It subdivides the signal into short
time segments (it is the same as using a small window to
divide the signal), and a discrete Fourier transform is
computed for each of these. For each frequency component,
however, the window length is fixed. So it is impossible to
choose an optimal window for each frequency component,
that is, the short time Fourier transform is unable to obtain
optimal analysis results for individual frequency
components.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;On the other hand, the DWT that was
carried out by Mattal’s fast algorithm also has a drawback
of lacking shift invariance although it can solve the problem
of the window width and obtain optimal frequency
resolution for each frequency component. Fortunately,
in order to improve the fault, a Complex Discrete Wavelet
Transform (CDWT) was proposed and it has been applied
widely to signal and image analysis&lt;/p&gt;

&lt;h2 id=&quot;ica-and-cdwt-for-blind-source-seperation&quot;&gt;ICA and CDWT for blind source seperation&lt;/h2&gt;

&lt;p&gt;In this method, the signals first were transformed from the time–domain into the
time–frequency domain by using the CDWT and then the ICA was carried out in the time–frequency domain. As in traditional methods, such as the STFT + ICA and the DWT + ICA, the following two problems when the ICA processing was carry out in the time–frequency domain
occurred. 
	1. Scaling problem: the signal’s amplitude and phase obtained by the ICA was changed, 
	2. Permutation problem: the separated signals are replaced at every frequency level mutually.&lt;/p&gt;

&lt;p&gt;This method discuss the technique for solving the scaling and the permutation problems. Finally, the separated signals are obtained by the inverse CDWT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/snips/ica_cdwt.png&quot; alt=&quot;ICA_CDWT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the case of two sound sources and two mikes has been considered for simplicity. First of all,
the two sound signals $x_n(t) (n=1,2,$ where $n$ is the number of channels and $t$ is the time) were observed by mikes from two sound sources $s_{i}(t) ( i =1, 2)$. The relation between the observed signal $x_n(t)$ and the sound source $s_{i}(t)$ is as follows:&lt;/p&gt;

\[x_{n}(t) = \mathbf{A_{n,i}}(t) \ast  S_{i}(t) \tag{1}\]

&lt;p&gt;where $\ast$ denotes convolution, the $\mathbf{A_{n,i}}(t)$ , is the impulse response that is from the sound sources to the mikes and can be shown as the following equation:&lt;/p&gt;

\[\mathbf{A_{n,i}}(t) = \begin{bmatrix} 
a_{11}(t) &amp;amp; a_{12}(t)\\ a_{21}(t) &amp;amp; a_{22}(t)\\
\end{bmatrix}, n,i = 1,2\]

&lt;p&gt;If one transforms the signal $x_{n}(t)$ from the time domain to the time–frequency domain by the CDWT then the (1) can be expressed as (2), in which the convolution of the sound source and the impulse response was changed to simple multiplication.&lt;/p&gt;

\[x_{n}(\omega , T) = \mathbf{A_{n,i}}(\omega) S_{i}(\omega,T) \tag{2}\]

&lt;p&gt;where, $\omega$ is the frequency and $T$ the time in the time–frequency domain.&lt;/p&gt;

&lt;p&gt;Next, whitening of the observed signal was carried out as follows:&lt;/p&gt;

\[\hat{x}_{n}(\omega , T) = Q(\omega) x_{n}(\omega,T) \tag{3}\]

&lt;p&gt;where $\hat{x}_{n}(\omega ,T)$&lt;/p&gt;

&lt;p&gt;is a whitened signal matrix and $Q(\omega)$ a whitened mixture, which can be obtained from the observed mixture signal $x_{n}(\omega , T)$ in each frequency.&lt;/p&gt;

&lt;p&gt;Finally, the ICA was carried out by using the whitened signal matrix $\hat{x}_{n}(\omega , T)$&lt;/p&gt;

&lt;p&gt;, in which the separation matrix $W(\omega)$ can be presumed. As a result, the separated signal $u_{i}(\omega , T)$ shown in (4) can be obtained.&lt;/p&gt;

\[u_{i}(\omega , T) = W(\omega) \hat{x}_{n}(\omega,T) \tag{4}\]

&lt;p&gt;The convolution mixture signal $x_{n}(t)$  shown in (1) can be transformed into the linear mixture signal
$x_{n}(\omega, T)$ shown in (2) by using the CDWT which simplifies the preprocessing of the ICA. A complex wavelet like, RI-Daubechies 6 wavelet can be applied as the mother wavelet.&lt;/p&gt;

&lt;h2 id=&quot;problem-and-correction-rule-of-ica-processing&quot;&gt;Problem and correction rule of ICA processing&lt;/h2&gt;

&lt;p&gt;The ICA is a technique for presuming the sound source $s_{i}(\omega,T)$  and the inverse matrix of $A_{ni}(\omega)$ from statistics without any former information of the observed signal. In this case, the amplitude of the separated signal $u_{i}(\omega,T)$ is a constant times the amplitude of the sound source $s_{i}(\omega,T)$ and a correction is needed.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;In this method, a similar amplitude change at each frequency level also occurred in the preprocessing by CDWT introduced. This phenomenon is called a scaling irregularity. Furthermore,
same as in the traditional method, it is also possible that the separated sound is replaced with noise at every frequency
level mutually. This phenomenon is called the permutation
problem. Therefore, after these two problems are solved, the
inverse complex discrete wavelet transform is performed,
and it is necessary to restore the sound signal observed with
each mike.&lt;/p&gt;

&lt;h3 id=&quot;scaling-problem-and-correction-rule&quot;&gt;Scaling problem and correction rule&lt;/h3&gt;

&lt;p&gt;Not only the amplitude of the restored signal but the phase also differs depending on the frequency by making for signal whitening (making no correlation). To take an arbitrary complex value by processing, this is caused. In order to solve the scaling irregularity problem, a method has
been proposed by Murata that uses the independent element $u_{i}(\omega,T)$ , which is obtained at each frequency and divides the spectrum. In this study, the method of correcting scaling is adopted from the divided spectrum and can be shown as follows:&lt;/p&gt;

\[v_{1}(\omega,T)=\begin{bmatrix}
v_{11}(\omega,T)\\ v_{12}(\omega,T)
\end{bmatrix} = (W(\omega)Q(\omega))^{-1}\begin{bmatrix}
u_{1}(\omega,T) \\
0
\end{bmatrix}\]

\[v_{2}(\omega,T)=\begin{bmatrix}
v_{21}(\omega,T)\\ v_{22}(\omega,T)
\end{bmatrix} = (W(\omega)Q(\omega))^{-1}\begin{bmatrix} 0 \\
u_{2}(\omega,T) 
\end{bmatrix}\]

&lt;p&gt;where $v_{11}(\omega,T),v_{22}(\omega,T)$ are divided spectrums. If the sum $v_{1}(\omega,T)+v_{2}(\omega,T)$ is calculated then the sum $v_{11}(\omega,T)+v_{21}(\omega,T)$ is the mixture signal $x_{1}(\omega,T)$ and the sum $v_{11}(\omega,T)+v_{22}(\omega,T)$ is the mixture signal $x_{2}(\omega,T)$&lt;/p&gt;

&lt;h3 id=&quot;permutation-problem-and-correction-rule&quot;&gt;Permutation problem and correction rule&lt;/h3&gt;

&lt;p align=&quot;justify&quot;&gt;The complex Fast-ICA performs separation based on
non-Gaussian characteristics. Therefore, the possibility of
the separating signal with higher non-Gaussian
characteristics being output as the first channel is very high.
However, the height of the frequency is not determined and
noise with low non-Gaussian characteristics might also be
output. &lt;/p&gt;

&lt;p&gt;Therefore, there is a possibility that the output of each frequency level is separated without being united by the sound. The separated signal $u_{1}(\omega,T), u_{2}(\omega,T)$ of the 1st and 2nd channel without permutation is shown as follows.&lt;/p&gt;

\[u_{1}(\omega ,k) \approx s_{1}(\omega,T) \tag{5} ; u_{2}(\omega ,k) \approx s_{2}(\omega,T)\]

&lt;p&gt;The above expression can be shown as the following equation by using the divided spectrum of the scaling correction.&lt;/p&gt;

\[\begin{bmatrix}
v_{11}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{11}(\omega,T)u_{1}(\omega,T) \\
g_{21}(\omega,T)u_{1}(\omega,T)
\end{bmatrix}\]

\[\begin{bmatrix}
v_{21}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{12}(\omega,T)u_{2}(\omega,T) \\
g_{22}(\omega,T)u_{2}(\omega,T)
\end{bmatrix}\]

&lt;p&gt;On the other hand, when permutation occurs, (5) becomes the next expression&lt;/p&gt;

\[u_{1}(\omega ,k) \approx s_{2}(\omega,T) \tag{6} ; u_{2}(\omega ,k) \approx s_{1}(\omega,T)\]

\[\begin{bmatrix}
v_{11}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{12}(\omega,T)u_{2}(\omega,T) \\
g_{22}(\omega,T)u_{2}(\omega,T)
\end{bmatrix}\]

\[\begin{bmatrix}
v_{21}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{11}(\omega,T)u_{1}(\omega,T) \\
g_{21}(\omega,T)u_{1}(\omega,T)
\end{bmatrix}\]

&lt;p&gt;From these, we can know that the divided spectrum can be shown as a multiplication of the separated signal $u_{i}(\omega,T)$ and the transfer function $g_{ni}(\omega)$ , which is from the sound source to the mike.&lt;/p&gt;

&lt;p&gt;The separation matrix $W(\omega)$ and the whitening matrix $Q(\omega)$ presumed by the ICA processing are used and the next equation can be obtained.&lt;/p&gt;

\[D = (W(\omega)Q(\omega))^{-1} = \begin{bmatrix}
g_{11}(\omega) &amp;amp; g_{12}(\omega) \\
g_{21}(\omega) &amp;amp; g_{22}(\omega)
\end{bmatrix} = e\begin{bmatrix}
a_{11}(\omega) &amp;amp; a_{12}(\omega) \\
a_{21}(\omega) &amp;amp; a_{22}(\omega)
\end{bmatrix} = eA_{ni}(\omega), e \in \mathbb{R}\]

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;CDWT +Complex Value Fast ICA outperforms, STFT+ Complex value Fast ICA and DWT+Real Value Fast ICA&lt;/p&gt;

&lt;p&gt;Reference : BLIND SOURCE SEPARATION BY COMBINING INDEPANDENT COMPONENT ANALYSIS WITH COMPLEX DISCRETE WAVELET TRANSFORM ZHONG ZHANG , TAKESHI ENOMOTO , TETSUO MIYAKE , TAKASHI IMAMURA&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Piecewise linear representation of time series</title>
   <link href="http://localhost:4000/2017/04/07/piecewise-linear-representation"/>
   <updated>2017-04-07T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/07/piecewise-linear-representation</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;The time series data usually fluctuate frequently and exit a lot of noise. So data mining in the original sequence data directly will not only cost highly in the storage and computation, but also probably affect the accuracy and reliability of the data mining algorithms. Therefore, many time series models are proposed, which can transform original series to new series. Modeling may not only compress the data, but also keep the main form and ignore fine changes. Accordingly, it can help improve the efficiency and accuracy of the data mining algorithms, which will provide policy support for data analysts.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation Based on Important Point:&lt;/b&gt;&lt;/u&gt; Pratt and Fink proposed a piecewise linear representation based on the important points. The important points are defined as the points which are the extreme points within the local scope and the ratio of the important point and the endpoint exceeds the parameters $R$. After extracting the important points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series and get various piecewise linear representation with different fine and granularity by selecting different parameters  $R$.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Aggregate Approximation (PAA):&lt;/b&gt;&lt;/u&gt; Keogh and Yi proposed the method of the piecewise aggregate approximation independently. The algorithm divides the time series by the same time width and each sub-segment is represent by the average of the sub-segment. The method is simple, intuitionistic. It not only can support the similarity queries, all the Minkowski metric and the weighted Euclidean distance, but also can be used to index to improve query efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation based on the characteristic points:&lt;/b&gt;&lt;/u&gt; Xiao  proposed a method of piecewise linear representation based on the characteristic points. After extracting the characteristic points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation Based on Slope Extract Edge Point (SEEP)&lt;/b&gt;&lt;/u&gt;:** ZHAN Yan-Yan brought forward a new piecewise linear representation combining slope with the characteristics of time series. The algorithm can select some change points according to the rate of slope change firstly, and then combines the points with the line sequentially. Finally it will generate a new time series.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The literatures above are analyzed as follows:&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;The piecewise linear representation gets some characteristics (e.g., extreme point, trend, etc.) of each section by segmenting the series mainly. The above methods not only have the advantages of simple and intuitive, but also can support dynamic incremental updates, clustering, fast similarity search, and so on. But the cost and fitting error is different.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation of Time Series based on Slope Change Threshold (SCT):&lt;/b&gt;&lt;/u&gt;
Firstly, the algorithm calculates the two segments’ slope of the certain point connecting with the
two adjacent points(except the two endpoints of time series).Secondly, it determines the change points by the ratio of slope. And then it combines the points with the line orderly. In this way, a new time series arises.&lt;/p&gt;

&lt;p&gt;The key of the algorithm is determining the change points. The change points must follow the
following principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first point and last point are both change point;&lt;/li&gt;
  &lt;li&gt;When the slope of the line combining the certain point with its left neighboring point is zero, we
look on the point as change point if the slope of the line combining the certain point with its right
neighboring point is out the range of$(-d,+d);$&lt;/li&gt;
  &lt;li&gt;When the slope of the line combining the certain point with its left neighboring point is not zero, we look on the point as change point if the slope ratio of two lines is beyond the range of $(1-d,1+d).$ The two lines refer to the line which combines the certain point with its right neighboring point and the line which combines the certain point with its left neighboring point. Above $d$ is a threshold parameter.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>ICA</title>
   <link href="http://localhost:4000/2017/04/06/blind-source-seperation-ica"/>
   <updated>2017-04-06T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/06/blind-source-seperation-ica</id>
   <content type="html">&lt;p&gt;The main idea can be briefly expressed by the following mixed model:
\(x(t) = \mathbf{A} s(t) + n(t)\)
The statistical model in the above equation is called ICA model, which describes how the observed data are mixed through the components $s(t)$ . The $m$ dimension column vector $x(t)$ is the observed data. $\mathbf{A}$ is a $m \times n$ mixing matrix; $n(t)$ denotes the additive noise vector. The matrix $\mathbf{A}$ is assumed to be unknown. All we observe is the random vector x(t) , and we must estimate both Aand s(t) . Since $\mathbf{A}$ is unknown, so $s(t)$ seems to be unsolvable.&lt;/p&gt;

&lt;p&gt;Fortunately, there are many mathematical methods for calculating the coefficients of $\mathbf{A}$ by requiring the High-Order Statistics (HOS) information during the search for independent
components.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Extreme Point Symmetric Mode Decomposition</title>
   <link href="http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition"/>
   <updated>2017-04-05T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;Considering the extraction methods of trend item, including the difference method, average slope method, moving average method, low pass filtering method, and least square fitting method, the type of trend term often needs to be presupposed. These methods are not suitable for processing the nonstationary signals with complex or random change trends.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;According to previous studies, the wavelet transform-based method is required for preselecting the wavelet basis and decomposition levels. This method is influenced easily by
artificial factors and has no self-adaptability. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The method based on Empirical mode decomposition (EMD) can adaptively decompose non-stationary signals regardless of the type of trend term. But, EMD is affected by mode mixing and
end effect, causing the decomposed trend function is rough and the extraction accuracy is restricted. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Professor Wang et al. recently proposed a self-adaptive method called ESMD. The ESMD is a novel development derived from Hilbert Huang transform that can be used to process non-stationary signal. ESMD has been applied to many studies.&lt;/p&gt;

&lt;h2 id=&quot;esmd&quot;&gt;ESMD&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Identify all local extreme points, including maxima and minima points, of the data $Y$. Mark them as $E_{i} (1 ≤ i ≤ n);$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Connect all adjacent Ei with line segments, and mark their midpoints as $F_{i} (1 ≤ i ≤ n-1);$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Add left and right boundary midpoints $F_{0}$ and $F_{n}$ using linear interpolation method;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Construct $p$ interpolating curves $L_{1}, L_{2},\dots L_{p} (p≥1)$ with all $n+1$ midpoints and calculate their mean value by using equation $L^{\ast}$.&lt;/li&gt;
&lt;/ul&gt;

\[L^{*}= \frac{L_{1}+L_{2}+\dots + L_{p}}{p}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Repeat steps $1$ to $4$ on $Y - L^{\ast}$ until $||L^{\ast}|| \le \epsilon$ ($\epsilon$ is a permitted error), or until the sifting times attain a preset maximum number $K$. Then, the first mode $M_{1}$ is obtained.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 6:&lt;/strong&gt; Repeat steps $1$ to $5$ on the residual $Y - M_1$ and obtain $M_2, M_3 \dots$ until the last residual $R$ with no more than a certain number of extreme points.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 7:&lt;/strong&gt; Change the maximum number $K$ on a finite integer interval $[K_{min}, K_{max}],$ and repeat all previous steps. Calculate the variance $\sigma^2$ of $Y - R$ and plot a figure with $\frac{\sigma}{\sigma_{0}}$ and $K$, $\sigma_{0}$ is the standard deviation of $Y$.&lt;/li&gt;
&lt;/ul&gt;

\[\sigma^2 = \frac{1}{N}\sum_{i=1}^N(y_{i}-r_{i})^2\]

\[\sigma_{0}^2 = \frac{1}{N}\sum_{i=1}^N(y_{i}-\bar{Y})^2\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Step 8 :&lt;/strong&gt; Identify the number $K_0$ which corresponds to the minimum  $\frac{\sigma}{\sigma_{0}}$  in $[K_{min}, K_{max}].$  Use this $K_{0}$ to repeat steps $1$ to $6$ and obtain the whole modes. Then last residual $R$ is an optimal Adaptive global mean (AGM) curve.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the steps above, ESMD can decompose signal into limited intrinsic mode functions and a residual component. The residual component $R$ is an optimal AGM curve that can be considered as the trend term of the original signal.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;The extraction error of EMD is larger than that of ESMD. &lt;/u&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The EMD-based extraction method can adaptively extract the signal trend. Whereas, the extracted trend curve limits the number of its extreme points (no more than 1), and no optimal strategy to find it. Therefore, the extraction error of EMD is relatively larger than that of ESMD.&lt;/li&gt;
  &lt;li&gt;The ESMD-based extraction method has commendable self-adaptability. It can obtain the signal trend with high precision using adaptive decomposition and optimization. The trend type of signal does not need to be preset. And, the extraction results of ESMD are better than that of EMD.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Reference: Adaptive extraction method for trend term of machinery signal based on
extreme-point symmetric mode decomposition - Yong Zhu, Wan-lu Jiang and Xiang-dong Kong&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>PCA ICA for 1D timeseries</title>
   <link href="http://localhost:4000/2017/04/03/pca-ica-1d-timeseries"/>
   <updated>2017-04-03T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/03/PCA-ICA-1d-timeseries</id>
   <content type="html">&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;p align=&quot;justify&quot;&gt;Principal component analysis (PCA) isa method that transforms multiple data series into uncorrelated data series. Independent component analysis &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/06/blind-source-seperation-ica&quot;&gt;ICA&lt;/a&gt; is a method that separates multiple data series into independent data series. However, both require signals from at least two separate sensors. To overcome this requirement and utilize the fault detection capability of ICA and PCA, we propose to use wavelet transform to pre-process the data collected from a single sensor and then use the coefficients of the wavelet transforms at different scales as input to ICA and PCA. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Independent components analysis (ICA) requires little prior knowledge about the components to be isolated; however, at least two sensors must be available for signal collection and the number of sensors must be at least equal to the number of sources to be separated and this method cannot be applied directly when there is only one sensor collecting signals.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Principal component analysis (PCA) is a multivariate data analysis technique that transforms a set of correlated variables into a set of uncorrelated variables. Each member of the resulting set of uncorrelated variables is called a principal component. We are interested in determining its suitability for fault detection because one of the identified principal components may reveal the signature of a hidden fault. As with ICA, however, this method cannot be applied directly when only a single variable is observed.&lt;/p&gt;

&lt;p&gt;Wavelet transform may be considered as a series of band pass filters when applied to the data
collected from a single sensor. The results of the transform, which exist in different frequency
regions, say $N$ regions, may be considered as different mixtures of the sources that have
generated the collected signals.These $N$ groups of data can then be used as input to ICA or PCA
for identification of the hidden sources.&lt;/p&gt;

&lt;h3 id=&quot;ica-and-pca&quot;&gt;ICA and PCA&lt;/h3&gt;

&lt;p&gt;ICA is a technique for separating independent sources linearly mixed in signals. Suppose that
there are $N$ independent sources of vibration, and $N$ sensors at different locations are used to
record vibration signals. The signals recorded by each sensor come from different sources with
different mixing ratios. Let $s_{1}(t),s_{2}(t),  \dots ,s_{N}(t)$ be the signals produced by the $N$ independent
sources and $x_{1}(t),x_{2}(t),  \dots ,x_{N}(t)$ be the observations from the $N$ sensors. The sensors record these signals simultaneously. The task of ICA is to estimate the mixing ratios of the source signals in the collected signals and obtain the independent source signals.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;To identify the independent components successfully, we need a rule for evaluating the
independency of the identified components. According to the Central Limit Theorem, the
distribution of the sum of a large number of independent random variables tends to a Gaussian
distribution. Since the collected signals are weighted sums of the independent sources, the sources
to be isolated must have less Gaussianity than the collected signals. Thus, non-Gaussianity can be
used for separating independent components. Hyvarinen and Oja proposed to use negentropy
to evaluate the non-Gaussianity of the separated components so as to evaluate separation
performance. With this concept, we can seek the separation that provides the least Gaussianness of the separated components. The popular FastICA algorithm proposed by Havarinen and Oja
isoften used to carry out the ICA procedure.&lt;/p&gt;

&lt;p&gt;PCA is a technique that obtains linear transformations of a group of correlated variables such
that the transformed variables are uncorrelated. For example, consider two variables, $x_{1}$ and
$x_{2}$. For each variable, we have obtained the following $N$ observations:
\(x_{11}, x_{12}, \dots , x_{1N}; x_{21}, x_{22}, \dots; x_{2N}\)
where $x_{1i}$ and $x_{2j}$ denote the $i^{th}$ and the $j^{th}$ observations of variables $x_{1}$ and $x_{2}$, respectively. The PCA method seeks two new axes, D1 and D2, that make the projectionsof the collected data onto
D1 have the largest variability and at the same time, the projections of the collected data onto D2
have the smallest variability. This way, we have expressed the collected data as their two principal
components. Most of the variation in the original data is explained by the first principal
component, D1, and the remaining variation in the original data isexplained by the second
principal component, D2.&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
ICA renders the separated components independent of one another while PCA rendersthe
separated components uncorrelated with one another. PCA separates the components based only
on the second-order cumulant while ICA separates the components on high-order cumulants.
Therefore, ICA can be considered a generalization of PCA.&lt;/p&gt;

&lt;h3 id=&quot;method-of-preprocessing-to-apply-ica-or-pca-on-1d-time-series&quot;&gt;Method of preprocessing to apply ICA or PCA on 1D Time series&lt;/h3&gt;

&lt;p&gt;The available data is a single time series. To apply ICA or PCA for feature extraction, we need
to have more than one time series. A method to generate multiple time series from the single available time series is given below.&lt;/p&gt;

&lt;p&gt;Wavelet transform decomposes a signal series in the time domain into a two-dimensional
function in the time-scale (frequency) plane. The wavelet coefficients measure the time-scale (frequency) content in a signal indexed by the scale parameter and the translation parameter. Let
$\varphi(t)$ be the mother wavelet. The wavelet family consists of a series of daughter wavelets that are
generated by dilation and translation from the mother wavelet $\varphi(t)$&lt;/p&gt;

\[\varphi_{a, b}(t)=\sqrt{|a|} \varphi[(t-b) / a]\]

&lt;p&gt;where $a$ is the scale parameter, $b$ isthe location parameter, and $\sqrt{  |a|}$
isused to guarantee energy preservation. The wavelet transform of signal $x(t)$ isdefined as the inner product of $\varphi_{a, b}(t)$ and $x(t)$ in the Hilbert space of $L^2$ norm defined as:&lt;/p&gt;

\[W(a, b)=\left\langle\varphi_{a, b}(t), x(t)\right\rangle=\int x(t) \varphi_{a, b}^*(t) \mathrm{d} t\]

&lt;p&gt;where the symbol * stands for the complex conjugate.&lt;/p&gt;

&lt;p&gt;Wavelet transform can be thought of as a series of band pass filters. The results of the
transform, which exist in different frequency regions, may be thought of as different mixtures of the independent sources. These different mixtures may be considered to be signals collected at
different ‘‘locations’’, or more accurately, through different ‘‘sensors’’ with different frequency
ranges. This way, the one-dimensional signal is transformed into multidimensional data
that satisfy the requirements of ICA and PCA. The preprocessing of the one-dimensional data with wavelet transform makes ICA and PCA usable for identification of a hidden
source.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/snips/img1.png&quot; alt=&quot;Flowchart&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference-&quot;&gt;Reference :&lt;/h3&gt;
&lt;p&gt;Feature separation using ICA for a one-dimensional time series and its application in fault detection - Ming J. Zuo, Jing Lin, Xianfeng Fan&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Latex cheatsheet</title>
   <link href="http://localhost:4000/2017/03/21/latex-cheat-sheet"/>
   <updated>2017-03-21T00:00:00+00:00</updated>
   <id>http://localhost:4000/2017/03/21/latex-cheat-sheet</id>
   <content type="html">&lt;h1 id=&quot;description&quot;&gt;Description&lt;/h1&gt;
&lt;p&gt;Cheatsheet for LaTex, using Markdown for markup. I use this with &lt;a href=&quot;https://atom.io/&quot;&gt;atom.io&lt;/a&gt;
and :package:&lt;code&gt;markdown-preview-plus&lt;/code&gt; to write math stuff. :package:&lt;code&gt;keyboard-localization&lt;/code&gt;
is necessary when using an international layout (like [swiss] german).&lt;/p&gt;

&lt;p&gt;$\mathrm{abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ23456}$&lt;/p&gt;

&lt;p&gt;$\text{abcdeABCDEF ASDFASDF} \alpha, \beta, \gamma$&lt;/p&gt;

\[E = mc^2\]

&lt;p&gt;$\text{This is a text for math} \Bigg(\frac{a}{b} \Bigg)$&lt;/p&gt;

&lt;p&gt;Further Reference and source: ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdf&lt;/p&gt;

&lt;h1 id=&quot;example-expressions--functions&quot;&gt;Example expressions / functions&lt;/h1&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Input&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Rendered&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code&gt;$a = b + c − d$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$a = b + c − d$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code&gt;$\sqrt{?\frac{\pi}{2}}$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$\sqrt{\frac{\pi}{2}}$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;code&gt;$y = a x_1^2 + b x_2 + c$&lt;/code&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$y = a x_1^2 + b x_2 + c$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;special-characters--symbols&quot;&gt;Special characters / Symbols&lt;/h1&gt;
&lt;p&gt;###Latin:
#####No dot:&lt;br /&gt;
&lt;code&gt;\imath&lt;/code&gt; $\rightarrow$ $\imath$,
&lt;code&gt;\jmath&lt;/code&gt; $\rightarrow$ $\jmath$&lt;/p&gt;

&lt;p&gt;#####Hat:&lt;br /&gt;
&lt;code&gt;\hat{\imath}&lt;/code&gt;  $\rightarrow$ $\hat{\imath}$,
&lt;code&gt;\hat{\jmath}&lt;/code&gt;  $\rightarrow$ $\hat{\jmath}$&lt;/p&gt;

&lt;p&gt;###Greek Letters:
#####Capital:
LaTex      |   | LaTex    |   |
———-:|–:|———:|–:|
&lt;code&gt;\Gamma&lt;/code&gt;   | Γ | &lt;code&gt;\Delta&lt;/code&gt; | ∆ |
&lt;code&gt;\Lambda&lt;/code&gt;  | Λ | &lt;code&gt;\Phi&lt;/code&gt;   | Φ |
&lt;code&gt;\Pi&lt;/code&gt;      | Π | &lt;code&gt;\Psi&lt;/code&gt;   | Ψ |
&lt;code&gt;\Sigma&lt;/code&gt;   | Σ | &lt;code&gt;\Theta&lt;/code&gt; | Θ |
&lt;code&gt;\Upsilon&lt;/code&gt; | Υ | &lt;code&gt;\Xi&lt;/code&gt;    | Ξ |
&lt;code&gt;\Omega&lt;/code&gt;   | Ω |          |   |&lt;/p&gt;

&lt;p&gt;#####Lowercase:
LaTex      |   | LaTex     |   |
———-:|–:|———-:|–:|
&lt;code&gt;\alpha&lt;/code&gt;   | α | &lt;code&gt;\nu&lt;/code&gt;     | ν |
&lt;code&gt;\beta&lt;/code&gt;    | β | &lt;code&gt;\kappa&lt;/code&gt;  | κ |
&lt;code&gt;\gamma&lt;/code&gt;   | γ | &lt;code&gt;\lambda&lt;/code&gt; | λ |
&lt;code&gt;\delta&lt;/code&gt;   | δ |  &lt;code&gt;\mu&lt;/code&gt;    | µ |  &lt;br /&gt;
&lt;code&gt;\epsilon&lt;/code&gt; | ϵ | &lt;code&gt;\zeta&lt;/code&gt;   | ζ |
&lt;code&gt;\eta&lt;/code&gt;     | η | &lt;code&gt;\theta&lt;/code&gt;  | θ |
&lt;code&gt;\iota&lt;/code&gt;    | ι | &lt;code&gt;\xi&lt;/code&gt;     | ξ |
&lt;code&gt;\pi&lt;/code&gt;      | π | &lt;code&gt;\rho&lt;/code&gt;    | ρ |
&lt;code&gt;\sigma&lt;/code&gt;   | σ | &lt;code&gt;\tau&lt;/code&gt;    | τ |
&lt;code&gt;\upsilon&lt;/code&gt; | υ | &lt;code&gt;\phi&lt;/code&gt;    | φ |
&lt;code&gt;\chi&lt;/code&gt;     | χ | &lt;code&gt;\psi&lt;/code&gt;    | ψ |
&lt;code&gt;\omega&lt;/code&gt;   | ω |           |   |&lt;/p&gt;

&lt;p&gt;#####Other:
LaTex       |   | LaTex       |   |
———–:|—|————:|–:|
&lt;code&gt;\digamma&lt;/code&gt;  | ϝ | &lt;code&gt;varepsilon&lt;/code&gt;| ε       |
&lt;code&gt;\varkappa&lt;/code&gt; | ϰ | &lt;code&gt;\varphi&lt;/code&gt;   | ϕ       |
&lt;code&gt;\varpi&lt;/code&gt;    | ϖ | &lt;code&gt;\varrho&lt;/code&gt;   | ϱ       |
&lt;code&gt;\varsigma&lt;/code&gt; | ς | &lt;code&gt;\vartheta&lt;/code&gt; | ϑ       |
&lt;code&gt;\eth&lt;/code&gt;      | ð | &lt;code&gt;\hbar&lt;/code&gt;     | $\hbar$ |&lt;/p&gt;

&lt;p&gt;###Other:
####Other Symbols
LaTex         |   | LaTex            |   |
————-:|—|—————–:|–:|
&lt;code&gt;\partial&lt;/code&gt;    | ∂ | &lt;code&gt;\infty&lt;/code&gt;         | ∞ |
&lt;code&gt;\wedge&lt;/code&gt;      | ∧ | &lt;code&gt;\vee&lt;/code&gt;           | ∨ |
&lt;code&gt;\neg&lt;/code&gt; &lt;code&gt;\not&lt;/code&gt; | ¬ |                  |   |
&lt;code&gt;\bot&lt;/code&gt;        | ⊥ | &lt;code&gt;\top&lt;/code&gt;           | ⊤ |
&lt;code&gt;\nabla&lt;/code&gt;      | ∇ | &lt;code&gt;\varnothing&lt;/code&gt;    | ∅ |
&lt;code&gt;\angle&lt;/code&gt;      | ∠ | &lt;code&gt;\measuredangle&lt;/code&gt; | ∡ |
&lt;code&gt;\surd&lt;/code&gt;       | √ | &lt;code&gt;\forall&lt;/code&gt;        | ∀ |
&lt;code&gt;\exists&lt;/code&gt;     | ∃ | &lt;code&gt;\nexists&lt;/code&gt;       | ∄ |&lt;/p&gt;

&lt;p&gt;####Relational Symbols
LaTex             |   | LaTex              |          |
—————–:|—|——————-:|———:|
&lt;code&gt;\hookrightarrow&lt;/code&gt; | ↪      | &lt;code&gt;\Rightarrow&lt;/code&gt;     | ⇒         |
&lt;code&gt;\rightarrow&lt;/code&gt;     | →      | &lt;code&gt;\Leftrightarrow&lt;/code&gt; | ⇔         |
&lt;code&gt;\nrightarrow&lt;/code&gt;    | ↛      | &lt;code&gt;\mapsto&lt;/code&gt;         | $\mapsto$ |
&lt;code&gt;\geq&lt;/code&gt;            | ≥      | &lt;code&gt;\leq&lt;/code&gt;            | ≤         |
&lt;code&gt;\equiv&lt;/code&gt;          | ≡      | &lt;code&gt;\sim&lt;/code&gt;            | ∼         |
&lt;code&gt;\gg&lt;/code&gt;             | ≫      | &lt;code&gt;\ll&lt;/code&gt;            | ≪          |
&lt;code&gt;\subset&lt;/code&gt;          | ⊂     | &lt;code&gt;\subseteq&lt;/code&gt;     | ⊆           |
&lt;code&gt;\in&lt;/code&gt;             | ∈      | &lt;code&gt;\notin&lt;/code&gt;         | ∉          |
&lt;code&gt;\mid&lt;/code&gt;            | $\mid$ | &lt;code&gt;\propto&lt;/code&gt;        | ∝          |
&lt;code&gt;\perp&lt;/code&gt;            | ⊥     | ` \parallel&lt;code&gt;     | ∥          |
&lt;/code&gt;\vartriangle`     | $\vartriangle$&lt;/p&gt;

&lt;p&gt;####Binary operators
LaTex        |   | LaTex  |   |
————:|—|——-:|–:|
&lt;code&gt;\wedge&lt;/code&gt;     | ∧ | &lt;code&gt;\vee&lt;/code&gt; | ∨ |
&lt;code&gt;\neg&lt;/code&gt;&lt;code&gt;\not&lt;/code&gt; | ¬ |        |   |&lt;/p&gt;

&lt;p&gt;####Cumulative operators
LaTex     |           | LaTex       |             |
———:|———–|————:|————:|
&lt;code&gt;\int&lt;/code&gt;    | ∫         | &lt;code&gt;\iint&lt;/code&gt;     | $\iint$     |
&lt;code&gt;\iiint&lt;/code&gt;  | $\iiint$  | &lt;code&gt;\idotsint&lt;/code&gt; | $\idotsint$ |
&lt;code&gt;\prod&lt;/code&gt;   | $\prod$   | &lt;code&gt;\sum&lt;/code&gt;      | $\sum$      |
&lt;code&gt;\bigcup&lt;/code&gt; | $\bigcup$ | &lt;code&gt;\bigcap&lt;/code&gt;   | $\bigcap$   |&lt;/p&gt;

&lt;p&gt;####Named operators
$\arccos$,
$\arcsin$,
$\arctan$,
$\arg$,
$\cos$,
$\cosh$,
$\cot$,
$\coth$,
$\deg$,
$\det$,
$\dim$,
$\exp$,
$\gcd$,
$\hom$,
$\inf$,
$\injlim$,
$\lg$,
$\lim$,
$\liminf$,
$\limsup$,
$\ln$,
$\log$,
$\max$,
$\min$,
$\Pr$,
$\projlim$,
$\sec$,
$\sin$,
$\sinh$,
$\sup$&lt;/p&gt;
</content>
 </entry>
 

</feed>
