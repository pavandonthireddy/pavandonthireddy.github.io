<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Pavan Donthireddy</title>
 <link href="http://localhost:4000/atom.xml" rel="self"/>
 <link href="http://localhost:4000/"/>
 <updated>2023-04-09T00:13:39+01:00</updated>
 <id>http://localhost:4000</id>
 <author>
   <name>Pavan Donthireddy</name>
   <email>quarterpastix@gmail.com</email>
 </author>

 
 <entry>
   <title>Optimal Finite Impulse Response (OFIR) Filter</title>
   <link href="http://localhost:4000/2017/04/13/ofir-filter"/>
   <updated>2017-04-13T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/13/ofir-filter</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;The Kalman filter (KF) is the
most widely used real-time optimal estimator. However, the KF is a
Bayesian estimator and its recursive algorithm has the infinite impulse
response (IIR), owing to which the KF often suffers of insufficient robustness. Better robustness is inherent to finite memory filters and to filters with finite impulse response (FIR).&lt;/p&gt;

&lt;p&gt;Unlike the KF, the FIR filter utilizes measurements on an interval of
N most recent neighbouring points called horizon. Compared to the KF,
FIR filters demonstrate many useful properties such as the bound input/
bound output (BIBO) stability, higher robustness against temporary
model uncertainties and round-off errors , and
lower sensitivity to noise.&lt;/p&gt;

&lt;h3 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h3&gt;

&lt;p&gt;Consider a general class of discrete-time linear systems represented in state-space with time-variant coefficients as&lt;/p&gt;

\[x_{k}=A_{k} x_{k-1}+B_{k} w_{k} \tag{1}\]

&lt;p&gt;\(y_{k}=C_{k} x_{k}+v_{k}\tag{2}\)
in which $k$ is the discrete time index, $x_{k} \in \mathbb{R}^{n}$ is the state vector, $y_{k} \in \mathbb{R}^{p}$ is the measurement vector, and $A_{k} \in \mathbb{R}^{n \times n}, B_{k} \in \mathbb{R}^{n \times u}$, and $C_{k} \in \mathbb{R}^{p \times n}$ are time-variant matrices. Here, $w_{k} \in \mathbb{R}^{u}$ and $v_{k} \in \mathbb{R}^{p}$ are additive process and measurement noise sources with known covariances&lt;/p&gt;

&lt;p&gt;$Q_k=\E ( w_k w_{k}^{T} ) $ and $R_{k}=\mathbb{E}( v_{k} v_{k}^{T})$, respectively. We suppose that $w_{k}$ and $v_{k}$ are zero mean, white, and mutually uncorrelated; that is, $\mathbb{E}( w_{k})=0$, $\mathbb{E}( v_{k})=0, \mathbb{E}(w_{k} w_{j}^{T})=0$ and $\mathbb{E}( v_{k} v_{j}^{T})=0$ for all $k$ and $j \neq k$, and $\mathbb{E}( w_{k} v_{j}^{T})=0$ for all $k$ and $j$.&lt;/p&gt;

&lt;p&gt;The FIR filter requires simultaneously $N$ data points taken from the horizon $[l=k-N+1, k]$. Therefore, (1) and (2) need to be extended on $[l, k]$. That can be done if to use the recursively computed forward in-time solutions and write&lt;/p&gt;

\[X_{k, l}=A_{k, l} x_{l}+B_{k, l} W_{k, l}\tag{3}\]

\[Y_{k, l}=C_{k, l} x_{l}+H_{k, l} W_{k, l}+V_{k, l} \tag{5}\]

&lt;p&gt;where the extended vectors are $X_{k, l}=\left[x_{k}^{T}, x_{k-1}^{T}, \ldots, x_{l}^{T}\right]^{T} \in \mathbb{R}^{N n \times 1}$, $Y_{k, l}=\left[y_{k}^{T}, y_{k-1}^{T}, \ldots, y_{l}^{T}\right]^{T} \in \mathbb{R}^{N p \times 1}, W_{k, l}=\left[w_{k}^{T}, w_{k-1}^{T}, \ldots, w_{l}^{T}\right]^{T} \in \mathbb{R}^{N u \times 1}$, and $V_{k, l}=\left[v_{k}^{T}, v_{k-1}^{T}, \ldots, v_{l}^{T}\right]^{T} \in \mathbb{R}^{N p \times 1}$. The extended $k$ - and $N$-variant matrices $A_{k, l} \in \mathbb{R}^{N n \times n}, B_{k, l} \in \mathbb{R}^{N n \times N u}, C_{k, l} \in \mathbb{R}^{N p \times n}$, and $H_{k, l} \in \mathbb{R}^{N p \times N u}$ can be represented as, respectively,&lt;/p&gt;

&lt;p&gt;\(A_{k, l}=\left[\mathscr{A}_{k, l+1}^{T}, \mathscr{A}_{k-1, l+1}^{T}, \ldots, \mathscr{A}_{l+1, l+1}^{T}, I\right]^{T}\)
\(B_{k, l}=\left[\begin{array}{ccccc}B_{k} &amp;amp; \mathscr{A}_{k, k} B_{k-1} &amp;amp; \cdots &amp;amp; \mathbf{A}_{k, l+2} B_{l+1} &amp;amp; \mathscr{A}_{k, l+1} B_{l} \\ 0 &amp;amp; B_{k-1} &amp;amp; \cdots &amp;amp; \mathscr{A}_{k-1, l+2} B_{l+1} &amp;amp; \mathscr{A}_{k-1, l+1} B_{l} \\ \vdots &amp;amp; \vdots &amp;amp; \cdots &amp;amp; \vdots &amp;amp; \vdots \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; B_{l+1} &amp;amp; \mathscr{A}_{l+1, l+1} B_{l} \\ 0 &amp;amp; 0 &amp;amp; \cdots &amp;amp; 0 &amp;amp; B_{l}\end{array}\right]\)
,&lt;/p&gt;

\[C_{k, l}=\bar{C}_{k, l} A_{k, l}\]

\[H_{k, l}=\bar{C}_{k, l} B_{k, l}\]

&lt;p&gt;where&lt;/p&gt;

\[\bar{C}_{k, l}=\operatorname{diag}\left(C_{k} C_{k-1} \cdots C_{l}\right)\]

\[\mathbf{A}_{i, j}=\prod_{r=0}^{i-j} A_{i-r}=A_{i} A_{i-1} \ldots A_{j}\]

&lt;p&gt;At the initial horizon point, (3) becomes $x_{l}=x_{l}+B_{l} w_{l}$ that is uniquely satisfied if $w_{l}$ is zero-valued, provided that $B_{l}$ is not zeroth. That means that the initial state must be known in advance or estimated optimally.&lt;/p&gt;

&lt;p&gt;The FIR filtering estimate can be obtained at $k$ via (4) using the discrete convolution as&lt;/p&gt;

\[\hat{x}_{k \mid k}=K_{k} Y_{k, l} \tag{5}\]

&lt;p&gt;where $x_{t \vert r} $ means the estimate at $t$ via measurements from the past to and including at $r$ and $K_{k}$ is the FIR filter gain, which needs to be defined to obey some cost function. Note that the aforementioned inherent properties of FIR filtering are associated with the fact that measurements prior to $l$ are discarded in (5) and thus do not affect the estimate unlike in the KF which has IIR. It is also necessary to emphasize that when the system considered is time-invariant, the FIR estimate (5) will becomes $x_{k \mid k}=K_{N} Y_{k, l}$, which means that the filter gain $K_{N}$ is time-invariant and can be determined off-line once the horizon length $N$ is available. In this case, $K_{N}$ is not necessarily to be realized into iterative computation structure.&lt;/p&gt;

&lt;p&gt;The optimal gain $K_{k}$ can be obtained for (5) in the minimum MSE sense by minimizing the trace of the MSE as&lt;/p&gt;

\[\hat{K}_{k}=\underset{K_{k}}{\arg \min } E\left\{\operatorname{tr}\left(e_{k} e_{k}^{T}\right)\right\}\]

&lt;p&gt;where $e_{k}=x_{k}-x_{k \mid k}$ is the estimation error. Provided $x_{k \mid k}$ via (5), the one-step prediction required by feedback control and associated with receding horizon filtering can be formed as $x_{k+1 \mid k}=A_{k+1} x_{k \mid k}$, similarly to the KF.&lt;/p&gt;

&lt;h3 id=&quot;ofir-algorithm&quot;&gt;OFIR algorithm&lt;/h3&gt;

&lt;p&gt;Given the model (1) and (2) with white and mutually uncorrelated noise processes $w_{k}$ and $v_{k}$ which have covariances $Q_{k}$ and $R_{k}$, respectively. The iterative form for OFIR estimate (10) with gain (16) is the following,&lt;/p&gt;

\[\begin{aligned}
\Xi_{i}= &amp;amp; A_{i} \Xi_{i-1} A_{i}^{T}+B_{i} Q_{i} B_{i}^{T}-A_{i} \Xi_{i-1} C_{i-1}^{T} \\
&amp;amp; \times\left(R_{i-1}+C_{i-1} \Xi_{i-1} C_{i-1}^{T}\right)^{-1} C_{i-1} \Xi_{i-1} A_{i}^{T} \\
G_{i}= &amp;amp; \Xi_{i} C_{i}^{T}\left(R_{i}+C_{i} \Xi_{i} C_{i}^{T}\right)^{-1}
\end{aligned} \tag{6}\]

\[\hat{x}_{i \mid i}=A_{i} \hat{x}_{i-1 \mid i-1}+G_{i}\left(y_{i}-C_{i} A_{i} \hat{x}_{i-1 \mid i-1}\right)\tag{7}\]

&lt;p&gt;where $i$ ranges from $l+1$ to $k$ and the output is taken when $i=k$. The initial state $x_{l \vert l}$ is given and the initial prior error $\Xi_{i}$ is provided at l by&lt;/p&gt;

&lt;p&gt;$\Xi_{l}=\Theta_{x, l}+B_{l} Q_{l} B_{l}^{T}$&lt;/p&gt;

&lt;p&gt;where $\Theta_{x, l}$ is given.&lt;/p&gt;

&lt;p&gt;As can be seen (7), is the Kalman-like recursion in which $A_{i} x_{i-1 \mid i-1}$ predicts the state from $i-1$ to $i$ and the bias correction gain (6) corrects the prediction for the residual. Although the KF and OFIR filter both minimize the MSE, $G_{i}$ is not the Kalman gain, because the KF has IIR. However, an increase in the horizon length $N$ reduces the estimation error and makes it such that the OFIR estimate converges to the KF estimate: the estimates become practically equal when $N&amp;gt;N_{\mathrm{opt}}$ . In this sense, the KF can be considered as a special case of a more general OFIR filter when $N=\infty$, provided the initial conditions. Another difference is that $N$ measurements are processed by the OFIR filter simultaneously at each time index $k$, while only one measurement is processed by the KF at $k$. That means that the computational complexity of OFIR filter $\mathcal{O}(N)$ is $N$ times larger than $\mathcal{O}(1)$ of the KF. On the other hand, the iterative algorithm reduces essentially the computational complexity $\mathcal{O}\left(N^{2}\right)$ of the batch OFIR form.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Hilbert Huang Transform</title>
   <link href="http://localhost:4000/2017/04/12/hht"/>
   <updated>2017-04-12T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/12/HHT</id>
   <content type="html">&lt;p&gt;The EMD method is necessary to reduce any data from non-stationary and nonlinear processes into simple oscillatory function that will yield meaningful instantaneous frequency through the Hilbert transform. Contrary to almost all the previous decomposing methods, EMD is empirical, intuitive, direct, and adaptive, with the a posteriori defined basis derived from the data. The decomposition is designed to seek the different simple intrinsic modes of oscillations in any data based on the principle of scale separation. The data, depending on it complexity, may have many different coexisting modes of oscillation at the same time. Each of these oscillatory modes is represented by an Intrinsic Mode Function (IMF) with the following definitions:&lt;/p&gt;

&lt;p&gt;(a) in the whole data set, the number of extrema and the number of zero-crossings must either equal or differ at most by one, and&lt;/p&gt;

&lt;p&gt;(b) at any point, the mean value of the envelope defined by the local maxima and the envelope defined by the local minima is zero.&lt;/p&gt;

&lt;p&gt;The IMF is a counter part to the simple harmonic function, but it is much more general: instead of constant amplitude and frequency, IMF can have both variable amplitude and frequency as functions of time. This definition is inspired by the simple example of constant plus sinusoidal function given above. The total number of the IMF components is limited to $\ln _{2} N$, where $\boldsymbol{N}$ is the total number of data points. It satisfies all the requirements for a meaningful instantaneous frequency through Hilbert transform.&lt;/p&gt;

&lt;p&gt;Pursuant to the above definition for IMF, one can implement the needed decomposition of any function, known as sifting, as follows: Take the test data; identify all the local extrema; divide the extrema into two sets: the maxima and the minima. Then connect all the local maxima by a cubic spline line to form an upper envelope. Repeat the procedure for the local minima to form a lower envelope. The upper and lower envelopes should encompass all the data between them. Their mean is designated as $m_1$, and the difference between the data and $m_1$ is designated as, $h_1$, a proto-IMF:&lt;/p&gt;

\[X(t)-m_{1}=h_{1}\]

&lt;p&gt;Ideally, $h_1$ should satisfy the definition of an IMF by construction of $h_1$ described above, which should have made it symmetric and having all maxima positive and all minima negative. Yet, in changing the local zero from a rectangular to a curvilinear coordinate system some inflection points could become additional extrema. New extrema generated this way actually reveal the hidden modes missed in the initial treatment. The sifting process sometimes can recover signals representing low amplitude riding waves with repeated siftings.&lt;/p&gt;

&lt;p&gt;The sifting process serves two purposes: to eliminate riding waves and to make the wave profiles more symmetric. While the first condition is absolute necessary for Hilbert transform to give a meaningful instantaneous frequency, the second condition is also necessary in case the neighboring wave amplitudes having too large a disparity. As a result, the sifting process has to be repeated many times to reduce the extracted signal an IMF. In the subsequent sifting process, $h_1$ is treated as the data for the next round of sifting; therefore,&lt;/p&gt;

\[h_{1}-m_{11}=h_{11}\]

&lt;p&gt;After repeated sifting, up to $\mathrm{k}$ times, $h_{1k}$ :&lt;/p&gt;

\[h_{1(k-1)}-m_{1 k}=h_{1 k} \text {. }\]

&lt;p&gt;If $\boldsymbol{h}_{\boldsymbol{1} \boldsymbol{k}}$ becomes an IMF, it is designated as 
$C_1$ :&lt;/p&gt;

\[C_{1}=h_{1 k}\]

&lt;p&gt;the first IMF component from the data. Here one has a critical decision to make: when to stop. Too many rounds of sifting will reduce the IMF to FM page criterion; too few rounds of sifting will not have a valid IMF. In the past, different criteria have been used, including Cauchy type criterion (Huang et al. 19980), $\boldsymbol{S}$-number criterion (Huang et al. 2003), fixed-number criterion (Wu and Huang 2004), and etc.&lt;/p&gt;

&lt;p&gt;With any stoppage criterion, the, $c_1$ should contain the finest scale or the shortest period component of the signal. one can, then, remove $c_1$ from the rest of the data by&lt;/p&gt;

\[X(t)-C_{1}=r_{1}\]

&lt;p&gt;Since the residue, $r_1$, contains all longer period variations in the data, it is treated as the new data and subjected to the same sifting process as described above. This procedure can be repeated to all the subsequent $r_j$ ‘s, and the result is&lt;/p&gt;

\[\begin{gathered}
r_{1}-C_{2}=r_{2}, \\
\cdots \\
r_{n-1}-C_{n}=r_{n}
\end{gathered}\]

&lt;p&gt;The sifting process should stop when the residue, $\boldsymbol{r}_{n}$, becomes a constant, a monotonic function, or a function contains only a single extrema, from which no more IMF can be extracted. By summing up Equations (16) and (17), we finally obtain&lt;/p&gt;

\[X(t)=\sum_{j=1}^{n} C_{j}+r_{n}\]

&lt;p&gt;Thus, sifting process produces a decomposition of the data into $\boldsymbol{n}$-intrinsic modes, and a residue, $\boldsymbol{r}_{\boldsymbol{n}}$. When apply the EMD method, a mean or zero reference is not required; EMD needs only the locations of the local extrema. The sifting process generates the zero reference for each component. Without the need of the zero reference, EMD avoids the troublesome step of removing the mean values for the large non-zero mean.&lt;/p&gt;

&lt;p&gt;Two special notes here deserve our attention. First, the sifting process offered a way to circumvent the difficulty of define the local mean in a nonstationary time series, where no length scale exists for one to implement the traditional mean operation. The envelope mean employed here does not involve time scale; however, it is local. Second, the sifting process is a Reynolds-type decomposition: separating variations from the mean, except that the mean is a local instantaneous mean, so that the different modes are almost orthogonal to each other, except for the nonlinearity in the data.&lt;/p&gt;

&lt;p&gt;Recent studies by Flandrin et al. (2004) and Wu and Huang (2004) established that the EMD is equivalent to a dyadic filter bank, and it is also equivalent to an adaptive wavelet. Being adaptive, we have avoided the shortcomings of using any a priori-defined wavelet basis, and also avoided the spurious harmonics that would have resulted. The components of the EMD are usually physically meaningful, for the characteristic scales are defined by the physical data.&lt;/p&gt;

&lt;p&gt;Having established the decomposition, we can also identify a new use of the IMF components as filtering. Traditionally, filtering is carried out in frequency space only. But there is a great difficult in applying the frequency filtering when the data is either nonlinear or non-stationary or both, for both nonlinear and nonstationary data generate harmonics of all ranges. Therefore, any filtering will eliminate some of the harmonics, which will cause deformation of the data filtered. Using IMF, however, we can devise a time space filtering.&lt;/p&gt;

&lt;p&gt;For example, a low pass filtered results of a signal having $\boldsymbol{n}$-IMF components can be simply expressed as&lt;/p&gt;

\[X_{l k}(t)=\sum_{k}^{n} C_{j}+r_{n}\]

&lt;p&gt;a high pass results can be expressed as&lt;/p&gt;

\[X_{h k}(t)=\sum_{1}^{k} C_{j}\]

&lt;p&gt;and a band pass result can be expressed as&lt;/p&gt;

\[X_{b k}(t)=\sum_{b}^{k} C_{j}\]

&lt;p&gt;The advantage of this time space filtering is that the results preserve the full nonlinearity and nonstationarity in the physical space.&lt;/p&gt;

&lt;p&gt;Having obtained the Intrinsic Mode Function components, one can compute the instantaneous frequency for each IMF component as the derivative of the phase function. And one can also designate the instantaneous amplitude from the Hilbert transform to each IMF component. Finally, the original data can be expressed as the real part, RP, of the sum of the data in terms of time, frequency and energy as:&lt;/p&gt;

\[X(t)=R P \sum_{j=1}^{n} a_{j}(t) e^{i \int \omega_{j}(t) d t}\]

&lt;p&gt;Above equation gives both amplitude and frequency of each component as a function of time. The same data, if expanded in a Fourier representation, would have a constant amplitude and frequency for each component.&lt;/p&gt;

&lt;p&gt;The contrast between EMD and Fourier decomposition is clear: The IMF represents a generalized Fourier expansion with a time varying function for amplitude and frequency. This frequency-time distribution of the amplitude is designated as the Hilbert Amplitude Spectrum, $\boldsymbol{H}(\boldsymbol{\omega}, \boldsymbol{t})$, or simply the Hilbert spectrum.&lt;/p&gt;

&lt;p&gt;From the Hilbert spectrum, we can also define the marginal spectrum, $\boldsymbol{h}(\boldsymbol{\omega})$, as&lt;/p&gt;

\[h(\omega)=\int_{0}^{T} H(\omega, t) d t\]

&lt;p&gt;The marginal spectrum offers a measure of total amplitude (or energy) contribution from each frequency value. It represents the cumulated amplitude over the entire data span in a probabilistic sense.&lt;/p&gt;

&lt;p&gt;The combination of the Empirical Mode Decomposition and the Hilbert Spectral Analysis is designated by NASA as the Hilbert-Huang Transform (HHT) for short. Recent studies by various investigators indicate that HHT is a super tool for time-frequency analysis of nonlinear and nonstationary data (Huang and Attoh-Okine, 2005, Huang and Shen, 2005). It is based on an adaptive basis, and the frequency is defined through the Hilbert transform. Consequently, there is no need for the spurious harmonics to represent nonlinear waveform deformations as in any of the a priori basis methods, and there is no uncertainty principle limitation on time or frequency resolution from the convolution pairs based also on a priori bases. A summary of the comparison between Fourier, Wavelet and HHT analyses is given in Table 1.&lt;/p&gt;

&lt;p&gt;Table 1. Comparisons between Fourier, Wavelet and HilbertHuang Transform in Data analysis.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Transform&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Fourier&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Wavelet&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Hilbert&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Basis&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a priori&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;a priori&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;adaptive&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Frequency&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;convolution: global, uncertainty&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;convolution: regional, uncertainty&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;differentiation: local, certainty&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Presentation&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;energy-frequency&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;energy-time-frequency&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;energy-time-frequency&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Nonlinear&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Non-stationary&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Feature Extraction&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;no&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;discrete: no, continuous: yes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;yes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Theoretical Base&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;theory complete&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;theory complete&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;empirical&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After this basic development of the HHT method, there are some recent developments, which have either added insight to the results, enhanced the statistical significance of the results, and fixed some shortcomings in the HHT.&lt;/p&gt;

&lt;h2 id=&quot;recent-developments&quot;&gt;Recent developments&lt;/h2&gt;

&lt;h3 id=&quot;the-normalized-hilbert-transform-and-the-direct-quadrature&quot;&gt;The Normalized Hilbert Transform and the direct quadrature&lt;/h3&gt;

&lt;p&gt;It is well known that, although the Hilbert transform exists for any function of $\boldsymbol{L}^{p}$ class, the phase function of the transformed function will not always yield physically meaningful instantaneous frequencies. The limitations have been summarized succinctly in two theorems.&lt;/p&gt;

&lt;p&gt;First, in order to separate the contribution of the phase variation into the phase and amplitude parts, the function have to satisfy the limitation stipulated in the Bedrosian theorem (1963), which states that the Hilbert transform for the product of two functions, $\boldsymbol{f}(\boldsymbol{t})$ and $\boldsymbol{h}(\boldsymbol{t})$, can be written as&lt;/p&gt;

\[H[f(t) h(t)]=f(t) H[h(t)]\]

&lt;p&gt;only if the Fourier spectra for $\boldsymbol{f}(\boldsymbol{t})$ and $\boldsymbol{h}(\boldsymbol{t})$ are totally disjoint in frequency space, and the frequency content of the spectrum for $\boldsymbol{h}(\boldsymbol{t})$ is higher than that of $\boldsymbol{f}(\boldsymbol{t})$. This limitation is critical, for we need to have&lt;/p&gt;

&lt;p&gt;$H[a(t) \cos \theta(t)]=a(t) H[\cos \theta(t)]$, (28) otherwise, one cannot use Equation (6) to define the phase function, for the amplitude variation would mix with the phase function. Bedrosian theorem requires that the amplitude is varying be so slowly that the frequency spectra of the envelope and the carrier waves are disjoint. This is possible only for trivial cases, for unless the amplitude is constant, any local deviation can be considered as a sum of delta-functions, which has a wide white spectrum. Therefore, the spectrum for varying amplitude would never be totally separate from that of the carrier. This limitation has made the application of the Hilbert transform even to IMFs problematic. To satisfy this requirement, Huang and Long (2003) have proposed the normalization of the IMFs in the following steps: Starting from an IMF, they first find all the maxima of the IMFs, defining the envelope by spline through all the maxima, and designating the envelope as $\boldsymbol{E}(\boldsymbol{t})$. Now, normalize the IMF by dividing the IMF by $\boldsymbol{E}(\boldsymbol{t})$. Thus, they have the normalized function having amplitude always equal to unity, and have circumvented the limitation of Bedrosian theorem.&lt;/p&gt;

&lt;p&gt;Second, there is the new restriction given by the Nuttall theorem (1966), which stipulates that the Hilbert transform of cosine is not necessarily the sine with the same phase function for a cosine with an arbitrary phase function. Nuttall gave an energy based error bound, $\boldsymbol{\Delta E}$, defined as the difference between $y(t)$, the Hilbert transform of the data, and $\boldsymbol{Q}(\boldsymbol{t})$, the quadrature (with phase shift of exactly $90^{\circ}$ ) of the function as&lt;/p&gt;

\[\Delta E=\int_{t=0}^{T}|y(t)-Q(t)|^{2} d t=\int_{-\infty}^{0} S_{q}(\omega) d \omega,\]

&lt;p&gt;in which $\boldsymbol{S}_{\boldsymbol{q}}$ is Fourier spectrum of the quadrature function. Though the proof of this theorem is rigorous, the result is hardly useful, for it gives a constant error bound over the whole data range. With the normalized IMF, Huang and Long (2003) have proposed a variable error bound based on a simple argument, which goes as follows: compute the difference between squared amplitude of the normalized IMF and unity. If the Hilbert transform is exactly the quadrature, the difference between it and unity should be zero; otherwise, the Hilbert transform cannot be exactly the quadrature. Consequently, the error can be measured simply by the difference between the squared normalized IMF and unity, which is a function of time. Huang and Long (2003) and Huang et al. (2006) have conducted detailed comparisons and found the result quite satisfactory.&lt;/p&gt;

&lt;p&gt;Even with the error indicator, we can only know that the Hilbert transform is not exactly the quadrature; we still do not have the correct answer. This prompts a drastic alternative, eschewing the Hilbert transform totally. An exact direct quadrature has been found (Huang et al., 2006), and it would resolve the difficulties associated with the instantaneous frequency computation.&lt;/p&gt;

&lt;h3 id=&quot;the-confidence-limit&quot;&gt;The Confidence Limit&lt;/h3&gt;

&lt;p&gt;The confidence limit for the Fourier spectral analysis is based on the ergodic theory, where the temporal average is treated as the ensemble average. This approach is only valid if the processes are stationary. Huang et al. (2003) has proposed a different approach by utilizing the fact that there are infinite many ways to decompose one given function into difference components. Using EMD, one can still obtain many different sets of IMFs by changing the stoppage criteria. The confidence limit so derived does not depend on the ergodic theory From the confidence limit study, Huang et al. (2003) also found the optimal $\boldsymbol{S}$-number, when the differences reach a local minimum. Based on their experience from different data sets, they concluded that an $\boldsymbol{S}$-number in the range of 4 to 8 performed well. Logic also dictates that the $\boldsymbol{S}$-number should not be too high (which would drain all the physical meaning out of the IMF), nor too low (which would leave some riding waves remaining in the resulting IMFs).&lt;/p&gt;

&lt;h3 id=&quot;the-statistical-significance-of-imfs&quot;&gt;The Statistical Significance of IMFs&lt;/h3&gt;

&lt;p&gt;The EMD is a method to separate the data into different components by their scales. There is always the question: On what is the statistical significance of the IMFs based? In data containing noise, how can we separate the noise from information with confidence? This question was addressed by both Flandrin et al. (2004) and Wu and Huang (2004) through the study of signals consisting of noise only. Using white noise, Wu and Huang (2004) found the relationship between the mean period and RMS values of the IMFs. Furthermore, from the statistical properties of the scattering of the data, they found the bounds of the data distribution analytically. They concluded that when a data set is analyzed with EMD, if the mean period-RMS values exist outside the noise bounds, the components most likely contains signal, otherwise, a component could be resulted only from noise. Therefore, the components with their mean period-RMS values exceeding the noise bounds are statistically significant.&lt;/p&gt;

&lt;h3 id=&quot;ensemble-emd-eemd&quot;&gt;Ensemble EMD (EEMD)&lt;/h3&gt;

&lt;p&gt;One of the major problems existed in EMD is scale mixing: an IMF often contains local oscillations with dramatically different frequencies/scales (Huang et al 1999). Previous solution to that was introducing the intermittency check in which the frequency/scale range is subjectively determined. While such an approach works well in many cases, it also has side effect such as reducing adaptation of the EMD method.&lt;/p&gt;

&lt;p&gt;Recently, a new Ensemble Empirical Mode Decomposition (EEMD) method is presented. This new approach consists of an ensemble of decompositions of data with added white noise, and then treats the resultant mean as the final true result. Finite, not infinitesimal, amplitude white noise is necessary to force the ensemble to exhaust all possible solutions in the sifting process, thus requiring the different scale signals to collate in the proper intrinsic mode functions (IMF) dictated by the dyadic filter banks. The effect of the added white noise is to present a uniform reference frame in the timefrequency and time-scale space; and, therefore, the added noise provides a natural reference for the signals of comparable scale to collate in one IMF. With this ensemble mean, the scale can be clearly and naturally separated without any a priori subjective criterion selection, such as in the intermittence test for the original EMD algorithm. This new approach fully utilizes the statistical characteristics of white noise to perturb the data in its true solution neighborhood, and then cancel itself out (via ensemble averaging) after serving its purpose; therefore, it represents a substantial improvement over the original EMD and qualifies for a truly noise-assisted data analysis (NADA) method.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Sparse representation of signals</title>
   <link href="http://localhost:4000/2017/04/11/sparse-representation"/>
   <updated>2017-04-11T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/11/sparse-representation</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;Sparse representation is widely employed for expressing signals using very few linear combinations of elementary signals. These elementary signals are called atoms. Since the
number of the atoms is more than the dimension of the signal
space, any signal can be represented by linear combinations
of these atoms and the representations are not unique.&lt;/p&gt;

&lt;p&gt;Sparse representation is to use the minimum number of atoms to express the signals and this is actually an $L_{0}$ norm optimization problem. That is for a given overcomplete dictionary $A\in\Re^{N \times M}$ and a signal $x\in R^{N \times 1}$, where $N&amp;lt;M$ and $R^{a \times b}$ denotes the space of $a \times b$ real valued matrices, the representation problem is to find $z\in\Re^{M \times 1}$ such that $x=Az$ and $\lVert z\lVert_{0}$ is minimized. That is 
\(z_{0}^{\ast} = \mathrm{argmin}_{z} \lVert z\lVert_{0} \text{  subject to  } x=Az \tag{1}\)&lt;/p&gt;

&lt;p&gt;Here $\lVert z\lVert_{0}$ denotes $L_{0}$ norm of $z$, which is equivalent to the number of nonzero elements in $z$.&lt;/p&gt;

&lt;p&gt;The problem defined in (1) is nonconvex, nonsmooth and NP hard, it requires an exhaustive search for finding the solution. An approximate solution can be obrtained by solving the corresponding $L_{1}$ norm optimization problem if the isometry condition is satisfied. The $L_{1}$ norm optimization is as follows&lt;/p&gt;

\[z_{1}^{\ast} = \mathrm{argmin}_{z} \lVert z\lVert_{1} \text{  subject to  } x=Az \tag{2}\]

&lt;p&gt;Although $z_{1}^{\ast}$ is a good approximation of $z_{0}^{\ast}$ when the isometry condition is satisifed, these two solutions will be very different if $x$ contains significant amount of noise. Nevertheless, this is the typical case in practical circumstances. Hence the exact equality constraint is usually related to an inequality constraint as follows.&lt;/p&gt;

\[z^{\ast} = \mathrm{argmin}_{z} \lVert z\lVert_{1} \text{  subject to  } \lVert Az-x\lVert_{\infty} \le \epsilon \tag{3}\]

&lt;p&gt;where $\epsilon$ is the specification on the maximium absolute difference between $Az$ and $x$. This problem can be efficiently solved via reformulating the $L_{1}$ norm optimization problem to a linear programming problem.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Single Spectrum Analysis</title>
   <link href="http://localhost:4000/2017/04/10/ssa"/>
   <updated>2017-04-10T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/10/SSA</id>
   <content type="html">&lt;p&gt;The main steps of SSA can be summarized as follows:&lt;/p&gt;

&lt;p&gt;For a time series $x(n)$ for $n=1,2\dots N$, let the window length be $L$ , where  $1&amp;lt;L&amp;lt;N$. The first step of SSA is to construct a trajectory matrix as follows. Define the $L$ dimensional vectors as&lt;/p&gt;

\[X_{n}=\begin{bmatrix} x(n)\\ \vdots \\ x(n+L-1)
\end{bmatrix}\]

&lt;p&gt;for $n =1,2,\dots, N − L +1$.&lt;/p&gt;

&lt;p&gt;Denote $K = N − L +1$. These $K$ vectors are put into a matrix and the $L \times K$ trajectory matrix is constructed as follows:&lt;/p&gt;

\[X = \begin{bmatrix}
X_{1} &amp;amp; X_{2} &amp;amp; \dots &amp;amp; X_{K}
\end{bmatrix}\]

&lt;p&gt;The second step is to express $X$ as the sum of component matrices. Let $S=XX^T$ and the eigenvalues of $S$ be $\lambda_{1}\ge\lambda_{2}\dots\ge\lambda_{L}\ge 0$. Define $D=\max{j:\lambda_{j}&amp;gt;0}$. Let $U_{1},\dots,U_{D}$ be the corresponding eigenvectors.&lt;/p&gt;

&lt;p&gt;Denote $V_{j}=\frac{X^TU_{j}}{\sqrt{ \lambda_{j} }}$ for $j=1,2,\dots,D$  be the factor vectors.&lt;/p&gt;

&lt;p&gt;Define:&lt;/p&gt;

\[\tilde{X}_{j} =\sqrt{\lambda_{j}}U_{j}V_{j}^T\]

&lt;p&gt;for $j=1,2,\dots,D$. It can be shown that $X$ can be represented as&lt;/p&gt;

\[X = \tilde{X}_{1}+\dots+\tilde{X}_{D}\]

&lt;p&gt;The third step is to represent $X$ as the sum of grouped matrix components as follows. The indices set ${1,\dots,D}$ is partitioned into $M$ disjoint subsets $I_{1}\dots I_{M}$. Let $I_{m} ={i_{m_{1}},\dots,{i_{m_{c}}}}$ for $m=1,\dots, M$ and&lt;/p&gt;

\[\tilde{X}_{I_{m}}=\tilde{X}_{i_{m_{1}}}+\dots+\tilde{X}_{i_{m_{C}}}\]

&lt;p&gt;Hence we have&lt;/p&gt;

\[X=\tilde{X}_{I_{1}}+\dots+\tilde{X}_{I_{M}}\]

&lt;p&gt;The final step is to reconstruct the signal by the diagonal averaging method.&lt;/p&gt;

&lt;p&gt;First, transform  new text $\tilde{X}_{I_m}$ into new one dimensional signals of length $N$ by the hankelization like procedure.&lt;/p&gt;

&lt;p&gt;The vectors and the transform operator are denoted as $\tilde{x}_{I_m}$  for $m=1,\dots,M$ and  $\Im(.)$ respectievely. That is&lt;/p&gt;

\[\tilde{x}_{I_{m}} = \Im(\tilde{X}_{I_{m}}) ; m=1,\dots,M\]

&lt;p&gt;Thus the original time series can be expressed as a sum of $M$ series,&lt;/p&gt;

\[x(n)=\tilde{x}_{I_{1}}(n)+\dots+\tilde{x}_{I_{M}}(n) ; n=1,\dots,N\]
</content>
 </entry>
 
 <entry>
   <title>Trend extraction with SSA and Sparse Binary Programming</title>
   <link href="http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming"/>
   <updated>2017-04-09T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;The underlying trend is approximated by the sum of a part of &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/10/ssa&quot;&gt;SSA&lt;/a&gt; components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.&lt;/p&gt;

&lt;p&gt;As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem can
be approximated by a linear programming problem when the
isometry condition is satisfied, and the solution of the linear
programming problem can be obtained via existing simplex
methods or interior point methods.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;By applying the binary
quantization to the obtained solution of the linear programming
problem, the approximated solution of the original sparse
binary programming problem is obtained. Unlike previously
reported techniques that require a pre-cursor model or
parameter specifications, the proposed method is completely
adaptive.&lt;/p&gt;

&lt;h3 id=&quot;details&quot;&gt;Details&lt;/h3&gt;

&lt;p align=&quot;justify&quot;&gt;The conventional approach for selecting SSA
components for extracting the underlying trend is to employ
only the first several SSA components. However, this
selection rule fails when the underlying trend of a signal has a
complicated structure such as a high order polynomial
structure which cannot be characterized by only the first
several SSA components.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The idea is to formulate the
selection problem as a sparse binary programming problem and proposes an efficient methodology for approximating
the solution of the problem. In particular, the selection
problem is formulated as follows. The number of the
components to be selected is minimized subject to a
specification on the maximum absolute difference between
the approximated underlying trend and the original signal as
well as the binary valued constraint on the selection
coefficients. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Since the sparse binary programming problem is
nonsmooth, nonconvex and NP hard, it requires an exhaustive
search for finding the solution. As a result, the computational
effort for finding the solution is very large and an efficient
algorithm for approximating the solution is very useful and
important. &lt;/p&gt;

&lt;p&gt;To address these issues, the corresponding continuous valued optimization problem (the same
optimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective function
subject to an $L_{\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.&lt;/p&gt;

&lt;p&gt;By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem is
obtained.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;SSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.&lt;/p&gt;

&lt;p&gt;Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.&lt;/p&gt;

&lt;p&gt;However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.&lt;/p&gt;

&lt;p&gt;In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.&lt;/p&gt;

&lt;p&gt;The selection problem is formulated as the following sparse sparse binary programming problem.&lt;/p&gt;

\[z^{*}= \begin{bmatrix}
z_{1}^{*}&amp;amp; \dots, z_{D}^*
\end{bmatrix}^{T}= \mathrm{argmin}_{z} \lVert z\lVert_{0}\]

&lt;p&gt;subject to $\lVert Az-x\lVert_{\infty} \le \epsilon$ and $z_{i}\in{0,1}$ for $i=1,\dots,D$.&lt;/p&gt;

&lt;p&gt;Here&lt;/p&gt;

\[A=[\tilde{x}_1,\dots,\tilde{x}_D]\in R^{N \times D}\]

&lt;p&gt;and&lt;/p&gt;

\[\epsilon=0.5\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\]

&lt;p&gt;where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.&lt;/p&gt;

&lt;p&gt;If $z_{i}^{\ast}=1$ (or $z_{i}^{\ast}=0$), then the corresponding component $\tilde{x}_i$ for $i=1,\dots,D$ is selected (or excluded) for the representation of the underlying trend.&lt;/p&gt;

&lt;p&gt;Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.&lt;/p&gt;

&lt;p&gt;In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimization
problem is considered first. The solution of the $L_{0}$ norm
continuous valued optimization problem is approximated by
the solution of the corresponding $L_{1}$ norm continuous valued
optimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:&lt;/p&gt;

\[y^{*}= [y_{1}^{*},\dots,y_{D}^{*}]= \mathrm{arg}\min_{y}\lVert y\lVert_{1} \text{  subject to } \lVert Ay-x\lVert_{\infty}\le \epsilon\]

&lt;p&gt;By further applying the quantization on $y_{i}^{*}$ for $i=1,\dots,D$ to either $0$ or $1$ via the following operator.&lt;/p&gt;

\[W_{i}^{*}=
\begin{cases}
1 &amp;amp;  y_{i}^{*}\ge 0.5\\
0 &amp;amp;  y_{i}^{*}&amp;lt; 0.5
\end{cases}\]

&lt;p&gt;the corresponding component $\tilde{x}_{i}$&lt;/p&gt;

&lt;p&gt;for is selected (excluded) for the representation of the underlying trend if&lt;/p&gt;

&lt;p&gt;$W_{i}^{\ast}=1( \text{ or } W_{i}^{\ast}=0)$&lt;/p&gt;

&lt;p&gt;Finally the underlying trend of the signal is obtained by&lt;/p&gt;

\[\Gamma = AW^*\]

&lt;p&gt;where $W^{\ast}= [W_1^{\ast},\dots,W_D^{\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/11/sparse-representation&quot;&gt;sparse binary programming problem&lt;/a&gt;. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programming
problem. That is, $W^{\ast}$ in is very close to $z^{\ast}$ above.&lt;/p&gt;

&lt;p&gt;Therefore, the subset $I_{1}$ can be obtained by&lt;/p&gt;

\[I_{1} = \{i\vert W_{i}^{*}= 1, 1\le i 
\le D\}\]

&lt;p&gt;After obtaining the underlying trend $\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.&lt;/p&gt;

&lt;p&gt;Denote $\Gamma =[\gamma(1), \dots, \gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume that
there is an underlying structure in the time series and this
structure is preserved for the time period to be predicted. A
prediction model based on the linear recurrent formulae (LRF)
is employed.&lt;/p&gt;

&lt;p&gt;That is, the points $\gamma(N-L+2), \dots, \gamma(N)$ are employed for the prediction of $\gamma(N+1)$, and so on. In other words,&lt;/p&gt;

\[\gamma(n+1) = \sum_{k=0}^{L-2}a_{k}\gamma(n-k) \text{   for  } n\ge N,\]

&lt;p&gt;where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \dots, a_{0})^T$ is given by&lt;/p&gt;

\[R = \frac{1}{1-\nu^{2}}\sum_{i\in I_{1}}\pi_{i}U_{i}^{\nabla}\]

&lt;p&gt;Here, $\nu^{2=}\sum_{i\in I_{1}}\pi_{i}^2$, $\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\nabla }\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \in I_{1}.$&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>ICA with CDWT</title>
   <link href="http://localhost:4000/2017/04/08/ica-with-cdwt"/>
   <updated>2017-04-08T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/08/ICA-with-CDWT</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;It is well known that if the original sounds are mixed in
the real environment (in the time domain) then the observed
sounds are a convolution mixture between the original
sounds with a delay and a reverberation. In order to simplify
this convolution mixture, it is a good idea to convert the
signal from the time domain into the time–frequency
domain and transform the convolution mixture into the
linear mixture by a time–frequency analysis method. By
doing this the drawback of poor performance with unsteady
sounds of the ICA also can be improved.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The time-frequency analysis method is usually a
combination of the &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/06/blind-source-seperation-ica&quot;&gt;ICA&lt;/a&gt;, the Short Time Fourier Transform
(STFT) and the Discrete Wavelet Transform (DWT).&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The STFT is probably the most common approach for
time–frequency analysis. It subdivides the signal into short
time segments (it is the same as using a small window to
divide the signal), and a discrete Fourier transform is
computed for each of these. For each frequency component,
however, the window length is fixed. So it is impossible to
choose an optimal window for each frequency component,
that is, the short time Fourier transform is unable to obtain
optimal analysis results for individual frequency
components.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;On the other hand, the DWT that was
carried out by Mattal’s fast algorithm also has a drawback
of lacking shift invariance although it can solve the problem
of the window width and obtain optimal frequency
resolution for each frequency component. Fortunately,
in order to improve the fault, a Complex Discrete Wavelet
Transform (CDWT) was proposed and it has been applied
widely to signal and image analysis&lt;/p&gt;

&lt;h2 id=&quot;ica-and-cdwt-for-blind-source-seperation&quot;&gt;ICA and CDWT for blind source seperation&lt;/h2&gt;

&lt;p&gt;In this method, the signals first were transformed from the time–domain into the
time–frequency domain by using the CDWT and then the ICA was carried out in the time–frequency domain. As in traditional methods, such as the STFT + ICA and the DWT + ICA, the following two problems when the ICA processing was carry out in the time–frequency domain
occurred. 
	1. Scaling problem: the signal’s amplitude and phase obtained by the ICA was changed, 
	2. Permutation problem: the separated signals are replaced at every frequency level mutually.&lt;/p&gt;

&lt;p&gt;This method discuss the technique for solving the scaling and the permutation problems. Finally, the separated signals are obtained by the inverse CDWT.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/snips/ica_cdwt.png&quot; alt=&quot;ICA_CDWT&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the case of two sound sources and two mikes has been considered for simplicity. First of all,
the two sound signals $x_n(t) (n=1,2,$ where $n$ is the number of channels and $t$ is the time) were observed by mikes from two sound sources $s_{i}(t) ( i =1, 2)$. The relation between the observed signal $x_n(t)$ and the sound source $s_{i}(t)$ is as follows:&lt;/p&gt;

\[x_{n}(t) = \mathbf{A_{n,i}}(t) \ast  S_{i}(t) \tag{1}\]

&lt;p&gt;where $\ast$ denotes convolution, the $\mathbf{A_{n,i}}(t)$ , is the impulse response that is from the sound sources to the mikes and can be shown as the following equation:&lt;/p&gt;

\[\mathbf{A_{n,i}}(t) = \begin{bmatrix} 
a_{11}(t) &amp;amp; a_{12}(t)\\ a_{21}(t) &amp;amp; a_{22}(t)\\
\end{bmatrix}, n,i = 1,2\]

&lt;p&gt;If one transforms the signal $x_{n}(t)$ from the time domain to the time–frequency domain by the CDWT then the (1) can be expressed as (2), in which the convolution of the sound source and the impulse response was changed to simple multiplication.&lt;/p&gt;

\[x_{n}(\omega , T) = \mathbf{A_{n,i}}(\omega) S_{i}(\omega,T) \tag{2}\]

&lt;p&gt;where, $\omega$ is the frequency and $T$ the time in the time–frequency domain.&lt;/p&gt;

&lt;p&gt;Next, whitening of the observed signal was carried out as follows:&lt;/p&gt;

\[\hat{x}_{n}(\omega , T) = Q(\omega) x_{n}(\omega,T) \tag{3}\]

&lt;p&gt;where $\hat{x}_{n}(\omega ,T)$&lt;/p&gt;

&lt;p&gt;is a whitened signal matrix and $Q(\omega)$ a whitened mixture, which can be obtained from the observed mixture signal $x_{n}(\omega , T)$ in each frequency.&lt;/p&gt;

&lt;p&gt;Finally, the ICA was carried out by using the whitened signal matrix $\hat{x}_{n}(\omega , T)$&lt;/p&gt;

&lt;p&gt;, in which the separation matrix $W(\omega)$ can be presumed. As a result, the separated signal $u_{i}(\omega , T)$ shown in (4) can be obtained.&lt;/p&gt;

\[u_{i}(\omega , T) = W(\omega) \hat{x}_{n}(\omega,T) \tag{4}\]

&lt;p&gt;The convolution mixture signal $x_{n}(t)$  shown in (1) can be transformed into the linear mixture signal
$x_{n}(\omega, T)$ shown in (2) by using the CDWT which simplifies the preprocessing of the ICA. A complex wavelet like, RI-Daubechies 6 wavelet can be applied as the mother wavelet.&lt;/p&gt;

&lt;h2 id=&quot;problem-and-correction-rule-of-ica-processing&quot;&gt;Problem and correction rule of ICA processing&lt;/h2&gt;

&lt;p&gt;The ICA is a technique for presuming the sound source $s_{i}(\omega,T)$  and the inverse matrix of $A_{ni}(\omega)$ from statistics without any former information of the observed signal. In this case, the amplitude of the separated signal $u_{i}(\omega,T)$ is a constant times the amplitude of the sound source $s_{i}(\omega,T)$ and a correction is needed.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;In this method, a similar amplitude change at each frequency level also occurred in the preprocessing by CDWT introduced. This phenomenon is called a scaling irregularity. Furthermore,
same as in the traditional method, it is also possible that the separated sound is replaced with noise at every frequency
level mutually. This phenomenon is called the permutation
problem. Therefore, after these two problems are solved, the
inverse complex discrete wavelet transform is performed,
and it is necessary to restore the sound signal observed with
each mike.&lt;/p&gt;

&lt;h3 id=&quot;scaling-problem-and-correction-rule&quot;&gt;Scaling problem and correction rule&lt;/h3&gt;

&lt;p&gt;Not only the amplitude of the restored signal but the phase also differs depending on the frequency by making for signal whitening (making no correlation). To take an arbitrary complex value by processing, this is caused. In order to solve the scaling irregularity problem, a method has
been proposed by Murata that uses the independent element $u_{i}(\omega,T)$ , which is obtained at each frequency and divides the spectrum. In this study, the method of correcting scaling is adopted from the divided spectrum and can be shown as follows:&lt;/p&gt;

\[v_{1}(\omega,T)=\begin{bmatrix}
v_{11}(\omega,T)\\ v_{12}(\omega,T)
\end{bmatrix} = (W(\omega)Q(\omega))^{-1}\begin{bmatrix}
u_{1}(\omega,T) \\
0
\end{bmatrix}\]

\[v_{2}(\omega,T)=\begin{bmatrix}
v_{21}(\omega,T)\\ v_{22}(\omega,T)
\end{bmatrix} = (W(\omega)Q(\omega))^{-1}\begin{bmatrix} 0 \\
u_{2}(\omega,T) 
\end{bmatrix}\]

&lt;p&gt;where $v_{11}(\omega,T),v_{22}(\omega,T)$ are divided spectrums. If the sum $v_{1}(\omega,T)+v_{2}(\omega,T)$ is calculated then the sum $v_{11}(\omega,T)+v_{21}(\omega,T)$ is the mixture signal $x_{1}(\omega,T)$ and the sum $v_{11}(\omega,T)+v_{22}(\omega,T)$ is the mixture signal $x_{2}(\omega,T)$&lt;/p&gt;

&lt;h3 id=&quot;permutation-problem-and-correction-rule&quot;&gt;Permutation problem and correction rule&lt;/h3&gt;

&lt;p align=&quot;justify&quot;&gt;The complex Fast-ICA performs separation based on
non-Gaussian characteristics. Therefore, the possibility of
the separating signal with higher non-Gaussian
characteristics being output as the first channel is very high.
However, the height of the frequency is not determined and
noise with low non-Gaussian characteristics might also be
output. &lt;/p&gt;

&lt;p&gt;Therefore, there is a possibility that the output of each frequency level is separated without being united by the sound. The separated signal $u_{1}(\omega,T), u_{2}(\omega,T)$ of the 1st and 2nd channel without permutation is shown as follows.&lt;/p&gt;

\[u_{1}(\omega ,k) \approx s_{1}(\omega,T) \tag{5} ; u_{2}(\omega ,k) \approx s_{2}(\omega,T)\]

&lt;p&gt;The above expression can be shown as the following equation by using the divided spectrum of the scaling correction.&lt;/p&gt;

\[\begin{bmatrix}
v_{11}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{11}(\omega,T)u_{1}(\omega,T) \\
g_{21}(\omega,T)u_{1}(\omega,T)
\end{bmatrix}\]

\[\begin{bmatrix}
v_{21}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{12}(\omega,T)u_{2}(\omega,T) \\
g_{22}(\omega,T)u_{2}(\omega,T)
\end{bmatrix}\]

&lt;p&gt;On the other hand, when permutation occurs, (5) becomes the next expression&lt;/p&gt;

\[u_{1}(\omega ,k) \approx s_{2}(\omega,T) \tag{6} ; u_{2}(\omega ,k) \approx s_{1}(\omega,T)\]

\[\begin{bmatrix}
v_{11}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{12}(\omega,T)u_{2}(\omega,T) \\
g_{22}(\omega,T)u_{2}(\omega,T)
\end{bmatrix}\]

\[\begin{bmatrix}
v_{21}(\omega,T)\\ 
v_{22}(\omega,T)
\end{bmatrix} = \begin{bmatrix}
g_{11}(\omega,T)u_{1}(\omega,T) \\
g_{21}(\omega,T)u_{1}(\omega,T)
\end{bmatrix}\]

&lt;p&gt;From these, we can know that the divided spectrum can be shown as a multiplication of the separated signal $u_{i}(\omega,T)$ and the transfer function $g_{ni}(\omega)$ , which is from the sound source to the mike.&lt;/p&gt;

&lt;p&gt;The separation matrix $W(\omega)$ and the whitening matrix $Q(\omega)$ presumed by the ICA processing are used and the next equation can be obtained.&lt;/p&gt;

\[D = (W(\omega)Q(\omega))^{-1} = \begin{bmatrix}
g_{11}(\omega) &amp;amp; g_{12}(\omega) \\
g_{21}(\omega) &amp;amp; g_{22}(\omega)
\end{bmatrix} = e\begin{bmatrix}
a_{11}(\omega) &amp;amp; a_{12}(\omega) \\
a_{21}(\omega) &amp;amp; a_{22}(\omega)
\end{bmatrix} = eA_{ni}(\omega), e \in \mathbb{R}\]

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;CDWT +Complex Value Fast ICA outperforms, STFT+ Complex value Fast ICA and DWT+Real Value Fast ICA&lt;/p&gt;

&lt;p&gt;Reference : BLIND SOURCE SEPARATION BY COMBINING INDEPANDENT COMPONENT ANALYSIS WITH COMPLEX DISCRETE WAVELET TRANSFORM ZHONG ZHANG , TAKESHI ENOMOTO , TETSUO MIYAKE , TAKASHI IMAMURA&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Piecewise linear representation of time series</title>
   <link href="http://localhost:4000/2017/04/07/piecewise-linear-representation"/>
   <updated>2017-04-07T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/07/piecewise-linear-representation</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;The time series data usually fluctuate frequently and exit a lot of noise. So data mining in the original sequence data directly will not only cost highly in the storage and computation, but also probably affect the accuracy and reliability of the data mining algorithms. Therefore, many time series models are proposed, which can transform original series to new series. Modeling may not only compress the data, but also keep the main form and ignore fine changes. Accordingly, it can help improve the efficiency and accuracy of the data mining algorithms, which will provide policy support for data analysts.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation Based on Important Point:&lt;/b&gt;&lt;/u&gt; Pratt and Fink proposed a piecewise linear representation based on the important points. The important points are defined as the points which are the extreme points within the local scope and the ratio of the important point and the endpoint exceeds the parameters $R$. After extracting the important points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series and get various piecewise linear representation with different fine and granularity by selecting different parameters  $R$.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Aggregate Approximation (PAA):&lt;/b&gt;&lt;/u&gt; Keogh and Yi proposed the method of the piecewise aggregate approximation independently. The algorithm divides the time series by the same time width and each sub-segment is represent by the average of the sub-segment. The method is simple, intuitionistic. It not only can support the similarity queries, all the Minkowski metric and the weighted Euclidean distance, but also can be used to index to improve query efficiency.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation based on the characteristic points:&lt;/b&gt;&lt;/u&gt; Xiao  proposed a method of piecewise linear representation based on the characteristic points. After extracting the characteristic points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation Based on Slope Extract Edge Point (SEEP)&lt;/b&gt;&lt;/u&gt;:** ZHAN Yan-Yan brought forward a new piecewise linear representation combining slope with the characteristics of time series. The algorithm can select some change points according to the rate of slope change firstly, and then combines the points with the line sequentially. Finally it will generate a new time series.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The literatures above are analyzed as follows:&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;The piecewise linear representation gets some characteristics (e.g., extreme point, trend, etc.) of each section by segmenting the series mainly. The above methods not only have the advantages of simple and intuitive, but also can support dynamic incremental updates, clustering, fast similarity search, and so on. But the cost and fitting error is different.
&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;&lt;u&gt;&lt;b&gt;Piecewise Linear Representation of Time Series based on Slope Change Threshold (SCT):&lt;/b&gt;&lt;/u&gt;
Firstly, the algorithm calculates the two segments’ slope of the certain point connecting with the
two adjacent points(except the two endpoints of time series).Secondly, it determines the change points by the ratio of slope. And then it combines the points with the line orderly. In this way, a new time series arises.&lt;/p&gt;

&lt;p&gt;The key of the algorithm is determining the change points. The change points must follow the
following principles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first point and last point are both change point;&lt;/li&gt;
  &lt;li&gt;When the slope of the line combining the certain point with its left neighboring point is zero, we
look on the point as change point if the slope of the line combining the certain point with its right
neighboring point is out the range of$(-d,+d);$&lt;/li&gt;
  &lt;li&gt;When the slope of the line combining the certain point with its left neighboring point is not zero, we look on the point as change point if the slope ratio of two lines is beyond the range of $(1-d,1+d).$ The two lines refer to the line which combines the certain point with its right neighboring point and the line which combines the certain point with its left neighboring point. Above $d$ is a threshold parameter.&lt;/li&gt;
&lt;/ol&gt;

</content>
 </entry>
 
 <entry>
   <title>ICA</title>
   <link href="http://localhost:4000/2017/04/06/blind-source-seperation-ica"/>
   <updated>2017-04-06T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/06/blind-source-seperation-ica</id>
   <content type="html">&lt;p&gt;The main idea can be briefly expressed by the following mixed model:
\(x(t) = \mathbf{A} s(t) + n(t)\)
The statistical model in the above equation is called ICA model, which describes how the observed data are mixed through the components $s(t)$ . The $m$ dimension column vector $x(t)$ is the observed data. $\mathbf{A}$ is a $m \times n$ mixing matrix; $n(t)$ denotes the additive noise vector. The matrix $\mathbf{A}$ is assumed to be unknown. All we observe is the random vector x(t) , and we must estimate both Aand s(t) . Since $\mathbf{A}$ is unknown, so $s(t)$ seems to be unsolvable.&lt;/p&gt;

&lt;p&gt;Fortunately, there are many mathematical methods for calculating the coefficients of $\mathbf{A}$ by requiring the High-Order Statistics (HOS) information during the search for independent
components.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Extreme Point Symmetric Mode Decomposition</title>
   <link href="http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition"/>
   <updated>2017-04-05T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition</id>
   <content type="html">&lt;p align=&quot;justify&quot;&gt;Considering the extraction methods of trend item, including the difference method, average slope method, moving average method, low pass filtering method, and least square fitting method, the type of trend term often needs to be presupposed. These methods are not suitable for processing the nonstationary signals with complex or random change trends.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;According to previous studies, the wavelet transform-based method is required for preselecting the wavelet basis and decomposition levels. This method is influenced easily by
artificial factors and has no self-adaptability. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;The method based on Empirical mode decomposition (EMD) can adaptively decompose non-stationary signals regardless of the type of trend term. But, EMD is affected by mode mixing and
end effect, causing the decomposed trend function is rough and the extraction accuracy is restricted. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Professor Wang et al. recently proposed a self-adaptive method called ESMD. The ESMD is a novel development derived from Hilbert Huang transform that can be used to process non-stationary signal. ESMD has been applied to many studies.&lt;/p&gt;

&lt;h2 id=&quot;esmd&quot;&gt;ESMD&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; Identify all local extreme points, including maxima and minima points, of the data $Y$. Mark them as $E_{i} (1 ≤ i ≤ n);$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; Connect all adjacent Ei with line segments, and mark their midpoints as $F_{i} (1 ≤ i ≤ n-1);$&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; Add left and right boundary midpoints $F_{0}$ and $F_{n}$ using linear interpolation method;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; Construct $p$ interpolating curves $L_{1}, L_{2},\dots L_{p} (p≥1)$ with all $n+1$ midpoints and calculate their mean value by using equation $L^{\ast}$.&lt;/li&gt;
&lt;/ul&gt;

\[L^{*}= \frac{L_{1}+L_{2}+\dots + L_{p}}{p}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Step 5:&lt;/strong&gt; Repeat steps $1$ to $4$ on $Y - L^{\ast}$ until $||L^{\ast}|| \le \epsilon$ ($\epsilon$ is a permitted error), or until the sifting times attain a preset maximum number $K$. Then, the first mode $M_{1}$ is obtained.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 6:&lt;/strong&gt; Repeat steps $1$ to $5$ on the residual $Y - M_1$ and obtain $M_2, M_3 \dots$ until the last residual $R$ with no more than a certain number of extreme points.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 7:&lt;/strong&gt; Change the maximum number $K$ on a finite integer interval $[K_{min}, K_{max}],$ and repeat all previous steps. Calculate the variance $\sigma^2$ of $Y - R$ and plot a figure with $\frac{\sigma}{\sigma_{0}}$ and $K$, $\sigma_{0}$ is the standard deviation of $Y$.&lt;/li&gt;
&lt;/ul&gt;

\[\sigma^2 = \frac{1}{N}\sum_{i=1}^N(y_{i}-r_{i})^2\]

\[\sigma_{0}^2 = \frac{1}{N}\sum_{i=1}^N(y_{i}-\bar{Y})^2\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Step 8 :&lt;/strong&gt; Identify the number $K_0$ which corresponds to the minimum  $\frac{\sigma}{\sigma_{0}}$  in $[K_{min}, K_{max}].$  Use this $K_{0}$ to repeat steps $1$ to $6$ and obtain the whole modes. Then last residual $R$ is an optimal Adaptive global mean (AGM) curve.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the steps above, ESMD can decompose signal into limited intrinsic mode functions and a residual component. The residual component $R$ is an optimal AGM curve that can be considered as the trend term of the original signal.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;The extraction error of EMD is larger than that of ESMD. &lt;/u&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The EMD-based extraction method can adaptively extract the signal trend. Whereas, the extracted trend curve limits the number of its extreme points (no more than 1), and no optimal strategy to find it. Therefore, the extraction error of EMD is relatively larger than that of ESMD.&lt;/li&gt;
  &lt;li&gt;The ESMD-based extraction method has commendable self-adaptability. It can obtain the signal trend with high precision using adaptive decomposition and optimization. The trend type of signal does not need to be preset. And, the extraction results of ESMD are better than that of EMD.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Reference: Adaptive extraction method for trend term of machinery signal based on
extreme-point symmetric mode decomposition - Yong Zhu, Wan-lu Jiang and Xiang-dong Kong&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>PCA ICA for 1D timeseries</title>
   <link href="http://localhost:4000/2017/04/03/pca-ica-1d-timeseries"/>
   <updated>2017-04-03T00:00:00+01:00</updated>
   <id>http://localhost:4000/2017/04/03/PCA-ICA-1d-timeseries</id>
   <content type="html">&lt;h3 id=&quot;idea&quot;&gt;Idea&lt;/h3&gt;
&lt;p align=&quot;justify&quot;&gt;Principal component analysis (PCA) isa method that transforms multiple data series into uncorrelated data series. Independent component analysis &lt;a class=&quot;internal-link&quot; href=&quot;/2017/04/06/blind-source-seperation-ica&quot;&gt;ICA&lt;/a&gt; is a method that separates multiple data series into independent data series. However, both require signals from at least two separate sensors. To overcome this requirement and utilize the fault detection capability of ICA and PCA, we propose to use wavelet transform to pre-process the data collected from a single sensor and then use the coefficients of the wavelet transforms at different scales as input to ICA and PCA. &lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Independent components analysis (ICA) requires little prior knowledge about the components to be isolated; however, at least two sensors must be available for signal collection and the number of sensors must be at least equal to the number of sources to be separated and this method cannot be applied directly when there is only one sensor collecting signals.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;Principal component analysis (PCA) is a multivariate data analysis technique that transforms a set of correlated variables into a set of uncorrelated variables. Each member of the resulting set of uncorrelated variables is called a principal component. We are interested in determining its suitability for fault detection because one of the identified principal components may reveal the signature of a hidden fault. As with ICA, however, this method cannot be applied directly when only a single variable is observed.&lt;/p&gt;

&lt;p&gt;Wavelet transform may be considered as a series of band pass filters when applied to the data
collected from a single sensor. The results of the transform, which exist in different frequency
regions, say $N$ regions, may be considered as different mixtures of the sources that have
generated the collected signals.These $N$ groups of data can then be used as input to ICA or PCA
for identification of the hidden sources.&lt;/p&gt;

&lt;h3 id=&quot;ica-and-pca&quot;&gt;ICA and PCA&lt;/h3&gt;

&lt;p&gt;ICA is a technique for separating independent sources linearly mixed in signals. Suppose that
there are $N$ independent sources of vibration, and $N$ sensors at different locations are used to
record vibration signals. The signals recorded by each sensor come from different sources with
different mixing ratios. Let $s_{1}(t),s_{2}(t),  \dots ,s_{N}(t)$ be the signals produced by the $N$ independent
sources and $x_{1}(t),x_{2}(t),  \dots ,x_{N}(t)$ be the observations from the $N$ sensors. The sensors record these signals simultaneously. The task of ICA is to estimate the mixing ratios of the source signals in the collected signals and obtain the independent source signals.&lt;/p&gt;

&lt;p align=&quot;justify&quot;&gt;To identify the independent components successfully, we need a rule for evaluating the
independency of the identified components. According to the Central Limit Theorem, the
distribution of the sum of a large number of independent random variables tends to a Gaussian
distribution. Since the collected signals are weighted sums of the independent sources, the sources
to be isolated must have less Gaussianity than the collected signals. Thus, non-Gaussianity can be
used for separating independent components. Hyvarinen and Oja proposed to use negentropy
to evaluate the non-Gaussianity of the separated components so as to evaluate separation
performance. With this concept, we can seek the separation that provides the least Gaussianness of the separated components. The popular FastICA algorithm proposed by Havarinen and Oja
isoften used to carry out the ICA procedure.&lt;/p&gt;

&lt;p&gt;PCA is a technique that obtains linear transformations of a group of correlated variables such
that the transformed variables are uncorrelated. For example, consider two variables, $x_{1}$ and
$x_{2}$. For each variable, we have obtained the following $N$ observations:
\(x_{11}, x_{12}, \dots , x_{1N}; x_{21}, x_{22}, \dots; x_{2N}\)
where $x_{1i}$ and $x_{2j}$ denote the $i^{th}$ and the $j^{th}$ observations of variables $x_{1}$ and $x_{2}$, respectively. The PCA method seeks two new axes, D1 and D2, that make the projectionsof the collected data onto
D1 have the largest variability and at the same time, the projections of the collected data onto D2
have the smallest variability. This way, we have expressed the collected data as their two principal
components. Most of the variation in the original data is explained by the first principal
component, D1, and the remaining variation in the original data isexplained by the second
principal component, D2.&lt;/p&gt;
&lt;p align=&quot;justify&quot;&gt;
ICA renders the separated components independent of one another while PCA rendersthe
separated components uncorrelated with one another. PCA separates the components based only
on the second-order cumulant while ICA separates the components on high-order cumulants.
Therefore, ICA can be considered a generalization of PCA.&lt;/p&gt;

&lt;h3 id=&quot;method-of-preprocessing-to-apply-ica-or-pca-on-1d-time-series&quot;&gt;Method of preprocessing to apply ICA or PCA on 1D Time series&lt;/h3&gt;

&lt;p&gt;The available data is a single time series. To apply ICA or PCA for feature extraction, we need
to have more than one time series. A method to generate multiple time series from the single available time series is given below.&lt;/p&gt;

&lt;p&gt;Wavelet transform decomposes a signal series in the time domain into a two-dimensional
function in the time-scale (frequency) plane. The wavelet coefficients measure the time-scale (frequency) content in a signal indexed by the scale parameter and the translation parameter. Let
$\varphi(t)$ be the mother wavelet. The wavelet family consists of a series of daughter wavelets that are
generated by dilation and translation from the mother wavelet $\varphi(t)$&lt;/p&gt;

\[\varphi_{a, b}(t)=\sqrt{|a|} \varphi[(t-b) / a]\]

&lt;p&gt;where $a$ is the scale parameter, $b$ isthe location parameter, and $\sqrt{  |a|}$
isused to guarantee energy preservation. The wavelet transform of signal $x(t)$ isdefined as the inner product of $\varphi_{a, b}(t)$ and $x(t)$ in the Hilbert space of $L^2$ norm defined as:&lt;/p&gt;

\[W(a, b)=\left\langle\varphi_{a, b}(t), x(t)\right\rangle=\int x(t) \varphi_{a, b}^*(t) \mathrm{d} t\]

&lt;p&gt;where the symbol * stands for the complex conjugate.&lt;/p&gt;

&lt;p&gt;Wavelet transform can be thought of as a series of band pass filters. The results of the
transform, which exist in different frequency regions, may be thought of as different mixtures of the independent sources. These different mixtures may be considered to be signals collected at
different ‘‘locations’’, or more accurately, through different ‘‘sensors’’ with different frequency
ranges. This way, the one-dimensional signal is transformed into multidimensional data
that satisfy the requirements of ICA and PCA. The preprocessing of the one-dimensional data with wavelet transform makes ICA and PCA usable for identification of a hidden
source.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/snips/img1.png&quot; alt=&quot;Flowchart&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;reference-&quot;&gt;Reference :&lt;/h3&gt;
&lt;p&gt;Feature separation using ICA for a one-dimensional time series and its application in fault detection - Ming J. Zuo, Jing Lin, Xianfeng Fan&lt;/p&gt;
</content>
 </entry>
 

</feed>
