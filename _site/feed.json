{
    "version": "https://jsonfeed.org/version/1",
    "title": "Pavan Donthireddy",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Digital space of notes and thoughts",
    "icon": "http://localhost:4000/apple-touch-iconfavicon.png",
    "favicon": "http://localhost:4000/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "Pavan Donthireddy",
        "url": "https://pavandonthireddy.github.io",
        "avatar": null
    },
    
"items": [
    
        {
            "id": "http://localhost:4000/2017/04/13/ofir-filter",
            "title": "Optimal Finite Impulse Response (OFIR) Filter",
            "summary": "Optimal Finite Impulse Response (OFIR) Filter",
            "content_text": "The Kalman filter (KF) is themost widely used real-time optimal estimator. However, the KF is aBayesian estimator and its recursive algorithm has the infinite impulseresponse (IIR), owing to which the KF often suffers of insufficient robustness. Better robustness is inherent to finite memory filters and to filters with finite impulse response (FIR).Unlike the KF, the FIR filter utilizes measurements on an interval ofN most recent neighbouring points called horizon. Compared to the KF,FIR filters demonstrate many useful properties such as the bound input/bound output (BIBO) stability, higher robustness against temporarymodel uncertainties and round-off errors , andlower sensitivity to noise.PreliminariesConsider a general class of discrete-time linear systems represented in state-space with time-variant coefficients as\\[x_{k}=A_{k} x_{k-1}+B_{k} w_{k} \\tag{1}\\]\\(y_{k}=C_{k} x_{k}+v_{k}\\tag{2}\\)in which $k$ is the discrete time index, $x_{k} \\in \\mathbb{R}^{n}$ is the state vector, $y_{k} \\in \\mathbb{R}^{p}$ is the measurement vector, and $A_{k} \\in \\mathbb{R}^{n \\times n}, B_{k} \\in \\mathbb{R}^{n \\times u}$, and $C_{k} \\in \\mathbb{R}^{p \\times n}$ are time-variant matrices. Here, $w_{k} \\in \\mathbb{R}^{u}$ and $v_{k} \\in \\mathbb{R}^{p}$ are additive process and measurement noise sources with known covariances$Q_k=\\E ( w_k w_{k}^{T} ) $ and $R_{k}=\\mathbb{E}( v_{k} v_{k}^{T})$, respectively. We suppose that $w_{k}$ and $v_{k}$ are zero mean, white, and mutually uncorrelated; that is, $\\mathbb{E}( w_{k})=0$, $\\mathbb{E}( v_{k})=0, \\mathbb{E}(w_{k} w_{j}^{T})=0$ and $\\mathbb{E}( v_{k} v_{j}^{T})=0$ for all $k$ and $j \\neq k$, and $\\mathbb{E}( w_{k} v_{j}^{T})=0$ for all $k$ and $j$.The FIR filter requires simultaneously $N$ data points taken from the horizon $[l=k-N+1, k]$. Therefore, (1) and (2) need to be extended on $[l, k]$. That can be done if to use the recursively computed forward in-time solutions and write\\[X_{k, l}=A_{k, l} x_{l}+B_{k, l} W_{k, l}\\tag{3}\\]\\[Y_{k, l}=C_{k, l} x_{l}+H_{k, l} W_{k, l}+V_{k, l} \\tag{5}\\]where the extended vectors are $X_{k, l}=\\left[x_{k}^{T}, x_{k-1}^{T}, \\ldots, x_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N n \\times 1}$, $Y_{k, l}=\\left[y_{k}^{T}, y_{k-1}^{T}, \\ldots, y_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N p \\times 1}, W_{k, l}=\\left[w_{k}^{T}, w_{k-1}^{T}, \\ldots, w_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N u \\times 1}$, and $V_{k, l}=\\left[v_{k}^{T}, v_{k-1}^{T}, \\ldots, v_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N p \\times 1}$. The extended $k$ - and $N$-variant matrices $A_{k, l} \\in \\mathbb{R}^{N n \\times n}, B_{k, l} \\in \\mathbb{R}^{N n \\times N u}, C_{k, l} \\in \\mathbb{R}^{N p \\times n}$, and $H_{k, l} \\in \\mathbb{R}^{N p \\times N u}$ can be represented as, respectively,\\(A_{k, l}=\\left[\\mathscr{A}_{k, l+1}^{T}, \\mathscr{A}_{k-1, l+1}^{T}, \\ldots, \\mathscr{A}_{l+1, l+1}^{T}, I\\right]^{T}\\)\\(B_{k, l}=\\left[\\begin{array}{ccccc}B_{k} &amp; \\mathscr{A}_{k, k} B_{k-1} &amp; \\cdots &amp; \\mathbf{A}_{k, l+2} B_{l+1} &amp; \\mathscr{A}_{k, l+1} B_{l} \\\\ 0 &amp; B_{k-1} &amp; \\cdots &amp; \\mathscr{A}_{k-1, l+2} B_{l+1} &amp; \\mathscr{A}_{k-1, l+1} B_{l} \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; B_{l+1} &amp; \\mathscr{A}_{l+1, l+1} B_{l} \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; B_{l}\\end{array}\\right]\\),\\[C_{k, l}=\\bar{C}_{k, l} A_{k, l}\\]\\[H_{k, l}=\\bar{C}_{k, l} B_{k, l}\\]where\\[\\bar{C}_{k, l}=\\operatorname{diag}\\left(C_{k} C_{k-1} \\cdots C_{l}\\right)\\]\\[\\mathbf{A}_{i, j}=\\prod_{r=0}^{i-j} A_{i-r}=A_{i} A_{i-1} \\ldots A_{j}\\]At the initial horizon point, (3) becomes $x_{l}=x_{l}+B_{l} w_{l}$ that is uniquely satisfied if $w_{l}$ is zero-valued, provided that $B_{l}$ is not zeroth. That means that the initial state must be known in advance or estimated optimally.The FIR filtering estimate can be obtained at $k$ via (4) using the discrete convolution as\\[\\hat{x}_{k \\mid k}=K_{k} Y_{k, l} \\tag{5}\\]where $x_{t \\vert r} $ means the estimate at $t$ via measurements from the past to and including at $r$ and $K_{k}$ is the FIR filter gain, which needs to be defined to obey some cost function. Note that the aforementioned inherent properties of FIR filtering are associated with the fact that measurements prior to $l$ are discarded in (5) and thus do not affect the estimate unlike in the KF which has IIR. It is also necessary to emphasize that when the system considered is time-invariant, the FIR estimate (5) will becomes $x_{k \\mid k}=K_{N} Y_{k, l}$, which means that the filter gain $K_{N}$ is time-invariant and can be determined off-line once the horizon length $N$ is available. In this case, $K_{N}$ is not necessarily to be realized into iterative computation structure.The optimal gain $K_{k}$ can be obtained for (5) in the minimum MSE sense by minimizing the trace of the MSE as\\[\\hat{K}_{k}=\\underset{K_{k}}{\\arg \\min } E\\left\\{\\operatorname{tr}\\left(e_{k} e_{k}^{T}\\right)\\right\\}\\]where $e_{k}=x_{k}-x_{k \\mid k}$ is the estimation error. Provided $x_{k \\mid k}$ via (5), the one-step prediction required by feedback control and associated with receding horizon filtering can be formed as $x_{k+1 \\mid k}=A_{k+1} x_{k \\mid k}$, similarly to the KF.OFIR algorithmGiven the model (1) and (2) with white and mutually uncorrelated noise processes $w_{k}$ and $v_{k}$ which have covariances $Q_{k}$ and $R_{k}$, respectively. The iterative form for OFIR estimate (10) with gain (16) is the following,\\[\\begin{aligned}\\Xi_{i}= &amp; A_{i} \\Xi_{i-1} A_{i}^{T}+B_{i} Q_{i} B_{i}^{T}-A_{i} \\Xi_{i-1} C_{i-1}^{T} \\\\&amp; \\times\\left(R_{i-1}+C_{i-1} \\Xi_{i-1} C_{i-1}^{T}\\right)^{-1} C_{i-1} \\Xi_{i-1} A_{i}^{T} \\\\G_{i}= &amp; \\Xi_{i} C_{i}^{T}\\left(R_{i}+C_{i} \\Xi_{i} C_{i}^{T}\\right)^{-1}\\end{aligned} \\tag{6}\\]\\[\\hat{x}_{i \\mid i}=A_{i} \\hat{x}_{i-1 \\mid i-1}+G_{i}\\left(y_{i}-C_{i} A_{i} \\hat{x}_{i-1 \\mid i-1}\\right)\\tag{7}\\]where $i$ ranges from $l+1$ to $k$ and the output is taken when $i=k$. The initial state $x_{l \\vert l}$ is given and the initial prior error $\\Xi_{i}$ is provided at l by$\\Xi_{l}=\\Theta_{x, l}+B_{l} Q_{l} B_{l}^{T}$where $\\Theta_{x, l}$ is given.As can be seen (7), is the Kalman-like recursion in which $A_{i} x_{i-1 \\mid i-1}$ predicts the state from $i-1$ to $i$ and the bias correction gain (6) corrects the prediction for the residual. Although the KF and OFIR filter both minimize the MSE, $G_{i}$ is not the Kalman gain, because the KF has IIR. However, an increase in the horizon length $N$ reduces the estimation error and makes it such that the OFIR estimate converges to the KF estimate: the estimates become practically equal when $N&gt;N_{\\mathrm{opt}}$ . In this sense, the KF can be considered as a special case of a more general OFIR filter when $N=\\infty$, provided the initial conditions. Another difference is that $N$ measurements are processed by the OFIR filter simultaneously at each time index $k$, while only one measurement is processed by the KF at $k$. That means that the computational complexity of OFIR filter $\\mathcal{O}(N)$ is $N$ times larger than $\\mathcal{O}(1)$ of the KF. On the other hand, the iterative algorithm reduces essentially the computational complexity $\\mathcal{O}\\left(N^{2}\\right)$ of the batch OFIR form.",
            "content_html": "<p align=\"justify\">The Kalman filter (KF) is themost widely used real-time optimal estimator. However, the KF is aBayesian estimator and its recursive algorithm has the infinite impulseresponse (IIR), owing to which the KF often suffers of insufficient robustness. Better robustness is inherent to finite memory filters and to filters with finite impulse response (FIR).</p><p>Unlike the KF, the FIR filter utilizes measurements on an interval ofN most recent neighbouring points called horizon. Compared to the KF,FIR filters demonstrate many useful properties such as the bound input/bound output (BIBO) stability, higher robustness against temporarymodel uncertainties and round-off errors , andlower sensitivity to noise.</p><h3 id=\"preliminaries\">Preliminaries</h3><p>Consider a general class of discrete-time linear systems represented in state-space with time-variant coefficients as</p>\\[x_{k}=A_{k} x_{k-1}+B_{k} w_{k} \\tag{1}\\]<p>\\(y_{k}=C_{k} x_{k}+v_{k}\\tag{2}\\)in which $k$ is the discrete time index, $x_{k} \\in \\mathbb{R}^{n}$ is the state vector, $y_{k} \\in \\mathbb{R}^{p}$ is the measurement vector, and $A_{k} \\in \\mathbb{R}^{n \\times n}, B_{k} \\in \\mathbb{R}^{n \\times u}$, and $C_{k} \\in \\mathbb{R}^{p \\times n}$ are time-variant matrices. Here, $w_{k} \\in \\mathbb{R}^{u}$ and $v_{k} \\in \\mathbb{R}^{p}$ are additive process and measurement noise sources with known covariances</p><p>$Q_k=\\E ( w_k w_{k}^{T} ) $ and $R_{k}=\\mathbb{E}( v_{k} v_{k}^{T})$, respectively. We suppose that $w_{k}$ and $v_{k}$ are zero mean, white, and mutually uncorrelated; that is, $\\mathbb{E}( w_{k})=0$, $\\mathbb{E}( v_{k})=0, \\mathbb{E}(w_{k} w_{j}^{T})=0$ and $\\mathbb{E}( v_{k} v_{j}^{T})=0$ for all $k$ and $j \\neq k$, and $\\mathbb{E}( w_{k} v_{j}^{T})=0$ for all $k$ and $j$.</p><p>The FIR filter requires simultaneously $N$ data points taken from the horizon $[l=k-N+1, k]$. Therefore, (1) and (2) need to be extended on $[l, k]$. That can be done if to use the recursively computed forward in-time solutions and write</p>\\[X_{k, l}=A_{k, l} x_{l}+B_{k, l} W_{k, l}\\tag{3}\\]\\[Y_{k, l}=C_{k, l} x_{l}+H_{k, l} W_{k, l}+V_{k, l} \\tag{5}\\]<p>where the extended vectors are $X_{k, l}=\\left[x_{k}^{T}, x_{k-1}^{T}, \\ldots, x_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N n \\times 1}$, $Y_{k, l}=\\left[y_{k}^{T}, y_{k-1}^{T}, \\ldots, y_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N p \\times 1}, W_{k, l}=\\left[w_{k}^{T}, w_{k-1}^{T}, \\ldots, w_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N u \\times 1}$, and $V_{k, l}=\\left[v_{k}^{T}, v_{k-1}^{T}, \\ldots, v_{l}^{T}\\right]^{T} \\in \\mathbb{R}^{N p \\times 1}$. The extended $k$ - and $N$-variant matrices $A_{k, l} \\in \\mathbb{R}^{N n \\times n}, B_{k, l} \\in \\mathbb{R}^{N n \\times N u}, C_{k, l} \\in \\mathbb{R}^{N p \\times n}$, and $H_{k, l} \\in \\mathbb{R}^{N p \\times N u}$ can be represented as, respectively,</p><p>\\(A_{k, l}=\\left[\\mathscr{A}_{k, l+1}^{T}, \\mathscr{A}_{k-1, l+1}^{T}, \\ldots, \\mathscr{A}_{l+1, l+1}^{T}, I\\right]^{T}\\)\\(B_{k, l}=\\left[\\begin{array}{ccccc}B_{k} &amp; \\mathscr{A}_{k, k} B_{k-1} &amp; \\cdots &amp; \\mathbf{A}_{k, l+2} B_{l+1} &amp; \\mathscr{A}_{k, l+1} B_{l} \\\\ 0 &amp; B_{k-1} &amp; \\cdots &amp; \\mathscr{A}_{k-1, l+2} B_{l+1} &amp; \\mathscr{A}_{k-1, l+1} B_{l} \\\\ \\vdots &amp; \\vdots &amp; \\cdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; B_{l+1} &amp; \\mathscr{A}_{l+1, l+1} B_{l} \\\\ 0 &amp; 0 &amp; \\cdots &amp; 0 &amp; B_{l}\\end{array}\\right]\\),</p>\\[C_{k, l}=\\bar{C}_{k, l} A_{k, l}\\]\\[H_{k, l}=\\bar{C}_{k, l} B_{k, l}\\]<p>where</p>\\[\\bar{C}_{k, l}=\\operatorname{diag}\\left(C_{k} C_{k-1} \\cdots C_{l}\\right)\\]\\[\\mathbf{A}_{i, j}=\\prod_{r=0}^{i-j} A_{i-r}=A_{i} A_{i-1} \\ldots A_{j}\\]<p>At the initial horizon point, (3) becomes $x_{l}=x_{l}+B_{l} w_{l}$ that is uniquely satisfied if $w_{l}$ is zero-valued, provided that $B_{l}$ is not zeroth. That means that the initial state must be known in advance or estimated optimally.</p><p>The FIR filtering estimate can be obtained at $k$ via (4) using the discrete convolution as</p>\\[\\hat{x}_{k \\mid k}=K_{k} Y_{k, l} \\tag{5}\\]<p>where $x_{t \\vert r} $ means the estimate at $t$ via measurements from the past to and including at $r$ and $K_{k}$ is the FIR filter gain, which needs to be defined to obey some cost function. Note that the aforementioned inherent properties of FIR filtering are associated with the fact that measurements prior to $l$ are discarded in (5) and thus do not affect the estimate unlike in the KF which has IIR. It is also necessary to emphasize that when the system considered is time-invariant, the FIR estimate (5) will becomes $x_{k \\mid k}=K_{N} Y_{k, l}$, which means that the filter gain $K_{N}$ is time-invariant and can be determined off-line once the horizon length $N$ is available. In this case, $K_{N}$ is not necessarily to be realized into iterative computation structure.</p><p>The optimal gain $K_{k}$ can be obtained for (5) in the minimum MSE sense by minimizing the trace of the MSE as</p>\\[\\hat{K}_{k}=\\underset{K_{k}}{\\arg \\min } E\\left\\{\\operatorname{tr}\\left(e_{k} e_{k}^{T}\\right)\\right\\}\\]<p>where $e_{k}=x_{k}-x_{k \\mid k}$ is the estimation error. Provided $x_{k \\mid k}$ via (5), the one-step prediction required by feedback control and associated with receding horizon filtering can be formed as $x_{k+1 \\mid k}=A_{k+1} x_{k \\mid k}$, similarly to the KF.</p><h3 id=\"ofir-algorithm\">OFIR algorithm</h3><p>Given the model (1) and (2) with white and mutually uncorrelated noise processes $w_{k}$ and $v_{k}$ which have covariances $Q_{k}$ and $R_{k}$, respectively. The iterative form for OFIR estimate (10) with gain (16) is the following,</p>\\[\\begin{aligned}\\Xi_{i}= &amp; A_{i} \\Xi_{i-1} A_{i}^{T}+B_{i} Q_{i} B_{i}^{T}-A_{i} \\Xi_{i-1} C_{i-1}^{T} \\\\&amp; \\times\\left(R_{i-1}+C_{i-1} \\Xi_{i-1} C_{i-1}^{T}\\right)^{-1} C_{i-1} \\Xi_{i-1} A_{i}^{T} \\\\G_{i}= &amp; \\Xi_{i} C_{i}^{T}\\left(R_{i}+C_{i} \\Xi_{i} C_{i}^{T}\\right)^{-1}\\end{aligned} \\tag{6}\\]\\[\\hat{x}_{i \\mid i}=A_{i} \\hat{x}_{i-1 \\mid i-1}+G_{i}\\left(y_{i}-C_{i} A_{i} \\hat{x}_{i-1 \\mid i-1}\\right)\\tag{7}\\]<p>where $i$ ranges from $l+1$ to $k$ and the output is taken when $i=k$. The initial state $x_{l \\vert l}$ is given and the initial prior error $\\Xi_{i}$ is provided at l by</p><p>$\\Xi_{l}=\\Theta_{x, l}+B_{l} Q_{l} B_{l}^{T}$</p><p>where $\\Theta_{x, l}$ is given.</p><p>As can be seen (7), is the Kalman-like recursion in which $A_{i} x_{i-1 \\mid i-1}$ predicts the state from $i-1$ to $i$ and the bias correction gain (6) corrects the prediction for the residual. Although the KF and OFIR filter both minimize the MSE, $G_{i}$ is not the Kalman gain, because the KF has IIR. However, an increase in the horizon length $N$ reduces the estimation error and makes it such that the OFIR estimate converges to the KF estimate: the estimates become practically equal when $N&gt;N_{\\mathrm{opt}}$ . In this sense, the KF can be considered as a special case of a more general OFIR filter when $N=\\infty$, provided the initial conditions. Another difference is that $N$ measurements are processed by the OFIR filter simultaneously at each time index $k$, while only one measurement is processed by the KF at $k$. That means that the computational complexity of OFIR filter $\\mathcal{O}(N)$ is $N$ times larger than $\\mathcal{O}(1)$ of the KF. On the other hand, the iterative algorithm reduces essentially the computational complexity $\\mathcal{O}\\left(N^{2}\\right)$ of the batch OFIR form.</p>",
            "url": "http://localhost:4000/2017/04/13/ofir-filter",
            
            
            
            "tags": ["filters","OFIR","FIR","IIR","kalman"],
            
            "date_published": "2017-04-13T00:00:00+01:00",
            "date_modified": "2017-04-13T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/12/hht",
            "title": "Hilbert Huang Transform",
            "summary": "Hilbert Huang Transform",
            "content_text": "The EMD method is necessary to reduce any data from non-stationary and nonlinear processes into simple oscillatory function that will yield meaningful instantaneous frequency through the Hilbert transform. Contrary to almost all the previous decomposing methods, EMD is empirical, intuitive, direct, and adaptive, with the a posteriori defined basis derived from the data. The decomposition is designed to seek the different simple intrinsic modes of oscillations in any data based on the principle of scale separation. The data, depending on it complexity, may have many different coexisting modes of oscillation at the same time. Each of these oscillatory modes is represented by an Intrinsic Mode Function (IMF) with the following definitions:(a) in the whole data set, the number of extrema and the number of zero-crossings must either equal or differ at most by one, and(b) at any point, the mean value of the envelope defined by the local maxima and the envelope defined by the local minima is zero.The IMF is a counter part to the simple harmonic function, but it is much more general: instead of constant amplitude and frequency, IMF can have both variable amplitude and frequency as functions of time. This definition is inspired by the simple example of constant plus sinusoidal function given above. The total number of the IMF components is limited to $\\ln _{2} N$, where $\\boldsymbol{N}$ is the total number of data points. It satisfies all the requirements for a meaningful instantaneous frequency through Hilbert transform.Pursuant to the above definition for IMF, one can implement the needed decomposition of any function, known as sifting, as follows: Take the test data; identify all the local extrema; divide the extrema into two sets: the maxima and the minima. Then connect all the local maxima by a cubic spline line to form an upper envelope. Repeat the procedure for the local minima to form a lower envelope. The upper and lower envelopes should encompass all the data between them. Their mean is designated as $m_1$, and the difference between the data and $m_1$ is designated as, $h_1$, a proto-IMF:\\[X(t)-m_{1}=h_{1}\\]Ideally, $h_1$ should satisfy the definition of an IMF by construction of $h_1$ described above, which should have made it symmetric and having all maxima positive and all minima negative. Yet, in changing the local zero from a rectangular to a curvilinear coordinate system some inflection points could become additional extrema. New extrema generated this way actually reveal the hidden modes missed in the initial treatment. The sifting process sometimes can recover signals representing low amplitude riding waves with repeated siftings.The sifting process serves two purposes: to eliminate riding waves and to make the wave profiles more symmetric. While the first condition is absolute necessary for Hilbert transform to give a meaningful instantaneous frequency, the second condition is also necessary in case the neighboring wave amplitudes having too large a disparity. As a result, the sifting process has to be repeated many times to reduce the extracted signal an IMF. In the subsequent sifting process, $h_1$ is treated as the data for the next round of sifting; therefore,\\[h_{1}-m_{11}=h_{11}\\]After repeated sifting, up to $\\mathrm{k}$ times, $h_{1k}$ :\\[h_{1(k-1)}-m_{1 k}=h_{1 k} \\text {. }\\]If $\\boldsymbol{h}_{\\boldsymbol{1} \\boldsymbol{k}}$ becomes an IMF, it is designated as $C_1$ :\\[C_{1}=h_{1 k}\\]the first IMF component from the data. Here one has a critical decision to make: when to stop. Too many rounds of sifting will reduce the IMF to FM page criterion; too few rounds of sifting will not have a valid IMF. In the past, different criteria have been used, including Cauchy type criterion (Huang et al. 19980), $\\boldsymbol{S}$-number criterion (Huang et al. 2003), fixed-number criterion (Wu and Huang 2004), and etc.With any stoppage criterion, the, $c_1$ should contain the finest scale or the shortest period component of the signal. one can, then, remove $c_1$ from the rest of the data by\\[X(t)-C_{1}=r_{1}\\]Since the residue, $r_1$, contains all longer period variations in the data, it is treated as the new data and subjected to the same sifting process as described above. This procedure can be repeated to all the subsequent $r_j$ ‘s, and the result is\\[\\begin{gathered}r_{1}-C_{2}=r_{2}, \\\\\\cdots \\\\r_{n-1}-C_{n}=r_{n}\\end{gathered}\\]The sifting process should stop when the residue, $\\boldsymbol{r}_{n}$, becomes a constant, a monotonic function, or a function contains only a single extrema, from which no more IMF can be extracted. By summing up Equations (16) and (17), we finally obtain\\[X(t)=\\sum_{j=1}^{n} C_{j}+r_{n}\\]Thus, sifting process produces a decomposition of the data into $\\boldsymbol{n}$-intrinsic modes, and a residue, $\\boldsymbol{r}_{\\boldsymbol{n}}$. When apply the EMD method, a mean or zero reference is not required; EMD needs only the locations of the local extrema. The sifting process generates the zero reference for each component. Without the need of the zero reference, EMD avoids the troublesome step of removing the mean values for the large non-zero mean.Two special notes here deserve our attention. First, the sifting process offered a way to circumvent the difficulty of define the local mean in a nonstationary time series, where no length scale exists for one to implement the traditional mean operation. The envelope mean employed here does not involve time scale; however, it is local. Second, the sifting process is a Reynolds-type decomposition: separating variations from the mean, except that the mean is a local instantaneous mean, so that the different modes are almost orthogonal to each other, except for the nonlinearity in the data.Recent studies by Flandrin et al. (2004) and Wu and Huang (2004) established that the EMD is equivalent to a dyadic filter bank, and it is also equivalent to an adaptive wavelet. Being adaptive, we have avoided the shortcomings of using any a priori-defined wavelet basis, and also avoided the spurious harmonics that would have resulted. The components of the EMD are usually physically meaningful, for the characteristic scales are defined by the physical data.Having established the decomposition, we can also identify a new use of the IMF components as filtering. Traditionally, filtering is carried out in frequency space only. But there is a great difficult in applying the frequency filtering when the data is either nonlinear or non-stationary or both, for both nonlinear and nonstationary data generate harmonics of all ranges. Therefore, any filtering will eliminate some of the harmonics, which will cause deformation of the data filtered. Using IMF, however, we can devise a time space filtering.For example, a low pass filtered results of a signal having $\\boldsymbol{n}$-IMF components can be simply expressed as\\[X_{l k}(t)=\\sum_{k}^{n} C_{j}+r_{n}\\]a high pass results can be expressed as\\[X_{h k}(t)=\\sum_{1}^{k} C_{j}\\]and a band pass result can be expressed as\\[X_{b k}(t)=\\sum_{b}^{k} C_{j}\\]The advantage of this time space filtering is that the results preserve the full nonlinearity and nonstationarity in the physical space.Having obtained the Intrinsic Mode Function components, one can compute the instantaneous frequency for each IMF component as the derivative of the phase function. And one can also designate the instantaneous amplitude from the Hilbert transform to each IMF component. Finally, the original data can be expressed as the real part, RP, of the sum of the data in terms of time, frequency and energy as:\\[X(t)=R P \\sum_{j=1}^{n} a_{j}(t) e^{i \\int \\omega_{j}(t) d t}\\]Above equation gives both amplitude and frequency of each component as a function of time. The same data, if expanded in a Fourier representation, would have a constant amplitude and frequency for each component.The contrast between EMD and Fourier decomposition is clear: The IMF represents a generalized Fourier expansion with a time varying function for amplitude and frequency. This frequency-time distribution of the amplitude is designated as the Hilbert Amplitude Spectrum, $\\boldsymbol{H}(\\boldsymbol{\\omega}, \\boldsymbol{t})$, or simply the Hilbert spectrum.From the Hilbert spectrum, we can also define the marginal spectrum, $\\boldsymbol{h}(\\boldsymbol{\\omega})$, as\\[h(\\omega)=\\int_{0}^{T} H(\\omega, t) d t\\]The marginal spectrum offers a measure of total amplitude (or energy) contribution from each frequency value. It represents the cumulated amplitude over the entire data span in a probabilistic sense.The combination of the Empirical Mode Decomposition and the Hilbert Spectral Analysis is designated by NASA as the Hilbert-Huang Transform (HHT) for short. Recent studies by various investigators indicate that HHT is a super tool for time-frequency analysis of nonlinear and nonstationary data (Huang and Attoh-Okine, 2005, Huang and Shen, 2005). It is based on an adaptive basis, and the frequency is defined through the Hilbert transform. Consequently, there is no need for the spurious harmonics to represent nonlinear waveform deformations as in any of the a priori basis methods, and there is no uncertainty principle limitation on time or frequency resolution from the convolution pairs based also on a priori bases. A summary of the comparison between Fourier, Wavelet and HHT analyses is given in Table 1.Table 1. Comparisons between Fourier, Wavelet and HilbertHuang Transform in Data analysis.            Transform      Fourier      Wavelet      Hilbert                  Basis      a priori      a priori      adaptive              Frequency      convolution: global, uncertainty      convolution: regional, uncertainty      differentiation: local, certainty              Presentation      energy-frequency      energy-time-frequency      energy-time-frequency              Nonlinear      no      no      yes              Non-stationary      no      yes      yes              Feature Extraction      no      discrete: no, continuous: yes      yes              Theoretical Base      theory complete      theory complete      empirical      After this basic development of the HHT method, there are some recent developments, which have either added insight to the results, enhanced the statistical significance of the results, and fixed some shortcomings in the HHT.Recent developmentsThe Normalized Hilbert Transform and the direct quadratureIt is well known that, although the Hilbert transform exists for any function of $\\boldsymbol{L}^{p}$ class, the phase function of the transformed function will not always yield physically meaningful instantaneous frequencies. The limitations have been summarized succinctly in two theorems.First, in order to separate the contribution of the phase variation into the phase and amplitude parts, the function have to satisfy the limitation stipulated in the Bedrosian theorem (1963), which states that the Hilbert transform for the product of two functions, $\\boldsymbol{f}(\\boldsymbol{t})$ and $\\boldsymbol{h}(\\boldsymbol{t})$, can be written as\\[H[f(t) h(t)]=f(t) H[h(t)]\\]only if the Fourier spectra for $\\boldsymbol{f}(\\boldsymbol{t})$ and $\\boldsymbol{h}(\\boldsymbol{t})$ are totally disjoint in frequency space, and the frequency content of the spectrum for $\\boldsymbol{h}(\\boldsymbol{t})$ is higher than that of $\\boldsymbol{f}(\\boldsymbol{t})$. This limitation is critical, for we need to have$H[a(t) \\cos \\theta(t)]=a(t) H[\\cos \\theta(t)]$, (28) otherwise, one cannot use Equation (6) to define the phase function, for the amplitude variation would mix with the phase function. Bedrosian theorem requires that the amplitude is varying be so slowly that the frequency spectra of the envelope and the carrier waves are disjoint. This is possible only for trivial cases, for unless the amplitude is constant, any local deviation can be considered as a sum of delta-functions, which has a wide white spectrum. Therefore, the spectrum for varying amplitude would never be totally separate from that of the carrier. This limitation has made the application of the Hilbert transform even to IMFs problematic. To satisfy this requirement, Huang and Long (2003) have proposed the normalization of the IMFs in the following steps: Starting from an IMF, they first find all the maxima of the IMFs, defining the envelope by spline through all the maxima, and designating the envelope as $\\boldsymbol{E}(\\boldsymbol{t})$. Now, normalize the IMF by dividing the IMF by $\\boldsymbol{E}(\\boldsymbol{t})$. Thus, they have the normalized function having amplitude always equal to unity, and have circumvented the limitation of Bedrosian theorem.Second, there is the new restriction given by the Nuttall theorem (1966), which stipulates that the Hilbert transform of cosine is not necessarily the sine with the same phase function for a cosine with an arbitrary phase function. Nuttall gave an energy based error bound, $\\boldsymbol{\\Delta E}$, defined as the difference between $y(t)$, the Hilbert transform of the data, and $\\boldsymbol{Q}(\\boldsymbol{t})$, the quadrature (with phase shift of exactly $90^{\\circ}$ ) of the function as\\[\\Delta E=\\int_{t=0}^{T}|y(t)-Q(t)|^{2} d t=\\int_{-\\infty}^{0} S_{q}(\\omega) d \\omega,\\]in which $\\boldsymbol{S}_{\\boldsymbol{q}}$ is Fourier spectrum of the quadrature function. Though the proof of this theorem is rigorous, the result is hardly useful, for it gives a constant error bound over the whole data range. With the normalized IMF, Huang and Long (2003) have proposed a variable error bound based on a simple argument, which goes as follows: compute the difference between squared amplitude of the normalized IMF and unity. If the Hilbert transform is exactly the quadrature, the difference between it and unity should be zero; otherwise, the Hilbert transform cannot be exactly the quadrature. Consequently, the error can be measured simply by the difference between the squared normalized IMF and unity, which is a function of time. Huang and Long (2003) and Huang et al. (2006) have conducted detailed comparisons and found the result quite satisfactory.Even with the error indicator, we can only know that the Hilbert transform is not exactly the quadrature; we still do not have the correct answer. This prompts a drastic alternative, eschewing the Hilbert transform totally. An exact direct quadrature has been found (Huang et al., 2006), and it would resolve the difficulties associated with the instantaneous frequency computation.The Confidence LimitThe confidence limit for the Fourier spectral analysis is based on the ergodic theory, where the temporal average is treated as the ensemble average. This approach is only valid if the processes are stationary. Huang et al. (2003) has proposed a different approach by utilizing the fact that there are infinite many ways to decompose one given function into difference components. Using EMD, one can still obtain many different sets of IMFs by changing the stoppage criteria. The confidence limit so derived does not depend on the ergodic theory From the confidence limit study, Huang et al. (2003) also found the optimal $\\boldsymbol{S}$-number, when the differences reach a local minimum. Based on their experience from different data sets, they concluded that an $\\boldsymbol{S}$-number in the range of 4 to 8 performed well. Logic also dictates that the $\\boldsymbol{S}$-number should not be too high (which would drain all the physical meaning out of the IMF), nor too low (which would leave some riding waves remaining in the resulting IMFs).The Statistical Significance of IMFsThe EMD is a method to separate the data into different components by their scales. There is always the question: On what is the statistical significance of the IMFs based? In data containing noise, how can we separate the noise from information with confidence? This question was addressed by both Flandrin et al. (2004) and Wu and Huang (2004) through the study of signals consisting of noise only. Using white noise, Wu and Huang (2004) found the relationship between the mean period and RMS values of the IMFs. Furthermore, from the statistical properties of the scattering of the data, they found the bounds of the data distribution analytically. They concluded that when a data set is analyzed with EMD, if the mean period-RMS values exist outside the noise bounds, the components most likely contains signal, otherwise, a component could be resulted only from noise. Therefore, the components with their mean period-RMS values exceeding the noise bounds are statistically significant.Ensemble EMD (EEMD)One of the major problems existed in EMD is scale mixing: an IMF often contains local oscillations with dramatically different frequencies/scales (Huang et al 1999). Previous solution to that was introducing the intermittency check in which the frequency/scale range is subjectively determined. While such an approach works well in many cases, it also has side effect such as reducing adaptation of the EMD method.Recently, a new Ensemble Empirical Mode Decomposition (EEMD) method is presented. This new approach consists of an ensemble of decompositions of data with added white noise, and then treats the resultant mean as the final true result. Finite, not infinitesimal, amplitude white noise is necessary to force the ensemble to exhaust all possible solutions in the sifting process, thus requiring the different scale signals to collate in the proper intrinsic mode functions (IMF) dictated by the dyadic filter banks. The effect of the added white noise is to present a uniform reference frame in the timefrequency and time-scale space; and, therefore, the added noise provides a natural reference for the signals of comparable scale to collate in one IMF. With this ensemble mean, the scale can be clearly and naturally separated without any a priori subjective criterion selection, such as in the intermittence test for the original EMD algorithm. This new approach fully utilizes the statistical characteristics of white noise to perturb the data in its true solution neighborhood, and then cancel itself out (via ensemble averaging) after serving its purpose; therefore, it represents a substantial improvement over the original EMD and qualifies for a truly noise-assisted data analysis (NADA) method.",
            "content_html": "<p>The EMD method is necessary to reduce any data from non-stationary and nonlinear processes into simple oscillatory function that will yield meaningful instantaneous frequency through the Hilbert transform. Contrary to almost all the previous decomposing methods, EMD is empirical, intuitive, direct, and adaptive, with the a posteriori defined basis derived from the data. The decomposition is designed to seek the different simple intrinsic modes of oscillations in any data based on the principle of scale separation. The data, depending on it complexity, may have many different coexisting modes of oscillation at the same time. Each of these oscillatory modes is represented by an Intrinsic Mode Function (IMF) with the following definitions:</p><p>(a) in the whole data set, the number of extrema and the number of zero-crossings must either equal or differ at most by one, and</p><p>(b) at any point, the mean value of the envelope defined by the local maxima and the envelope defined by the local minima is zero.</p><p>The IMF is a counter part to the simple harmonic function, but it is much more general: instead of constant amplitude and frequency, IMF can have both variable amplitude and frequency as functions of time. This definition is inspired by the simple example of constant plus sinusoidal function given above. The total number of the IMF components is limited to $\\ln _{2} N$, where $\\boldsymbol{N}$ is the total number of data points. It satisfies all the requirements for a meaningful instantaneous frequency through Hilbert transform.</p><p>Pursuant to the above definition for IMF, one can implement the needed decomposition of any function, known as sifting, as follows: Take the test data; identify all the local extrema; divide the extrema into two sets: the maxima and the minima. Then connect all the local maxima by a cubic spline line to form an upper envelope. Repeat the procedure for the local minima to form a lower envelope. The upper and lower envelopes should encompass all the data between them. Their mean is designated as $m_1$, and the difference between the data and $m_1$ is designated as, $h_1$, a proto-IMF:</p>\\[X(t)-m_{1}=h_{1}\\]<p>Ideally, $h_1$ should satisfy the definition of an IMF by construction of $h_1$ described above, which should have made it symmetric and having all maxima positive and all minima negative. Yet, in changing the local zero from a rectangular to a curvilinear coordinate system some inflection points could become additional extrema. New extrema generated this way actually reveal the hidden modes missed in the initial treatment. The sifting process sometimes can recover signals representing low amplitude riding waves with repeated siftings.</p><p>The sifting process serves two purposes: to eliminate riding waves and to make the wave profiles more symmetric. While the first condition is absolute necessary for Hilbert transform to give a meaningful instantaneous frequency, the second condition is also necessary in case the neighboring wave amplitudes having too large a disparity. As a result, the sifting process has to be repeated many times to reduce the extracted signal an IMF. In the subsequent sifting process, $h_1$ is treated as the data for the next round of sifting; therefore,</p>\\[h_{1}-m_{11}=h_{11}\\]<p>After repeated sifting, up to $\\mathrm{k}$ times, $h_{1k}$ :</p>\\[h_{1(k-1)}-m_{1 k}=h_{1 k} \\text {. }\\]<p>If $\\boldsymbol{h}_{\\boldsymbol{1} \\boldsymbol{k}}$ becomes an IMF, it is designated as $C_1$ :</p>\\[C_{1}=h_{1 k}\\]<p>the first IMF component from the data. Here one has a critical decision to make: when to stop. Too many rounds of sifting will reduce the IMF to FM page criterion; too few rounds of sifting will not have a valid IMF. In the past, different criteria have been used, including Cauchy type criterion (Huang et al. 19980), $\\boldsymbol{S}$-number criterion (Huang et al. 2003), fixed-number criterion (Wu and Huang 2004), and etc.</p><p>With any stoppage criterion, the, $c_1$ should contain the finest scale or the shortest period component of the signal. one can, then, remove $c_1$ from the rest of the data by</p>\\[X(t)-C_{1}=r_{1}\\]<p>Since the residue, $r_1$, contains all longer period variations in the data, it is treated as the new data and subjected to the same sifting process as described above. This procedure can be repeated to all the subsequent $r_j$ ‘s, and the result is</p>\\[\\begin{gathered}r_{1}-C_{2}=r_{2}, \\\\\\cdots \\\\r_{n-1}-C_{n}=r_{n}\\end{gathered}\\]<p>The sifting process should stop when the residue, $\\boldsymbol{r}_{n}$, becomes a constant, a monotonic function, or a function contains only a single extrema, from which no more IMF can be extracted. By summing up Equations (16) and (17), we finally obtain</p>\\[X(t)=\\sum_{j=1}^{n} C_{j}+r_{n}\\]<p>Thus, sifting process produces a decomposition of the data into $\\boldsymbol{n}$-intrinsic modes, and a residue, $\\boldsymbol{r}_{\\boldsymbol{n}}$. When apply the EMD method, a mean or zero reference is not required; EMD needs only the locations of the local extrema. The sifting process generates the zero reference for each component. Without the need of the zero reference, EMD avoids the troublesome step of removing the mean values for the large non-zero mean.</p><p>Two special notes here deserve our attention. First, the sifting process offered a way to circumvent the difficulty of define the local mean in a nonstationary time series, where no length scale exists for one to implement the traditional mean operation. The envelope mean employed here does not involve time scale; however, it is local. Second, the sifting process is a Reynolds-type decomposition: separating variations from the mean, except that the mean is a local instantaneous mean, so that the different modes are almost orthogonal to each other, except for the nonlinearity in the data.</p><p>Recent studies by Flandrin et al. (2004) and Wu and Huang (2004) established that the EMD is equivalent to a dyadic filter bank, and it is also equivalent to an adaptive wavelet. Being adaptive, we have avoided the shortcomings of using any a priori-defined wavelet basis, and also avoided the spurious harmonics that would have resulted. The components of the EMD are usually physically meaningful, for the characteristic scales are defined by the physical data.</p><p>Having established the decomposition, we can also identify a new use of the IMF components as filtering. Traditionally, filtering is carried out in frequency space only. But there is a great difficult in applying the frequency filtering when the data is either nonlinear or non-stationary or both, for both nonlinear and nonstationary data generate harmonics of all ranges. Therefore, any filtering will eliminate some of the harmonics, which will cause deformation of the data filtered. Using IMF, however, we can devise a time space filtering.</p><p>For example, a low pass filtered results of a signal having $\\boldsymbol{n}$-IMF components can be simply expressed as</p>\\[X_{l k}(t)=\\sum_{k}^{n} C_{j}+r_{n}\\]<p>a high pass results can be expressed as</p>\\[X_{h k}(t)=\\sum_{1}^{k} C_{j}\\]<p>and a band pass result can be expressed as</p>\\[X_{b k}(t)=\\sum_{b}^{k} C_{j}\\]<p>The advantage of this time space filtering is that the results preserve the full nonlinearity and nonstationarity in the physical space.</p><p>Having obtained the Intrinsic Mode Function components, one can compute the instantaneous frequency for each IMF component as the derivative of the phase function. And one can also designate the instantaneous amplitude from the Hilbert transform to each IMF component. Finally, the original data can be expressed as the real part, RP, of the sum of the data in terms of time, frequency and energy as:</p>\\[X(t)=R P \\sum_{j=1}^{n} a_{j}(t) e^{i \\int \\omega_{j}(t) d t}\\]<p>Above equation gives both amplitude and frequency of each component as a function of time. The same data, if expanded in a Fourier representation, would have a constant amplitude and frequency for each component.</p><p>The contrast between EMD and Fourier decomposition is clear: The IMF represents a generalized Fourier expansion with a time varying function for amplitude and frequency. This frequency-time distribution of the amplitude is designated as the Hilbert Amplitude Spectrum, $\\boldsymbol{H}(\\boldsymbol{\\omega}, \\boldsymbol{t})$, or simply the Hilbert spectrum.</p><p>From the Hilbert spectrum, we can also define the marginal spectrum, $\\boldsymbol{h}(\\boldsymbol{\\omega})$, as</p>\\[h(\\omega)=\\int_{0}^{T} H(\\omega, t) d t\\]<p>The marginal spectrum offers a measure of total amplitude (or energy) contribution from each frequency value. It represents the cumulated amplitude over the entire data span in a probabilistic sense.</p><p>The combination of the Empirical Mode Decomposition and the Hilbert Spectral Analysis is designated by NASA as the Hilbert-Huang Transform (HHT) for short. Recent studies by various investigators indicate that HHT is a super tool for time-frequency analysis of nonlinear and nonstationary data (Huang and Attoh-Okine, 2005, Huang and Shen, 2005). It is based on an adaptive basis, and the frequency is defined through the Hilbert transform. Consequently, there is no need for the spurious harmonics to represent nonlinear waveform deformations as in any of the a priori basis methods, and there is no uncertainty principle limitation on time or frequency resolution from the convolution pairs based also on a priori bases. A summary of the comparison between Fourier, Wavelet and HHT analyses is given in Table 1.</p><p>Table 1. Comparisons between Fourier, Wavelet and HilbertHuang Transform in Data analysis.</p><table>  <thead>    <tr>      <th style=\"text-align: center\">Transform</th>      <th style=\"text-align: center\">Fourier</th>      <th style=\"text-align: center\">Wavelet</th>      <th style=\"text-align: center\">Hilbert</th>    </tr>  </thead>  <tbody>    <tr>      <td style=\"text-align: center\">Basis</td>      <td style=\"text-align: center\">a priori</td>      <td style=\"text-align: center\">a priori</td>      <td style=\"text-align: center\">adaptive</td>    </tr>    <tr>      <td style=\"text-align: center\">Frequency</td>      <td style=\"text-align: center\">convolution: global, uncertainty</td>      <td style=\"text-align: center\">convolution: regional, uncertainty</td>      <td style=\"text-align: center\">differentiation: local, certainty</td>    </tr>    <tr>      <td style=\"text-align: center\">Presentation</td>      <td style=\"text-align: center\">energy-frequency</td>      <td style=\"text-align: center\">energy-time-frequency</td>      <td style=\"text-align: center\">energy-time-frequency</td>    </tr>    <tr>      <td style=\"text-align: center\">Nonlinear</td>      <td style=\"text-align: center\">no</td>      <td style=\"text-align: center\">no</td>      <td style=\"text-align: center\">yes</td>    </tr>    <tr>      <td style=\"text-align: center\">Non-stationary</td>      <td style=\"text-align: center\">no</td>      <td style=\"text-align: center\">yes</td>      <td style=\"text-align: center\">yes</td>    </tr>    <tr>      <td style=\"text-align: center\">Feature Extraction</td>      <td style=\"text-align: center\">no</td>      <td style=\"text-align: center\">discrete: no, continuous: yes</td>      <td style=\"text-align: center\">yes</td>    </tr>    <tr>      <td style=\"text-align: center\">Theoretical Base</td>      <td style=\"text-align: center\">theory complete</td>      <td style=\"text-align: center\">theory complete</td>      <td style=\"text-align: center\">empirical</td>    </tr>  </tbody></table><p>After this basic development of the HHT method, there are some recent developments, which have either added insight to the results, enhanced the statistical significance of the results, and fixed some shortcomings in the HHT.</p><h2 id=\"recent-developments\">Recent developments</h2><h3 id=\"the-normalized-hilbert-transform-and-the-direct-quadrature\">The Normalized Hilbert Transform and the direct quadrature</h3><p>It is well known that, although the Hilbert transform exists for any function of $\\boldsymbol{L}^{p}$ class, the phase function of the transformed function will not always yield physically meaningful instantaneous frequencies. The limitations have been summarized succinctly in two theorems.</p><p>First, in order to separate the contribution of the phase variation into the phase and amplitude parts, the function have to satisfy the limitation stipulated in the Bedrosian theorem (1963), which states that the Hilbert transform for the product of two functions, $\\boldsymbol{f}(\\boldsymbol{t})$ and $\\boldsymbol{h}(\\boldsymbol{t})$, can be written as</p>\\[H[f(t) h(t)]=f(t) H[h(t)]\\]<p>only if the Fourier spectra for $\\boldsymbol{f}(\\boldsymbol{t})$ and $\\boldsymbol{h}(\\boldsymbol{t})$ are totally disjoint in frequency space, and the frequency content of the spectrum for $\\boldsymbol{h}(\\boldsymbol{t})$ is higher than that of $\\boldsymbol{f}(\\boldsymbol{t})$. This limitation is critical, for we need to have</p><p>$H[a(t) \\cos \\theta(t)]=a(t) H[\\cos \\theta(t)]$, (28) otherwise, one cannot use Equation (6) to define the phase function, for the amplitude variation would mix with the phase function. Bedrosian theorem requires that the amplitude is varying be so slowly that the frequency spectra of the envelope and the carrier waves are disjoint. This is possible only for trivial cases, for unless the amplitude is constant, any local deviation can be considered as a sum of delta-functions, which has a wide white spectrum. Therefore, the spectrum for varying amplitude would never be totally separate from that of the carrier. This limitation has made the application of the Hilbert transform even to IMFs problematic. To satisfy this requirement, Huang and Long (2003) have proposed the normalization of the IMFs in the following steps: Starting from an IMF, they first find all the maxima of the IMFs, defining the envelope by spline through all the maxima, and designating the envelope as $\\boldsymbol{E}(\\boldsymbol{t})$. Now, normalize the IMF by dividing the IMF by $\\boldsymbol{E}(\\boldsymbol{t})$. Thus, they have the normalized function having amplitude always equal to unity, and have circumvented the limitation of Bedrosian theorem.</p><p>Second, there is the new restriction given by the Nuttall theorem (1966), which stipulates that the Hilbert transform of cosine is not necessarily the sine with the same phase function for a cosine with an arbitrary phase function. Nuttall gave an energy based error bound, $\\boldsymbol{\\Delta E}$, defined as the difference between $y(t)$, the Hilbert transform of the data, and $\\boldsymbol{Q}(\\boldsymbol{t})$, the quadrature (with phase shift of exactly $90^{\\circ}$ ) of the function as</p>\\[\\Delta E=\\int_{t=0}^{T}|y(t)-Q(t)|^{2} d t=\\int_{-\\infty}^{0} S_{q}(\\omega) d \\omega,\\]<p>in which $\\boldsymbol{S}_{\\boldsymbol{q}}$ is Fourier spectrum of the quadrature function. Though the proof of this theorem is rigorous, the result is hardly useful, for it gives a constant error bound over the whole data range. With the normalized IMF, Huang and Long (2003) have proposed a variable error bound based on a simple argument, which goes as follows: compute the difference between squared amplitude of the normalized IMF and unity. If the Hilbert transform is exactly the quadrature, the difference between it and unity should be zero; otherwise, the Hilbert transform cannot be exactly the quadrature. Consequently, the error can be measured simply by the difference between the squared normalized IMF and unity, which is a function of time. Huang and Long (2003) and Huang et al. (2006) have conducted detailed comparisons and found the result quite satisfactory.</p><p>Even with the error indicator, we can only know that the Hilbert transform is not exactly the quadrature; we still do not have the correct answer. This prompts a drastic alternative, eschewing the Hilbert transform totally. An exact direct quadrature has been found (Huang et al., 2006), and it would resolve the difficulties associated with the instantaneous frequency computation.</p><h3 id=\"the-confidence-limit\">The Confidence Limit</h3><p>The confidence limit for the Fourier spectral analysis is based on the ergodic theory, where the temporal average is treated as the ensemble average. This approach is only valid if the processes are stationary. Huang et al. (2003) has proposed a different approach by utilizing the fact that there are infinite many ways to decompose one given function into difference components. Using EMD, one can still obtain many different sets of IMFs by changing the stoppage criteria. The confidence limit so derived does not depend on the ergodic theory From the confidence limit study, Huang et al. (2003) also found the optimal $\\boldsymbol{S}$-number, when the differences reach a local minimum. Based on their experience from different data sets, they concluded that an $\\boldsymbol{S}$-number in the range of 4 to 8 performed well. Logic also dictates that the $\\boldsymbol{S}$-number should not be too high (which would drain all the physical meaning out of the IMF), nor too low (which would leave some riding waves remaining in the resulting IMFs).</p><h3 id=\"the-statistical-significance-of-imfs\">The Statistical Significance of IMFs</h3><p>The EMD is a method to separate the data into different components by their scales. There is always the question: On what is the statistical significance of the IMFs based? In data containing noise, how can we separate the noise from information with confidence? This question was addressed by both Flandrin et al. (2004) and Wu and Huang (2004) through the study of signals consisting of noise only. Using white noise, Wu and Huang (2004) found the relationship between the mean period and RMS values of the IMFs. Furthermore, from the statistical properties of the scattering of the data, they found the bounds of the data distribution analytically. They concluded that when a data set is analyzed with EMD, if the mean period-RMS values exist outside the noise bounds, the components most likely contains signal, otherwise, a component could be resulted only from noise. Therefore, the components with their mean period-RMS values exceeding the noise bounds are statistically significant.</p><h3 id=\"ensemble-emd-eemd\">Ensemble EMD (EEMD)</h3><p>One of the major problems existed in EMD is scale mixing: an IMF often contains local oscillations with dramatically different frequencies/scales (Huang et al 1999). Previous solution to that was introducing the intermittency check in which the frequency/scale range is subjectively determined. While such an approach works well in many cases, it also has side effect such as reducing adaptation of the EMD method.</p><p>Recently, a new Ensemble Empirical Mode Decomposition (EEMD) method is presented. This new approach consists of an ensemble of decompositions of data with added white noise, and then treats the resultant mean as the final true result. Finite, not infinitesimal, amplitude white noise is necessary to force the ensemble to exhaust all possible solutions in the sifting process, thus requiring the different scale signals to collate in the proper intrinsic mode functions (IMF) dictated by the dyadic filter banks. The effect of the added white noise is to present a uniform reference frame in the timefrequency and time-scale space; and, therefore, the added noise provides a natural reference for the signals of comparable scale to collate in one IMF. With this ensemble mean, the scale can be clearly and naturally separated without any a priori subjective criterion selection, such as in the intermittence test for the original EMD algorithm. This new approach fully utilizes the statistical characteristics of white noise to perturb the data in its true solution neighborhood, and then cancel itself out (via ensemble averaging) after serving its purpose; therefore, it represents a substantial improvement over the original EMD and qualifies for a truly noise-assisted data analysis (NADA) method.</p>",
            "url": "http://localhost:4000/2017/04/12/hht",
            
            
            
            "tags": ["trend_extraction","EMD","hilbert_spectral_analysis","HHT","EEMD"],
            
            "date_published": "2017-04-12T00:00:00+01:00",
            "date_modified": "2017-04-12T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/11/sparse-representation",
            "title": "Sparse representation of signals",
            "summary": "Sparse representation of signals",
            "content_text": "Sparse representation is widely employed for expressing signals using very few linear combinations of elementary signals. These elementary signals are called atoms. Since thenumber of the atoms is more than the dimension of the signalspace, any signal can be represented by linear combinationsof these atoms and the representations are not unique.Sparse representation is to use the minimum number of atoms to express the signals and this is actually an $L_{0}$ norm optimization problem. That is for a given overcomplete dictionary $A\\in\\Re^{N \\times M}$ and a signal $x\\in R^{N \\times 1}$, where $N&lt;M$ and $R^{a \\times b}$ denotes the space of $a \\times b$ real valued matrices, the representation problem is to find $z\\in\\Re^{M \\times 1}$ such that $x=Az$ and $\\lVert z\\lVert_{0}$ is minimized. That is \\(z_{0}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{0} \\text{  subject to  } x=Az \\tag{1}\\)Here $\\lVert z\\lVert_{0}$ denotes $L_{0}$ norm of $z$, which is equivalent to the number of nonzero elements in $z$.The problem defined in (1) is nonconvex, nonsmooth and NP hard, it requires an exhaustive search for finding the solution. An approximate solution can be obrtained by solving the corresponding $L_{1}$ norm optimization problem if the isometry condition is satisfied. The $L_{1}$ norm optimization is as follows\\[z_{1}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } x=Az \\tag{2}\\]Although $z_{1}^{\\ast}$ is a good approximation of $z_{0}^{\\ast}$ when the isometry condition is satisifed, these two solutions will be very different if $x$ contains significant amount of noise. Nevertheless, this is the typical case in practical circumstances. Hence the exact equality constraint is usually related to an inequality constraint as follows.\\[z^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } \\lVert Az-x\\lVert_{\\infty} \\le \\epsilon \\tag{3}\\]where $\\epsilon$ is the specification on the maximium absolute difference between $Az$ and $x$. This problem can be efficiently solved via reformulating the $L_{1}$ norm optimization problem to a linear programming problem.",
            "content_html": "<p align=\"justify\">Sparse representation is widely employed for expressing signals using very few linear combinations of elementary signals. These elementary signals are called atoms. Since thenumber of the atoms is more than the dimension of the signalspace, any signal can be represented by linear combinationsof these atoms and the representations are not unique.</p><p>Sparse representation is to use the minimum number of atoms to express the signals and this is actually an $L_{0}$ norm optimization problem. That is for a given overcomplete dictionary $A\\in\\Re^{N \\times M}$ and a signal $x\\in R^{N \\times 1}$, where $N&lt;M$ and $R^{a \\times b}$ denotes the space of $a \\times b$ real valued matrices, the representation problem is to find $z\\in\\Re^{M \\times 1}$ such that $x=Az$ and $\\lVert z\\lVert_{0}$ is minimized. That is \\(z_{0}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{0} \\text{  subject to  } x=Az \\tag{1}\\)</p><p>Here $\\lVert z\\lVert_{0}$ denotes $L_{0}$ norm of $z$, which is equivalent to the number of nonzero elements in $z$.</p><p>The problem defined in (1) is nonconvex, nonsmooth and NP hard, it requires an exhaustive search for finding the solution. An approximate solution can be obrtained by solving the corresponding $L_{1}$ norm optimization problem if the isometry condition is satisfied. The $L_{1}$ norm optimization is as follows</p>\\[z_{1}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } x=Az \\tag{2}\\]<p>Although $z_{1}^{\\ast}$ is a good approximation of $z_{0}^{\\ast}$ when the isometry condition is satisifed, these two solutions will be very different if $x$ contains significant amount of noise. Nevertheless, this is the typical case in practical circumstances. Hence the exact equality constraint is usually related to an inequality constraint as follows.</p>\\[z^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } \\lVert Az-x\\lVert_{\\infty} \\le \\epsilon \\tag{3}\\]<p>where $\\epsilon$ is the specification on the maximium absolute difference between $Az$ and $x$. This problem can be efficiently solved via reformulating the $L_{1}$ norm optimization problem to a linear programming problem.</p>",
            "url": "http://localhost:4000/2017/04/11/sparse-representation",
            
            
            
            "tags": ["representation","sparse_binary_programming"],
            
            "date_published": "2017-04-11T00:00:00+01:00",
            "date_modified": "2017-04-11T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/10/ssa",
            "title": "Single Spectrum Analysis",
            "summary": "SSA",
            "content_text": "The main steps of SSA can be summarized as follows:For a time series $x(n)$ for $n=1,2\\dots N$, let the window length be $L$ , where  $1&lt;L&lt;N$. The first step of SSA is to construct a trajectory matrix as follows. Define the $L$ dimensional vectors as\\[X_{n}=\\begin{bmatrix} x(n)\\\\ \\vdots \\\\ x(n+L-1)\\end{bmatrix}\\]for $n =1,2,\\dots, N − L +1$.Denote $K = N − L +1$. These $K$ vectors are put into a matrix and the $L \\times K$ trajectory matrix is constructed as follows:\\[X = \\begin{bmatrix}X_{1} &amp; X_{2} &amp; \\dots &amp; X_{K}\\end{bmatrix}\\]The second step is to express $X$ as the sum of component matrices. Let $S=XX^T$ and the eigenvalues of $S$ be $\\lambda_{1}\\ge\\lambda_{2}\\dots\\ge\\lambda_{L}\\ge 0$. Define $D=\\max{j:\\lambda_{j}&gt;0}$. Let $U_{1},\\dots,U_{D}$ be the corresponding eigenvectors.Denote $V_{j}=\\frac{X^TU_{j}}{\\sqrt{ \\lambda_{j} }}$ for $j=1,2,\\dots,D$  be the factor vectors.Define:\\[\\tilde{X}_{j} =\\sqrt{\\lambda_{j}}U_{j}V_{j}^T\\]for $j=1,2,\\dots,D$. It can be shown that $X$ can be represented as\\[X = \\tilde{X}_{1}+\\dots+\\tilde{X}_{D}\\]The third step is to represent $X$ as the sum of grouped matrix components as follows. The indices set ${1,\\dots,D}$ is partitioned into $M$ disjoint subsets $I_{1}\\dots I_{M}$. Let $I_{m} ={i_{m_{1}},\\dots,{i_{m_{c}}}}$ for $m=1,\\dots, M$ and\\[\\tilde{X}_{I_{m}}=\\tilde{X}_{i_{m_{1}}}+\\dots+\\tilde{X}_{i_{m_{C}}}\\]Hence we have\\[X=\\tilde{X}_{I_{1}}+\\dots+\\tilde{X}_{I_{M}}\\]The final step is to reconstruct the signal by the diagonal averaging method.First, transform  new text $\\tilde{X}_{I_m}$ into new one dimensional signals of length $N$ by the hankelization like procedure.The vectors and the transform operator are denoted as $\\tilde{x}_{I_m}$  for $m=1,\\dots,M$ and  $\\Im(.)$ respectievely. That is\\[\\tilde{x}_{I_{m}} = \\Im(\\tilde{X}_{I_{m}}) ; m=1,\\dots,M\\]Thus the original time series can be expressed as a sum of $M$ series,\\[x(n)=\\tilde{x}_{I_{1}}(n)+\\dots+\\tilde{x}_{I_{M}}(n) ; n=1,\\dots,N\\]",
            "content_html": "<p>The main steps of SSA can be summarized as follows:</p><p>For a time series $x(n)$ for $n=1,2\\dots N$, let the window length be $L$ , where  $1&lt;L&lt;N$. The first step of SSA is to construct a trajectory matrix as follows. Define the $L$ dimensional vectors as</p>\\[X_{n}=\\begin{bmatrix} x(n)\\\\ \\vdots \\\\ x(n+L-1)\\end{bmatrix}\\]<p>for $n =1,2,\\dots, N − L +1$.</p><p>Denote $K = N − L +1$. These $K$ vectors are put into a matrix and the $L \\times K$ trajectory matrix is constructed as follows:</p>\\[X = \\begin{bmatrix}X_{1} &amp; X_{2} &amp; \\dots &amp; X_{K}\\end{bmatrix}\\]<p>The second step is to express $X$ as the sum of component matrices. Let $S=XX^T$ and the eigenvalues of $S$ be $\\lambda_{1}\\ge\\lambda_{2}\\dots\\ge\\lambda_{L}\\ge 0$. Define $D=\\max{j:\\lambda_{j}&gt;0}$. Let $U_{1},\\dots,U_{D}$ be the corresponding eigenvectors.</p><p>Denote $V_{j}=\\frac{X^TU_{j}}{\\sqrt{ \\lambda_{j} }}$ for $j=1,2,\\dots,D$  be the factor vectors.</p><p>Define:</p>\\[\\tilde{X}_{j} =\\sqrt{\\lambda_{j}}U_{j}V_{j}^T\\]<p>for $j=1,2,\\dots,D$. It can be shown that $X$ can be represented as</p>\\[X = \\tilde{X}_{1}+\\dots+\\tilde{X}_{D}\\]<p>The third step is to represent $X$ as the sum of grouped matrix components as follows. The indices set ${1,\\dots,D}$ is partitioned into $M$ disjoint subsets $I_{1}\\dots I_{M}$. Let $I_{m} ={i_{m_{1}},\\dots,{i_{m_{c}}}}$ for $m=1,\\dots, M$ and</p>\\[\\tilde{X}_{I_{m}}=\\tilde{X}_{i_{m_{1}}}+\\dots+\\tilde{X}_{i_{m_{C}}}\\]<p>Hence we have</p>\\[X=\\tilde{X}_{I_{1}}+\\dots+\\tilde{X}_{I_{M}}\\]<p>The final step is to reconstruct the signal by the diagonal averaging method.</p><p>First, transform  new text $\\tilde{X}_{I_m}$ into new one dimensional signals of length $N$ by the hankelization like procedure.</p><p>The vectors and the transform operator are denoted as $\\tilde{x}_{I_m}$  for $m=1,\\dots,M$ and  $\\Im(.)$ respectievely. That is</p>\\[\\tilde{x}_{I_{m}} = \\Im(\\tilde{X}_{I_{m}}) ; m=1,\\dots,M\\]<p>Thus the original time series can be expressed as a sum of $M$ series,</p>\\[x(n)=\\tilde{x}_{I_{1}}(n)+\\dots+\\tilde{x}_{I_{M}}(n) ; n=1,\\dots,N\\]",
            "url": "http://localhost:4000/2017/04/10/ssa",
            
            
            
            "tags": ["trend_extraction","SSA"],
            
            "date_published": "2017-04-10T00:00:00+01:00",
            "date_modified": "2017-04-10T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming",
            "title": "Trend extraction with SSA and Sparse Binary Programming",
            "summary": "Trend extraction with SSA and Sparse Binary Programming",
            "content_text": "The underlying trend is approximated by the sum of a part of SSA components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem canbe approximated by a linear programming problem when theisometry condition is satisfied, and the solution of the linearprogramming problem can be obtained via existing simplexmethods or interior point methods.By applying the binaryquantization to the obtained solution of the linear programmingproblem, the approximated solution of the original sparsebinary programming problem is obtained. Unlike previouslyreported techniques that require a pre-cursor model orparameter specifications, the proposed method is completelyadaptive.DetailsThe conventional approach for selecting SSAcomponents for extracting the underlying trend is to employonly the first several SSA components. However, thisselection rule fails when the underlying trend of a signal has acomplicated structure such as a high order polynomialstructure which cannot be characterized by only the firstseveral SSA components.The idea is to formulate theselection problem as a sparse binary programming problem and proposes an efficient methodology for approximatingthe solution of the problem. In particular, the selectionproblem is formulated as follows. The number of thecomponents to be selected is minimized subject to aspecification on the maximum absolute difference betweenthe approximated underlying trend and the original signal aswell as the binary valued constraint on the selectioncoefficients. Since the sparse binary programming problem isnonsmooth, nonconvex and NP hard, it requires an exhaustivesearch for finding the solution. As a result, the computationaleffort for finding the solution is very large and an efficientalgorithm for approximating the solution is very useful andimportant. To address these issues, the corresponding continuous valued optimization problem (the sameoptimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective functionsubject to an $L_{\\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem isobtained.MethodologySSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.The selection problem is formulated as the following sparse sparse binary programming problem.\\[z^{*}= \\begin{bmatrix}z_{1}^{*}&amp; \\dots, z_{D}^*\\end{bmatrix}^{T}= \\mathrm{argmin}_{z} \\lVert z\\lVert_{0}\\]subject to $\\lVert Az-x\\lVert_{\\infty} \\le \\epsilon$ and $z_{i}\\in{0,1}$ for $i=1,\\dots,D$.Here\\[A=[\\tilde{x}_1,\\dots,\\tilde{x}_D]\\in R^{N \\times D}\\]and\\[\\epsilon=0.5\\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\\]where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.If $z_{i}^{\\ast}=1$ (or $z_{i}^{\\ast}=0$), then the corresponding component $\\tilde{x}_i$ for $i=1,\\dots,D$ is selected (or excluded) for the representation of the underlying trend.Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimizationproblem is considered first. The solution of the $L_{0}$ normcontinuous valued optimization problem is approximated bythe solution of the corresponding $L_{1}$ norm continuous valuedoptimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:\\[y^{*}= [y_{1}^{*},\\dots,y_{D}^{*}]= \\mathrm{arg}\\min_{y}\\lVert y\\lVert_{1} \\text{  subject to } \\lVert Ay-x\\lVert_{\\infty}\\le \\epsilon\\]By further applying the quantization on $y_{i}^{*}$ for $i=1,\\dots,D$ to either $0$ or $1$ via the following operator.\\[W_{i}^{*}=\\begin{cases}1 &amp;  y_{i}^{*}\\ge 0.5\\\\0 &amp;  y_{i}^{*}&lt; 0.5\\end{cases}\\]the corresponding component $\\tilde{x}_{i}$for is selected (excluded) for the representation of the underlying trend if$W_{i}^{\\ast}=1( \\text{ or } W_{i}^{\\ast}=0)$Finally the underlying trend of the signal is obtained by\\[\\Gamma = AW^*\\]where $W^{\\ast}= [W_1^{\\ast},\\dots,W_D^{\\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original sparse binary programming problem. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programmingproblem. That is, $W^{\\ast}$ in is very close to $z^{\\ast}$ above.Therefore, the subset $I_{1}$ can be obtained by\\[I_{1} = \\{i\\vert W_{i}^{*}= 1, 1\\le i \\le D\\}\\]After obtaining the underlying trend $\\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.Denote $\\Gamma =[\\gamma(1), \\dots, \\gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume thatthere is an underlying structure in the time series and thisstructure is preserved for the time period to be predicted. Aprediction model based on the linear recurrent formulae (LRF)is employed.That is, the points $\\gamma(N-L+2), \\dots, \\gamma(N)$ are employed for the prediction of $\\gamma(N+1)$, and so on. In other words,\\[\\gamma(n+1) = \\sum_{k=0}^{L-2}a_{k}\\gamma(n-k) \\text{   for  } n\\ge N,\\]where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \\dots, a_{0})^T$ is given by\\[R = \\frac{1}{1-\\nu^{2}}\\sum_{i\\in I_{1}}\\pi_{i}U_{i}^{\\nabla}\\]Here, $\\nu^{2=}\\sum_{i\\in I_{1}}\\pi_{i}^2$, $\\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\\nabla }\\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \\in I_{1}.$",
            "content_html": "<p align=\"justify\">The underlying trend is approximated by the sum of a part of <a class=\"internal-link\" href=\"/2017/04/10/ssa\">SSA</a> components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.</p><p>As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.</p><p align=\"justify\">This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem canbe approximated by a linear programming problem when theisometry condition is satisfied, and the solution of the linearprogramming problem can be obtained via existing simplexmethods or interior point methods.</p><p align=\"justify\">By applying the binaryquantization to the obtained solution of the linear programmingproblem, the approximated solution of the original sparsebinary programming problem is obtained. Unlike previouslyreported techniques that require a pre-cursor model orparameter specifications, the proposed method is completelyadaptive.</p><h3 id=\"details\">Details</h3><p align=\"justify\">The conventional approach for selecting SSAcomponents for extracting the underlying trend is to employonly the first several SSA components. However, thisselection rule fails when the underlying trend of a signal has acomplicated structure such as a high order polynomialstructure which cannot be characterized by only the firstseveral SSA components.</p><p align=\"justify\">The idea is to formulate theselection problem as a sparse binary programming problem and proposes an efficient methodology for approximatingthe solution of the problem. In particular, the selectionproblem is formulated as follows. The number of thecomponents to be selected is minimized subject to aspecification on the maximum absolute difference betweenthe approximated underlying trend and the original signal aswell as the binary valued constraint on the selectioncoefficients. </p><p align=\"justify\">Since the sparse binary programming problem isnonsmooth, nonconvex and NP hard, it requires an exhaustivesearch for finding the solution. As a result, the computationaleffort for finding the solution is very large and an efficientalgorithm for approximating the solution is very useful andimportant. </p><p>To address these issues, the corresponding continuous valued optimization problem (the sameoptimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective functionsubject to an $L_{\\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.</p><p>By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem isobtained.</p><h2 id=\"methodology\">Methodology</h2><p>SSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.</p><p>Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.</p><p>However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.</p><p>In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.</p><p>The selection problem is formulated as the following sparse sparse binary programming problem.</p>\\[z^{*}= \\begin{bmatrix}z_{1}^{*}&amp; \\dots, z_{D}^*\\end{bmatrix}^{T}= \\mathrm{argmin}_{z} \\lVert z\\lVert_{0}\\]<p>subject to $\\lVert Az-x\\lVert_{\\infty} \\le \\epsilon$ and $z_{i}\\in{0,1}$ for $i=1,\\dots,D$.</p><p>Here</p>\\[A=[\\tilde{x}_1,\\dots,\\tilde{x}_D]\\in R^{N \\times D}\\]<p>and</p>\\[\\epsilon=0.5\\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\\]<p>where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.</p><p>If $z_{i}^{\\ast}=1$ (or $z_{i}^{\\ast}=0$), then the corresponding component $\\tilde{x}_i$ for $i=1,\\dots,D$ is selected (or excluded) for the representation of the underlying trend.</p><p>Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.</p><p>In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimizationproblem is considered first. The solution of the $L_{0}$ normcontinuous valued optimization problem is approximated bythe solution of the corresponding $L_{1}$ norm continuous valuedoptimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:</p>\\[y^{*}= [y_{1}^{*},\\dots,y_{D}^{*}]= \\mathrm{arg}\\min_{y}\\lVert y\\lVert_{1} \\text{  subject to } \\lVert Ay-x\\lVert_{\\infty}\\le \\epsilon\\]<p>By further applying the quantization on $y_{i}^{*}$ for $i=1,\\dots,D$ to either $0$ or $1$ via the following operator.</p>\\[W_{i}^{*}=\\begin{cases}1 &amp;  y_{i}^{*}\\ge 0.5\\\\0 &amp;  y_{i}^{*}&lt; 0.5\\end{cases}\\]<p>the corresponding component $\\tilde{x}_{i}$</p><p>for is selected (excluded) for the representation of the underlying trend if</p><p>$W_{i}^{\\ast}=1( \\text{ or } W_{i}^{\\ast}=0)$</p><p>Finally the underlying trend of the signal is obtained by</p>\\[\\Gamma = AW^*\\]<p>where $W^{\\ast}= [W_1^{\\ast},\\dots,W_D^{\\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original <a class=\"internal-link\" href=\"/2017/04/11/sparse-representation\">sparse binary programming problem</a>. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programmingproblem. That is, $W^{\\ast}$ in is very close to $z^{\\ast}$ above.</p><p>Therefore, the subset $I_{1}$ can be obtained by</p>\\[I_{1} = \\{i\\vert W_{i}^{*}= 1, 1\\le i \\le D\\}\\]<p>After obtaining the underlying trend $\\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.</p><p>Denote $\\Gamma =[\\gamma(1), \\dots, \\gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume thatthere is an underlying structure in the time series and thisstructure is preserved for the time period to be predicted. Aprediction model based on the linear recurrent formulae (LRF)is employed.</p><p>That is, the points $\\gamma(N-L+2), \\dots, \\gamma(N)$ are employed for the prediction of $\\gamma(N+1)$, and so on. In other words,</p>\\[\\gamma(n+1) = \\sum_{k=0}^{L-2}a_{k}\\gamma(n-k) \\text{   for  } n\\ge N,\\]<p>where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \\dots, a_{0})^T$ is given by</p>\\[R = \\frac{1}{1-\\nu^{2}}\\sum_{i\\in I_{1}}\\pi_{i}U_{i}^{\\nabla}\\]<p>Here, $\\nu^{2=}\\sum_{i\\in I_{1}}\\pi_{i}^2$, $\\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\\nabla }\\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \\in I_{1}.$</p>",
            "url": "http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming",
            
            
            
            "tags": ["trend_extraction","SSA","sparse_binary_programming"],
            
            "date_published": "2017-04-09T00:00:00+01:00",
            "date_modified": "2017-04-09T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/08/ica-with-cdwt",
            "title": "ICA with CDWT",
            "summary": "ICA with CDWT",
            "content_text": "It is well known that if the original sounds are mixed inthe real environment (in the time domain) then the observedsounds are a convolution mixture between the originalsounds with a delay and a reverberation. In order to simplifythis convolution mixture, it is a good idea to convert thesignal from the time domain into the time–frequencydomain and transform the convolution mixture into thelinear mixture by a time–frequency analysis method. Bydoing this the drawback of poor performance with unsteadysounds of the ICA also can be improved.The time-frequency analysis method is usually acombination of the ICA, the Short Time Fourier Transform(STFT) and the Discrete Wavelet Transform (DWT).The STFT is probably the most common approach fortime–frequency analysis. It subdivides the signal into shorttime segments (it is the same as using a small window todivide the signal), and a discrete Fourier transform iscomputed for each of these. For each frequency component,however, the window length is fixed. So it is impossible tochoose an optimal window for each frequency component,that is, the short time Fourier transform is unable to obtainoptimal analysis results for individual frequencycomponents.On the other hand, the DWT that wascarried out by Mattal’s fast algorithm also has a drawbackof lacking shift invariance although it can solve the problemof the window width and obtain optimal frequencyresolution for each frequency component. Fortunately,in order to improve the fault, a Complex Discrete WaveletTransform (CDWT) was proposed and it has been appliedwidely to signal and image analysisICA and CDWT for blind source seperationIn this method, the signals first were transformed from the time–domain into thetime–frequency domain by using the CDWT and then the ICA was carried out in the time–frequency domain. As in traditional methods, such as the STFT + ICA and the DWT + ICA, the following two problems when the ICA processing was carry out in the time–frequency domainoccurred. \t1. Scaling problem: the signal’s amplitude and phase obtained by the ICA was changed, \t2. Permutation problem: the separated signals are replaced at every frequency level mutually.This method discuss the technique for solving the scaling and the permutation problems. Finally, the separated signals are obtained by the inverse CDWT.Here, the case of two sound sources and two mikes has been considered for simplicity. First of all,the two sound signals $x_n(t) (n=1,2,$ where $n$ is the number of channels and $t$ is the time) were observed by mikes from two sound sources $s_{i}(t) ( i =1, 2)$. The relation between the observed signal $x_n(t)$ and the sound source $s_{i}(t)$ is as follows:\\[x_{n}(t) = \\mathbf{A_{n,i}}(t) \\ast  S_{i}(t) \\tag{1}\\]where $\\ast$ denotes convolution, the $\\mathbf{A_{n,i}}(t)$ , is the impulse response that is from the sound sources to the mikes and can be shown as the following equation:\\[\\mathbf{A_{n,i}}(t) = \\begin{bmatrix} a_{11}(t) &amp; a_{12}(t)\\\\ a_{21}(t) &amp; a_{22}(t)\\\\\\end{bmatrix}, n,i = 1,2\\]If one transforms the signal $x_{n}(t)$ from the time domain to the time–frequency domain by the CDWT then the (1) can be expressed as (2), in which the convolution of the sound source and the impulse response was changed to simple multiplication.\\[x_{n}(\\omega , T) = \\mathbf{A_{n,i}}(\\omega) S_{i}(\\omega,T) \\tag{2}\\]where, $\\omega$ is the frequency and $T$ the time in the time–frequency domain.Next, whitening of the observed signal was carried out as follows:\\[\\hat{x}_{n}(\\omega , T) = Q(\\omega) x_{n}(\\omega,T) \\tag{3}\\]where $\\hat{x}_{n}(\\omega ,T)$is a whitened signal matrix and $Q(\\omega)$ a whitened mixture, which can be obtained from the observed mixture signal $x_{n}(\\omega , T)$ in each frequency.Finally, the ICA was carried out by using the whitened signal matrix $\\hat{x}_{n}(\\omega , T)$, in which the separation matrix $W(\\omega)$ can be presumed. As a result, the separated signal $u_{i}(\\omega , T)$ shown in (4) can be obtained.\\[u_{i}(\\omega , T) = W(\\omega) \\hat{x}_{n}(\\omega,T) \\tag{4}\\]The convolution mixture signal $x_{n}(t)$  shown in (1) can be transformed into the linear mixture signal$x_{n}(\\omega, T)$ shown in (2) by using the CDWT which simplifies the preprocessing of the ICA. A complex wavelet like, RI-Daubechies 6 wavelet can be applied as the mother wavelet.Problem and correction rule of ICA processingThe ICA is a technique for presuming the sound source $s_{i}(\\omega,T)$  and the inverse matrix of $A_{ni}(\\omega)$ from statistics without any former information of the observed signal. In this case, the amplitude of the separated signal $u_{i}(\\omega,T)$ is a constant times the amplitude of the sound source $s_{i}(\\omega,T)$ and a correction is needed.In this method, a similar amplitude change at each frequency level also occurred in the preprocessing by CDWT introduced. This phenomenon is called a scaling irregularity. Furthermore,same as in the traditional method, it is also possible that the separated sound is replaced with noise at every frequencylevel mutually. This phenomenon is called the permutationproblem. Therefore, after these two problems are solved, theinverse complex discrete wavelet transform is performed,and it is necessary to restore the sound signal observed witheach mike.Scaling problem and correction ruleNot only the amplitude of the restored signal but the phase also differs depending on the frequency by making for signal whitening (making no correlation). To take an arbitrary complex value by processing, this is caused. In order to solve the scaling irregularity problem, a method hasbeen proposed by Murata that uses the independent element $u_{i}(\\omega,T)$ , which is obtained at each frequency and divides the spectrum. In this study, the method of correcting scaling is adopted from the divided spectrum and can be shown as follows:\\[v_{1}(\\omega,T)=\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{12}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix}u_{1}(\\omega,T) \\\\0\\end{bmatrix}\\]\\[v_{2}(\\omega,T)=\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix} 0 \\\\u_{2}(\\omega,T) \\end{bmatrix}\\]where $v_{11}(\\omega,T),v_{22}(\\omega,T)$ are divided spectrums. If the sum $v_{1}(\\omega,T)+v_{2}(\\omega,T)$ is calculated then the sum $v_{11}(\\omega,T)+v_{21}(\\omega,T)$ is the mixture signal $x_{1}(\\omega,T)$ and the sum $v_{11}(\\omega,T)+v_{22}(\\omega,T)$ is the mixture signal $x_{2}(\\omega,T)$Permutation problem and correction ruleThe complex Fast-ICA performs separation based onnon-Gaussian characteristics. Therefore, the possibility ofthe separating signal with higher non-Gaussiancharacteristics being output as the first channel is very high.However, the height of the frequency is not determined andnoise with low non-Gaussian characteristics might also beoutput. Therefore, there is a possibility that the output of each frequency level is separated without being united by the sound. The separated signal $u_{1}(\\omega,T), u_{2}(\\omega,T)$ of the 1st and 2nd channel without permutation is shown as follows.\\[u_{1}(\\omega ,k) \\approx s_{1}(\\omega,T) \\tag{5} ; u_{2}(\\omega ,k) \\approx s_{2}(\\omega,T)\\]The above expression can be shown as the following equation by using the divided spectrum of the scaling correction.\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]On the other hand, when permutation occurs, (5) becomes the next expression\\[u_{1}(\\omega ,k) \\approx s_{2}(\\omega,T) \\tag{6} ; u_{2}(\\omega ,k) \\approx s_{1}(\\omega,T)\\]\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]From these, we can know that the divided spectrum can be shown as a multiplication of the separated signal $u_{i}(\\omega,T)$ and the transfer function $g_{ni}(\\omega)$ , which is from the sound source to the mike.The separation matrix $W(\\omega)$ and the whitening matrix $Q(\\omega)$ presumed by the ICA processing are used and the next equation can be obtained.\\[D = (W(\\omega)Q(\\omega))^{-1} = \\begin{bmatrix}g_{11}(\\omega) &amp; g_{12}(\\omega) \\\\g_{21}(\\omega) &amp; g_{22}(\\omega)\\end{bmatrix} = e\\begin{bmatrix}a_{11}(\\omega) &amp; a_{12}(\\omega) \\\\a_{21}(\\omega) &amp; a_{22}(\\omega)\\end{bmatrix} = eA_{ni}(\\omega), e \\in \\mathbb{R}\\]ConclusionCDWT +Complex Value Fast ICA outperforms, STFT+ Complex value Fast ICA and DWT+Real Value Fast ICAReference : BLIND SOURCE SEPARATION BY COMBINING INDEPANDENT COMPONENT ANALYSIS WITH COMPLEX DISCRETE WAVELET TRANSFORM ZHONG ZHANG , TAKESHI ENOMOTO , TETSUO MIYAKE , TAKASHI IMAMURA",
            "content_html": "<p align=\"justify\">It is well known that if the original sounds are mixed inthe real environment (in the time domain) then the observedsounds are a convolution mixture between the originalsounds with a delay and a reverberation. In order to simplifythis convolution mixture, it is a good idea to convert thesignal from the time domain into the time–frequencydomain and transform the convolution mixture into thelinear mixture by a time–frequency analysis method. Bydoing this the drawback of poor performance with unsteadysounds of the ICA also can be improved.</p><p align=\"justify\">The time-frequency analysis method is usually acombination of the <a class=\"internal-link\" href=\"/2017/04/06/blind-source-seperation-ica\">ICA</a>, the Short Time Fourier Transform(STFT) and the Discrete Wavelet Transform (DWT).</p><p align=\"justify\">The STFT is probably the most common approach fortime–frequency analysis. It subdivides the signal into shorttime segments (it is the same as using a small window todivide the signal), and a discrete Fourier transform iscomputed for each of these. For each frequency component,however, the window length is fixed. So it is impossible tochoose an optimal window for each frequency component,that is, the short time Fourier transform is unable to obtainoptimal analysis results for individual frequencycomponents.</p><p align=\"justify\">On the other hand, the DWT that wascarried out by Mattal’s fast algorithm also has a drawbackof lacking shift invariance although it can solve the problemof the window width and obtain optimal frequencyresolution for each frequency component. Fortunately,in order to improve the fault, a Complex Discrete WaveletTransform (CDWT) was proposed and it has been appliedwidely to signal and image analysis</p><h2 id=\"ica-and-cdwt-for-blind-source-seperation\">ICA and CDWT for blind source seperation</h2><p>In this method, the signals first were transformed from the time–domain into thetime–frequency domain by using the CDWT and then the ICA was carried out in the time–frequency domain. As in traditional methods, such as the STFT + ICA and the DWT + ICA, the following two problems when the ICA processing was carry out in the time–frequency domainoccurred. \t1. Scaling problem: the signal’s amplitude and phase obtained by the ICA was changed, \t2. Permutation problem: the separated signals are replaced at every frequency level mutually.</p><p>This method discuss the technique for solving the scaling and the permutation problems. Finally, the separated signals are obtained by the inverse CDWT.</p><p><img src=\"/assets/snips/ica_cdwt.png\" alt=\"ICA_CDWT\" /></p><p>Here, the case of two sound sources and two mikes has been considered for simplicity. First of all,the two sound signals $x_n(t) (n=1,2,$ where $n$ is the number of channels and $t$ is the time) were observed by mikes from two sound sources $s_{i}(t) ( i =1, 2)$. The relation between the observed signal $x_n(t)$ and the sound source $s_{i}(t)$ is as follows:</p>\\[x_{n}(t) = \\mathbf{A_{n,i}}(t) \\ast  S_{i}(t) \\tag{1}\\]<p>where $\\ast$ denotes convolution, the $\\mathbf{A_{n,i}}(t)$ , is the impulse response that is from the sound sources to the mikes and can be shown as the following equation:</p>\\[\\mathbf{A_{n,i}}(t) = \\begin{bmatrix} a_{11}(t) &amp; a_{12}(t)\\\\ a_{21}(t) &amp; a_{22}(t)\\\\\\end{bmatrix}, n,i = 1,2\\]<p>If one transforms the signal $x_{n}(t)$ from the time domain to the time–frequency domain by the CDWT then the (1) can be expressed as (2), in which the convolution of the sound source and the impulse response was changed to simple multiplication.</p>\\[x_{n}(\\omega , T) = \\mathbf{A_{n,i}}(\\omega) S_{i}(\\omega,T) \\tag{2}\\]<p>where, $\\omega$ is the frequency and $T$ the time in the time–frequency domain.</p><p>Next, whitening of the observed signal was carried out as follows:</p>\\[\\hat{x}_{n}(\\omega , T) = Q(\\omega) x_{n}(\\omega,T) \\tag{3}\\]<p>where $\\hat{x}_{n}(\\omega ,T)$</p><p>is a whitened signal matrix and $Q(\\omega)$ a whitened mixture, which can be obtained from the observed mixture signal $x_{n}(\\omega , T)$ in each frequency.</p><p>Finally, the ICA was carried out by using the whitened signal matrix $\\hat{x}_{n}(\\omega , T)$</p><p>, in which the separation matrix $W(\\omega)$ can be presumed. As a result, the separated signal $u_{i}(\\omega , T)$ shown in (4) can be obtained.</p>\\[u_{i}(\\omega , T) = W(\\omega) \\hat{x}_{n}(\\omega,T) \\tag{4}\\]<p>The convolution mixture signal $x_{n}(t)$  shown in (1) can be transformed into the linear mixture signal$x_{n}(\\omega, T)$ shown in (2) by using the CDWT which simplifies the preprocessing of the ICA. A complex wavelet like, RI-Daubechies 6 wavelet can be applied as the mother wavelet.</p><h2 id=\"problem-and-correction-rule-of-ica-processing\">Problem and correction rule of ICA processing</h2><p>The ICA is a technique for presuming the sound source $s_{i}(\\omega,T)$  and the inverse matrix of $A_{ni}(\\omega)$ from statistics without any former information of the observed signal. In this case, the amplitude of the separated signal $u_{i}(\\omega,T)$ is a constant times the amplitude of the sound source $s_{i}(\\omega,T)$ and a correction is needed.</p><p align=\"justify\">In this method, a similar amplitude change at each frequency level also occurred in the preprocessing by CDWT introduced. This phenomenon is called a scaling irregularity. Furthermore,same as in the traditional method, it is also possible that the separated sound is replaced with noise at every frequencylevel mutually. This phenomenon is called the permutationproblem. Therefore, after these two problems are solved, theinverse complex discrete wavelet transform is performed,and it is necessary to restore the sound signal observed witheach mike.</p><h3 id=\"scaling-problem-and-correction-rule\">Scaling problem and correction rule</h3><p>Not only the amplitude of the restored signal but the phase also differs depending on the frequency by making for signal whitening (making no correlation). To take an arbitrary complex value by processing, this is caused. In order to solve the scaling irregularity problem, a method hasbeen proposed by Murata that uses the independent element $u_{i}(\\omega,T)$ , which is obtained at each frequency and divides the spectrum. In this study, the method of correcting scaling is adopted from the divided spectrum and can be shown as follows:</p>\\[v_{1}(\\omega,T)=\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{12}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix}u_{1}(\\omega,T) \\\\0\\end{bmatrix}\\]\\[v_{2}(\\omega,T)=\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix} 0 \\\\u_{2}(\\omega,T) \\end{bmatrix}\\]<p>where $v_{11}(\\omega,T),v_{22}(\\omega,T)$ are divided spectrums. If the sum $v_{1}(\\omega,T)+v_{2}(\\omega,T)$ is calculated then the sum $v_{11}(\\omega,T)+v_{21}(\\omega,T)$ is the mixture signal $x_{1}(\\omega,T)$ and the sum $v_{11}(\\omega,T)+v_{22}(\\omega,T)$ is the mixture signal $x_{2}(\\omega,T)$</p><h3 id=\"permutation-problem-and-correction-rule\">Permutation problem and correction rule</h3><p align=\"justify\">The complex Fast-ICA performs separation based onnon-Gaussian characteristics. Therefore, the possibility ofthe separating signal with higher non-Gaussiancharacteristics being output as the first channel is very high.However, the height of the frequency is not determined andnoise with low non-Gaussian characteristics might also beoutput. </p><p>Therefore, there is a possibility that the output of each frequency level is separated without being united by the sound. The separated signal $u_{1}(\\omega,T), u_{2}(\\omega,T)$ of the 1st and 2nd channel without permutation is shown as follows.</p>\\[u_{1}(\\omega ,k) \\approx s_{1}(\\omega,T) \\tag{5} ; u_{2}(\\omega ,k) \\approx s_{2}(\\omega,T)\\]<p>The above expression can be shown as the following equation by using the divided spectrum of the scaling correction.</p>\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]<p>On the other hand, when permutation occurs, (5) becomes the next expression</p>\\[u_{1}(\\omega ,k) \\approx s_{2}(\\omega,T) \\tag{6} ; u_{2}(\\omega ,k) \\approx s_{1}(\\omega,T)\\]\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]<p>From these, we can know that the divided spectrum can be shown as a multiplication of the separated signal $u_{i}(\\omega,T)$ and the transfer function $g_{ni}(\\omega)$ , which is from the sound source to the mike.</p><p>The separation matrix $W(\\omega)$ and the whitening matrix $Q(\\omega)$ presumed by the ICA processing are used and the next equation can be obtained.</p>\\[D = (W(\\omega)Q(\\omega))^{-1} = \\begin{bmatrix}g_{11}(\\omega) &amp; g_{12}(\\omega) \\\\g_{21}(\\omega) &amp; g_{22}(\\omega)\\end{bmatrix} = e\\begin{bmatrix}a_{11}(\\omega) &amp; a_{12}(\\omega) \\\\a_{21}(\\omega) &amp; a_{22}(\\omega)\\end{bmatrix} = eA_{ni}(\\omega), e \\in \\mathbb{R}\\]<h2 id=\"conclusion\">Conclusion</h2><p>CDWT +Complex Value Fast ICA outperforms, STFT+ Complex value Fast ICA and DWT+Real Value Fast ICA</p><p>Reference : BLIND SOURCE SEPARATION BY COMBINING INDEPANDENT COMPONENT ANALYSIS WITH COMPLEX DISCRETE WAVELET TRANSFORM ZHONG ZHANG , TAKESHI ENOMOTO , TETSUO MIYAKE , TAKASHI IMAMURA</p>",
            "url": "http://localhost:4000/2017/04/08/ica-with-cdwt",
            
            
            
            "tags": ["BlindSourceSeperation","ICA","CDWT","STFT","DWT","wavelet","whitening","ICAissues"],
            
            "date_published": "2017-04-08T00:00:00+01:00",
            "date_modified": "2017-04-08T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/07/piecewise-linear-representation",
            "title": "Piecewise linear representation of time series",
            "summary": "Piecewise linear representation of time series",
            "content_text": "The time series data usually fluctuate frequently and exit a lot of noise. So data mining in the original sequence data directly will not only cost highly in the storage and computation, but also probably affect the accuracy and reliability of the data mining algorithms. Therefore, many time series models are proposed, which can transform original series to new series. Modeling may not only compress the data, but also keep the main form and ignore fine changes. Accordingly, it can help improve the efficiency and accuracy of the data mining algorithms, which will provide policy support for data analysts.  Piecewise Linear Representation Based on Important Point: Pratt and Fink proposed a piecewise linear representation based on the important points. The important points are defined as the points which are the extreme points within the local scope and the ratio of the important point and the endpoint exceeds the parameters $R$. After extracting the important points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series and get various piecewise linear representation with different fine and granularity by selecting different parameters  $R$.  Piecewise Aggregate Approximation (PAA): Keogh and Yi proposed the method of the piecewise aggregate approximation independently. The algorithm divides the time series by the same time width and each sub-segment is represent by the average of the sub-segment. The method is simple, intuitionistic. It not only can support the similarity queries, all the Minkowski metric and the weighted Euclidean distance, but also can be used to index to improve query efficiency.  Piecewise Linear Representation based on the characteristic points: Xiao  proposed a method of piecewise linear representation based on the characteristic points. After extracting the characteristic points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series.  Piecewise Linear Representation Based on Slope Extract Edge Point (SEEP):** ZHAN Yan-Yan brought forward a new piecewise linear representation combining slope with the characteristics of time series. The algorithm can select some change points according to the rate of slope change firstly, and then combines the points with the line sequentially. Finally it will generate a new time series.The literatures above are analyzed as follows:The piecewise linear representation gets some characteristics (e.g., extreme point, trend, etc.) of each section by segmenting the series mainly. The above methods not only have the advantages of simple and intuitive, but also can support dynamic incremental updates, clustering, fast similarity search, and so on. But the cost and fitting error is different.Piecewise Linear Representation of Time Series based on Slope Change Threshold (SCT):Firstly, the algorithm calculates the two segments’ slope of the certain point connecting with thetwo adjacent points(except the two endpoints of time series).Secondly, it determines the change points by the ratio of slope. And then it combines the points with the line orderly. In this way, a new time series arises.The key of the algorithm is determining the change points. The change points must follow thefollowing principles:  The first point and last point are both change point;  When the slope of the line combining the certain point with its left neighboring point is zero, welook on the point as change point if the slope of the line combining the certain point with its rightneighboring point is out the range of$(-d,+d);$  When the slope of the line combining the certain point with its left neighboring point is not zero, we look on the point as change point if the slope ratio of two lines is beyond the range of $(1-d,1+d).$ The two lines refer to the line which combines the certain point with its right neighboring point and the line which combines the certain point with its left neighboring point. Above $d$ is a threshold parameter.",
            "content_html": "<p align=\"justify\">The time series data usually fluctuate frequently and exit a lot of noise. So data mining in the original sequence data directly will not only cost highly in the storage and computation, but also probably affect the accuracy and reliability of the data mining algorithms. Therefore, many time series models are proposed, which can transform original series to new series. Modeling may not only compress the data, but also keep the main form and ignore fine changes. Accordingly, it can help improve the efficiency and accuracy of the data mining algorithms, which will provide policy support for data analysts.</p><ol>  <li><u><b>Piecewise Linear Representation Based on Important Point:</b></u> Pratt and Fink proposed a piecewise linear representation based on the important points. The important points are defined as the points which are the extreme points within the local scope and the ratio of the important point and the endpoint exceeds the parameters $R$. After extracting the important points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series and get various piecewise linear representation with different fine and granularity by selecting different parameters  $R$.</li>  <li><u><b>Piecewise Aggregate Approximation (PAA):</b></u> Keogh and Yi proposed the method of the piecewise aggregate approximation independently. The algorithm divides the time series by the same time width and each sub-segment is represent by the average of the sub-segment. The method is simple, intuitionistic. It not only can support the similarity queries, all the Minkowski metric and the weighted Euclidean distance, but also can be used to index to improve query efficiency.</li>  <li><u><b>Piecewise Linear Representation based on the characteristic points:</b></u> Xiao  proposed a method of piecewise linear representation based on the characteristic points. After extracting the characteristic points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series.</li>  <li><u><b>Piecewise Linear Representation Based on Slope Extract Edge Point (SEEP)</b></u>:** ZHAN Yan-Yan brought forward a new piecewise linear representation combining slope with the characteristics of time series. The algorithm can select some change points according to the rate of slope change firstly, and then combines the points with the line sequentially. Finally it will generate a new time series.</li></ol><p>The literatures above are analyzed as follows:</p><p align=\"justify\">The piecewise linear representation gets some characteristics (e.g., extreme point, trend, etc.) of each section by segmenting the series mainly. The above methods not only have the advantages of simple and intuitive, but also can support dynamic incremental updates, clustering, fast similarity search, and so on. But the cost and fitting error is different.</p><p align=\"justify\"><u><b>Piecewise Linear Representation of Time Series based on Slope Change Threshold (SCT):</b></u>Firstly, the algorithm calculates the two segments’ slope of the certain point connecting with thetwo adjacent points(except the two endpoints of time series).Secondly, it determines the change points by the ratio of slope. And then it combines the points with the line orderly. In this way, a new time series arises.</p><p>The key of the algorithm is determining the change points. The change points must follow thefollowing principles:</p><ol>  <li>The first point and last point are both change point;</li>  <li>When the slope of the line combining the certain point with its left neighboring point is zero, welook on the point as change point if the slope of the line combining the certain point with its rightneighboring point is out the range of$(-d,+d);$</li>  <li>When the slope of the line combining the certain point with its left neighboring point is not zero, we look on the point as change point if the slope ratio of two lines is beyond the range of $(1-d,1+d).$ The two lines refer to the line which combines the certain point with its right neighboring point and the line which combines the certain point with its left neighboring point. Above $d$ is a threshold parameter.</li></ol>",
            "url": "http://localhost:4000/2017/04/07/piecewise-linear-representation",
            
            
            
            "tags": ["timeseries","representation","piecewiseLinearRepresentation","changepoint","slope"],
            
            "date_published": "2017-04-07T00:00:00+01:00",
            "date_modified": "2017-04-07T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/06/blind-source-seperation-ica",
            "title": "ICA",
            "summary": "Blind source seperation based on ICA",
            "content_text": "The main idea can be briefly expressed by the following mixed model:\\(x(t) = \\mathbf{A} s(t) + n(t)\\)The statistical model in the above equation is called ICA model, which describes how the observed data are mixed through the components $s(t)$ . The $m$ dimension column vector $x(t)$ is the observed data. $\\mathbf{A}$ is a $m \\times n$ mixing matrix; $n(t)$ denotes the additive noise vector. The matrix $\\mathbf{A}$ is assumed to be unknown. All we observe is the random vector x(t) , and we must estimate both Aand s(t) . Since $\\mathbf{A}$ is unknown, so $s(t)$ seems to be unsolvable.Fortunately, there are many mathematical methods for calculating the coefficients of $\\mathbf{A}$ by requiring the High-Order Statistics (HOS) information during the search for independentcomponents.",
            "content_html": "<p>The main idea can be briefly expressed by the following mixed model:\\(x(t) = \\mathbf{A} s(t) + n(t)\\)The statistical model in the above equation is called ICA model, which describes how the observed data are mixed through the components $s(t)$ . The $m$ dimension column vector $x(t)$ is the observed data. $\\mathbf{A}$ is a $m \\times n$ mixing matrix; $n(t)$ denotes the additive noise vector. The matrix $\\mathbf{A}$ is assumed to be unknown. All we observe is the random vector x(t) , and we must estimate both Aand s(t) . Since $\\mathbf{A}$ is unknown, so $s(t)$ seems to be unsolvable.</p><p>Fortunately, there are many mathematical methods for calculating the coefficients of $\\mathbf{A}$ by requiring the High-Order Statistics (HOS) information during the search for independentcomponents.</p>",
            "url": "http://localhost:4000/2017/04/06/blind-source-seperation-ica",
            
            
            
            "tags": ["BlindSourceSeperation","ICA"],
            
            "date_published": "2017-04-06T00:00:00+01:00",
            "date_modified": "2017-04-06T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition",
            "title": "Extreme Point Symmetric Mode Decomposition",
            "summary": "How to extract trend based on Extreme Point Symmetric Mode Decomposition",
            "content_text": "Considering the extraction methods of trend item, including the difference method, average slope method, moving average method, low pass filtering method, and least square fitting method, the type of trend term often needs to be presupposed. These methods are not suitable for processing the nonstationary signals with complex or random change trends.According to previous studies, the wavelet transform-based method is required for preselecting the wavelet basis and decomposition levels. This method is influenced easily byartificial factors and has no self-adaptability. The method based on Empirical mode decomposition (EMD) can adaptively decompose non-stationary signals regardless of the type of trend term. But, EMD is affected by mode mixing andend effect, causing the decomposed trend function is rough and the extraction accuracy is restricted. Professor Wang et al. recently proposed a self-adaptive method called ESMD. The ESMD is a novel development derived from Hilbert Huang transform that can be used to process non-stationary signal. ESMD has been applied to many studies.ESMD  Step 1: Identify all local extreme points, including maxima and minima points, of the data $Y$. Mark them as $E_{i} (1 ≤ i ≤ n);$  Step 2: Connect all adjacent Ei with line segments, and mark their midpoints as $F_{i} (1 ≤ i ≤ n-1);$  Step 3: Add left and right boundary midpoints $F_{0}$ and $F_{n}$ using linear interpolation method;  Step 4: Construct $p$ interpolating curves $L_{1}, L_{2},\\dots L_{p} (p≥1)$ with all $n+1$ midpoints and calculate their mean value by using equation $L^{\\ast}$.\\[L^{*}= \\frac{L_{1}+L_{2}+\\dots + L_{p}}{p}\\]  Step 5: Repeat steps $1$ to $4$ on $Y - L^{\\ast}$ until $||L^{\\ast}|| \\le \\epsilon$ ($\\epsilon$ is a permitted error), or until the sifting times attain a preset maximum number $K$. Then, the first mode $M_{1}$ is obtained.  Step 6: Repeat steps $1$ to $5$ on the residual $Y - M_1$ and obtain $M_2, M_3 \\dots$ until the last residual $R$ with no more than a certain number of extreme points.  Step 7: Change the maximum number $K$ on a finite integer interval $[K_{min}, K_{max}],$ and repeat all previous steps. Calculate the variance $\\sigma^2$ of $Y - R$ and plot a figure with $\\frac{\\sigma}{\\sigma_{0}}$ and $K$, $\\sigma_{0}$ is the standard deviation of $Y$.\\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-r_{i})^2\\]\\[\\sigma_{0}^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-\\bar{Y})^2\\]  Step 8 : Identify the number $K_0$ which corresponds to the minimum  $\\frac{\\sigma}{\\sigma_{0}}$  in $[K_{min}, K_{max}].$  Use this $K_{0}$ to repeat steps $1$ to $6$ and obtain the whole modes. Then last residual $R$ is an optimal Adaptive global mean (AGM) curve.Based on the steps above, ESMD can decompose signal into limited intrinsic mode functions and a residual component. The residual component $R$ is an optimal AGM curve that can be considered as the trend term of the original signal.The extraction error of EMD is larger than that of ESMD.   The EMD-based extraction method can adaptively extract the signal trend. Whereas, the extracted trend curve limits the number of its extreme points (no more than 1), and no optimal strategy to find it. Therefore, the extraction error of EMD is relatively larger than that of ESMD.  The ESMD-based extraction method has commendable self-adaptability. It can obtain the signal trend with high precision using adaptive decomposition and optimization. The trend type of signal does not need to be preset. And, the extraction results of ESMD are better than that of EMD.Reference: Adaptive extraction method for trend term of machinery signal based onextreme-point symmetric mode decomposition - Yong Zhu, Wan-lu Jiang and Xiang-dong Kong",
            "content_html": "<p align=\"justify\">Considering the extraction methods of trend item, including the difference method, average slope method, moving average method, low pass filtering method, and least square fitting method, the type of trend term often needs to be presupposed. These methods are not suitable for processing the nonstationary signals with complex or random change trends.</p><p align=\"justify\">According to previous studies, the wavelet transform-based method is required for preselecting the wavelet basis and decomposition levels. This method is influenced easily byartificial factors and has no self-adaptability. </p><p align=\"justify\">The method based on Empirical mode decomposition (EMD) can adaptively decompose non-stationary signals regardless of the type of trend term. But, EMD is affected by mode mixing andend effect, causing the decomposed trend function is rough and the extraction accuracy is restricted. </p><p align=\"justify\">Professor Wang et al. recently proposed a self-adaptive method called ESMD. The ESMD is a novel development derived from Hilbert Huang transform that can be used to process non-stationary signal. ESMD has been applied to many studies.</p><h2 id=\"esmd\">ESMD</h2><ul>  <li><strong>Step 1:</strong> Identify all local extreme points, including maxima and minima points, of the data $Y$. Mark them as $E_{i} (1 ≤ i ≤ n);$</li>  <li><strong>Step 2:</strong> Connect all adjacent Ei with line segments, and mark their midpoints as $F_{i} (1 ≤ i ≤ n-1);$</li>  <li><strong>Step 3:</strong> Add left and right boundary midpoints $F_{0}$ and $F_{n}$ using linear interpolation method;</li>  <li><strong>Step 4:</strong> Construct $p$ interpolating curves $L_{1}, L_{2},\\dots L_{p} (p≥1)$ with all $n+1$ midpoints and calculate their mean value by using equation $L^{\\ast}$.</li></ul>\\[L^{*}= \\frac{L_{1}+L_{2}+\\dots + L_{p}}{p}\\]<ul>  <li><strong>Step 5:</strong> Repeat steps $1$ to $4$ on $Y - L^{\\ast}$ until $||L^{\\ast}|| \\le \\epsilon$ ($\\epsilon$ is a permitted error), or until the sifting times attain a preset maximum number $K$. Then, the first mode $M_{1}$ is obtained.</li>  <li><strong>Step 6:</strong> Repeat steps $1$ to $5$ on the residual $Y - M_1$ and obtain $M_2, M_3 \\dots$ until the last residual $R$ with no more than a certain number of extreme points.</li>  <li><strong>Step 7:</strong> Change the maximum number $K$ on a finite integer interval $[K_{min}, K_{max}],$ and repeat all previous steps. Calculate the variance $\\sigma^2$ of $Y - R$ and plot a figure with $\\frac{\\sigma}{\\sigma_{0}}$ and $K$, $\\sigma_{0}$ is the standard deviation of $Y$.</li></ul>\\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-r_{i})^2\\]\\[\\sigma_{0}^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-\\bar{Y})^2\\]<ul>  <li><strong>Step 8 :</strong> Identify the number $K_0$ which corresponds to the minimum  $\\frac{\\sigma}{\\sigma_{0}}$  in $[K_{min}, K_{max}].$  Use this $K_{0}$ to repeat steps $1$ to $6$ and obtain the whole modes. Then last residual $R$ is an optimal Adaptive global mean (AGM) curve.</li></ul><p>Based on the steps above, ESMD can decompose signal into limited intrinsic mode functions and a residual component. The residual component $R$ is an optimal AGM curve that can be considered as the trend term of the original signal.</p><p><u>The extraction error of EMD is larger than that of ESMD. </u></p><ol>  <li>The EMD-based extraction method can adaptively extract the signal trend. Whereas, the extracted trend curve limits the number of its extreme points (no more than 1), and no optimal strategy to find it. Therefore, the extraction error of EMD is relatively larger than that of ESMD.</li>  <li>The ESMD-based extraction method has commendable self-adaptability. It can obtain the signal trend with high precision using adaptive decomposition and optimization. The trend type of signal does not need to be preset. And, the extraction results of ESMD are better than that of EMD.</li></ol><p>Reference: Adaptive extraction method for trend term of machinery signal based onextreme-point symmetric mode decomposition - Yong Zhu, Wan-lu Jiang and Xiang-dong Kong</p>",
            "url": "http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition",
            
            
            
            "tags": ["trend_extraction","EMD","ESMD"],
            
            "date_published": "2017-04-05T00:00:00+01:00",
            "date_modified": "2017-04-05T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/03/pca-ica-1d-timeseries",
            "title": "PCA ICA for 1D timeseries",
            "summary": "How to apply ICA, PCA for fault detection from single sensor?",
            "content_text": "IdeaPrincipal component analysis (PCA) isa method that transforms multiple data series into uncorrelated data series. Independent component analysis ICA is a method that separates multiple data series into independent data series. However, both require signals from at least two separate sensors. To overcome this requirement and utilize the fault detection capability of ICA and PCA, we propose to use wavelet transform to pre-process the data collected from a single sensor and then use the coefficients of the wavelet transforms at different scales as input to ICA and PCA. Independent components analysis (ICA) requires little prior knowledge about the components to be isolated; however, at least two sensors must be available for signal collection and the number of sensors must be at least equal to the number of sources to be separated and this method cannot be applied directly when there is only one sensor collecting signals.Principal component analysis (PCA) is a multivariate data analysis technique that transforms a set of correlated variables into a set of uncorrelated variables. Each member of the resulting set of uncorrelated variables is called a principal component. We are interested in determining its suitability for fault detection because one of the identified principal components may reveal the signature of a hidden fault. As with ICA, however, this method cannot be applied directly when only a single variable is observed.Wavelet transform may be considered as a series of band pass filters when applied to the datacollected from a single sensor. The results of the transform, which exist in different frequencyregions, say $N$ regions, may be considered as different mixtures of the sources that havegenerated the collected signals.These $N$ groups of data can then be used as input to ICA or PCAfor identification of the hidden sources.ICA and PCAICA is a technique for separating independent sources linearly mixed in signals. Suppose thatthere are $N$ independent sources of vibration, and $N$ sensors at different locations are used torecord vibration signals. The signals recorded by each sensor come from different sources withdifferent mixing ratios. Let $s_{1}(t),s_{2}(t),  \\dots ,s_{N}(t)$ be the signals produced by the $N$ independentsources and $x_{1}(t),x_{2}(t),  \\dots ,x_{N}(t)$ be the observations from the $N$ sensors. The sensors record these signals simultaneously. The task of ICA is to estimate the mixing ratios of the source signals in the collected signals and obtain the independent source signals.To identify the independent components successfully, we need a rule for evaluating theindependency of the identified components. According to the Central Limit Theorem, thedistribution of the sum of a large number of independent random variables tends to a Gaussiandistribution. Since the collected signals are weighted sums of the independent sources, the sourcesto be isolated must have less Gaussianity than the collected signals. Thus, non-Gaussianity can beused for separating independent components. Hyvarinen and Oja proposed to use negentropyto evaluate the non-Gaussianity of the separated components so as to evaluate separationperformance. With this concept, we can seek the separation that provides the least Gaussianness of the separated components. The popular FastICA algorithm proposed by Havarinen and Ojaisoften used to carry out the ICA procedure.PCA is a technique that obtains linear transformations of a group of correlated variables suchthat the transformed variables are uncorrelated. For example, consider two variables, $x_{1}$ and$x_{2}$. For each variable, we have obtained the following $N$ observations:\\(x_{11}, x_{12}, \\dots , x_{1N}; x_{21}, x_{22}, \\dots; x_{2N}\\)where $x_{1i}$ and $x_{2j}$ denote the $i^{th}$ and the $j^{th}$ observations of variables $x_{1}$ and $x_{2}$, respectively. The PCA method seeks two new axes, D1 and D2, that make the projectionsof the collected data ontoD1 have the largest variability and at the same time, the projections of the collected data onto D2have the smallest variability. This way, we have expressed the collected data as their two principalcomponents. Most of the variation in the original data is explained by the first principalcomponent, D1, and the remaining variation in the original data isexplained by the secondprincipal component, D2.ICA renders the separated components independent of one another while PCA renderstheseparated components uncorrelated with one another. PCA separates the components based onlyon the second-order cumulant while ICA separates the components on high-order cumulants.Therefore, ICA can be considered a generalization of PCA.Method of preprocessing to apply ICA or PCA on 1D Time seriesThe available data is a single time series. To apply ICA or PCA for feature extraction, we needto have more than one time series. A method to generate multiple time series from the single available time series is given below.Wavelet transform decomposes a signal series in the time domain into a two-dimensionalfunction in the time-scale (frequency) plane. The wavelet coefficients measure the time-scale (frequency) content in a signal indexed by the scale parameter and the translation parameter. Let$\\varphi(t)$ be the mother wavelet. The wavelet family consists of a series of daughter wavelets that aregenerated by dilation and translation from the mother wavelet $\\varphi(t)$\\[\\varphi_{a, b}(t)=\\sqrt{|a|} \\varphi[(t-b) / a]\\]where $a$ is the scale parameter, $b$ isthe location parameter, and $\\sqrt{  |a|}$isused to guarantee energy preservation. The wavelet transform of signal $x(t)$ isdefined as the inner product of $\\varphi_{a, b}(t)$ and $x(t)$ in the Hilbert space of $L^2$ norm defined as:\\[W(a, b)=\\left\\langle\\varphi_{a, b}(t), x(t)\\right\\rangle=\\int x(t) \\varphi_{a, b}^*(t) \\mathrm{d} t\\]where the symbol * stands for the complex conjugate.Wavelet transform can be thought of as a series of band pass filters. The results of thetransform, which exist in different frequency regions, may be thought of as different mixtures of the independent sources. These different mixtures may be considered to be signals collected atdifferent ‘‘locations’’, or more accurately, through different ‘‘sensors’’ with different frequencyranges. This way, the one-dimensional signal is transformed into multidimensional datathat satisfy the requirements of ICA and PCA. The preprocessing of the one-dimensional data with wavelet transform makes ICA and PCA usable for identification of a hiddensource.Reference :Feature separation using ICA for a one-dimensional time series and its application in fault detection - Ming J. Zuo, Jing Lin, Xianfeng Fan",
            "content_html": "<h3 id=\"idea\">Idea</h3><p align=\"justify\">Principal component analysis (PCA) isa method that transforms multiple data series into uncorrelated data series. Independent component analysis <a class=\"internal-link\" href=\"/2017/04/06/blind-source-seperation-ica\">ICA</a> is a method that separates multiple data series into independent data series. However, both require signals from at least two separate sensors. To overcome this requirement and utilize the fault detection capability of ICA and PCA, we propose to use wavelet transform to pre-process the data collected from a single sensor and then use the coefficients of the wavelet transforms at different scales as input to ICA and PCA. </p><p align=\"justify\">Independent components analysis (ICA) requires little prior knowledge about the components to be isolated; however, at least two sensors must be available for signal collection and the number of sensors must be at least equal to the number of sources to be separated and this method cannot be applied directly when there is only one sensor collecting signals.</p><p align=\"justify\">Principal component analysis (PCA) is a multivariate data analysis technique that transforms a set of correlated variables into a set of uncorrelated variables. Each member of the resulting set of uncorrelated variables is called a principal component. We are interested in determining its suitability for fault detection because one of the identified principal components may reveal the signature of a hidden fault. As with ICA, however, this method cannot be applied directly when only a single variable is observed.</p><p>Wavelet transform may be considered as a series of band pass filters when applied to the datacollected from a single sensor. The results of the transform, which exist in different frequencyregions, say $N$ regions, may be considered as different mixtures of the sources that havegenerated the collected signals.These $N$ groups of data can then be used as input to ICA or PCAfor identification of the hidden sources.</p><h3 id=\"ica-and-pca\">ICA and PCA</h3><p>ICA is a technique for separating independent sources linearly mixed in signals. Suppose thatthere are $N$ independent sources of vibration, and $N$ sensors at different locations are used torecord vibration signals. The signals recorded by each sensor come from different sources withdifferent mixing ratios. Let $s_{1}(t),s_{2}(t),  \\dots ,s_{N}(t)$ be the signals produced by the $N$ independentsources and $x_{1}(t),x_{2}(t),  \\dots ,x_{N}(t)$ be the observations from the $N$ sensors. The sensors record these signals simultaneously. The task of ICA is to estimate the mixing ratios of the source signals in the collected signals and obtain the independent source signals.</p><p align=\"justify\">To identify the independent components successfully, we need a rule for evaluating theindependency of the identified components. According to the Central Limit Theorem, thedistribution of the sum of a large number of independent random variables tends to a Gaussiandistribution. Since the collected signals are weighted sums of the independent sources, the sourcesto be isolated must have less Gaussianity than the collected signals. Thus, non-Gaussianity can beused for separating independent components. Hyvarinen and Oja proposed to use negentropyto evaluate the non-Gaussianity of the separated components so as to evaluate separationperformance. With this concept, we can seek the separation that provides the least Gaussianness of the separated components. The popular FastICA algorithm proposed by Havarinen and Ojaisoften used to carry out the ICA procedure.</p><p>PCA is a technique that obtains linear transformations of a group of correlated variables suchthat the transformed variables are uncorrelated. For example, consider two variables, $x_{1}$ and$x_{2}$. For each variable, we have obtained the following $N$ observations:\\(x_{11}, x_{12}, \\dots , x_{1N}; x_{21}, x_{22}, \\dots; x_{2N}\\)where $x_{1i}$ and $x_{2j}$ denote the $i^{th}$ and the $j^{th}$ observations of variables $x_{1}$ and $x_{2}$, respectively. The PCA method seeks two new axes, D1 and D2, that make the projectionsof the collected data ontoD1 have the largest variability and at the same time, the projections of the collected data onto D2have the smallest variability. This way, we have expressed the collected data as their two principalcomponents. Most of the variation in the original data is explained by the first principalcomponent, D1, and the remaining variation in the original data isexplained by the secondprincipal component, D2.</p><p align=\"justify\">ICA renders the separated components independent of one another while PCA renderstheseparated components uncorrelated with one another. PCA separates the components based onlyon the second-order cumulant while ICA separates the components on high-order cumulants.Therefore, ICA can be considered a generalization of PCA.</p><h3 id=\"method-of-preprocessing-to-apply-ica-or-pca-on-1d-time-series\">Method of preprocessing to apply ICA or PCA on 1D Time series</h3><p>The available data is a single time series. To apply ICA or PCA for feature extraction, we needto have more than one time series. A method to generate multiple time series from the single available time series is given below.</p><p>Wavelet transform decomposes a signal series in the time domain into a two-dimensionalfunction in the time-scale (frequency) plane. The wavelet coefficients measure the time-scale (frequency) content in a signal indexed by the scale parameter and the translation parameter. Let$\\varphi(t)$ be the mother wavelet. The wavelet family consists of a series of daughter wavelets that aregenerated by dilation and translation from the mother wavelet $\\varphi(t)$</p>\\[\\varphi_{a, b}(t)=\\sqrt{|a|} \\varphi[(t-b) / a]\\]<p>where $a$ is the scale parameter, $b$ isthe location parameter, and $\\sqrt{  |a|}$isused to guarantee energy preservation. The wavelet transform of signal $x(t)$ isdefined as the inner product of $\\varphi_{a, b}(t)$ and $x(t)$ in the Hilbert space of $L^2$ norm defined as:</p>\\[W(a, b)=\\left\\langle\\varphi_{a, b}(t), x(t)\\right\\rangle=\\int x(t) \\varphi_{a, b}^*(t) \\mathrm{d} t\\]<p>where the symbol * stands for the complex conjugate.</p><p>Wavelet transform can be thought of as a series of band pass filters. The results of thetransform, which exist in different frequency regions, may be thought of as different mixtures of the independent sources. These different mixtures may be considered to be signals collected atdifferent ‘‘locations’’, or more accurately, through different ‘‘sensors’’ with different frequencyranges. This way, the one-dimensional signal is transformed into multidimensional datathat satisfy the requirements of ICA and PCA. The preprocessing of the one-dimensional data with wavelet transform makes ICA and PCA usable for identification of a hiddensource.</p><p><img src=\"/assets/snips/img1.png\" alt=\"Flowchart\" /></p><h3 id=\"reference-\">Reference :</h3><p>Feature separation using ICA for a one-dimensional time series and its application in fault detection - Ming J. Zuo, Jing Lin, Xianfeng Fan</p>",
            "url": "http://localhost:4000/2017/04/03/pca-ica-1d-timeseries",
            
            
            
            "tags": ["ICA","faultdetection","wavelet","PCA","sensors","preprocessing"],
            
            "date_published": "2017-04-03T00:00:00+01:00",
            "date_modified": "2017-04-03T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        }
    
    ]
}