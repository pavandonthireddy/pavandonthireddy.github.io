{
    "version": "https://jsonfeed.org/version/1",
    "title": "Pavan Donthireddy",
    "home_page_url": "http://localhost:4000/",
    "feed_url": "http://localhost:4000/feed.json",
    "description": "Digital space of notes and thoughts",
    "icon": "http://localhost:4000/apple-touch-iconfavicon.png",
    "favicon": "http://localhost:4000/favicon.png",
    "expired": false,
    
    "author":  {
        "name": "Pavan Donthireddy",
        "url": "https://pavandonthireddy.github.io",
        "avatar": null
    },
    
"items": [
    
        {
            "id": "http://localhost:4000/2017/04/11/sparse-representation",
            "title": "Sparse representation of signals",
            "summary": "Sparse representation of signals",
            "content_text": "Sparse representation is widely employed for expressing signals using very few linear combinations of elementary signals. These elementary signals are called atoms. Since thenumber of the atoms is more than the dimension of the signalspace, any signal can be represented by linear combinationsof these atoms and the representations are not unique.Sparse representation is to use the minimum number of atoms to express the signals and this is actually an $L_{0}$ norm optimization problem. That is for a given overcomplete dictionary $A\\in\\Re^{N \\times M}$ and a signal $x\\in R^{N \\times 1}$, where $N&lt;M$ and $R^{a \\times b}$ denotes the space of $a \\times b$ real valued matrices, the representation problem is to find $z\\in\\Re^{M \\times 1}$ such that $x=Az$ and $\\lVert z\\lVert_{0}$ is minimized. That is \\(z_{0}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{0} \\text{  subject to  } x=Az \\tag{1}\\)Here $\\lVert z\\lVert_{0}$ denotes $L_{0}$ norm of $z$, which is equivalent to the number of nonzero elements in $z$.The problem defined in (1) is nonconvex, nonsmooth and NP hard, it requires an exhaustive search for finding the solution. An approximate solution can be obrtained by solving the corresponding $L_{1}$ norm optimization problem if the isometry condition is satisfied. The $L_{1}$ norm optimization is as follows\\[z_{1}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } x=Az \\tag{2}\\]Although $z_{1}^{\\ast}$ is a good approximation of $z_{0}^{\\ast}$ when the isometry condition is satisifed, these two solutions will be very different if $x$ contains significant amount of noise. Nevertheless, this is the typical case in practical circumstances. Hence the exact equality constraint is usually related to an inequality constraint as follows.\\[z^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } \\lVert Az-x\\lVert_{\\infty} \\le \\epsilon \\tag{3}\\]where $\\epsilon$ is the specification on the maximium absolute difference between $Az$ and $x$. This problem can be efficiently solved via reformulating the $L_{1}$ norm optimization problem to a linear programming problem.",
            "content_html": "<p align=\"justify\">Sparse representation is widely employed for expressing signals using very few linear combinations of elementary signals. These elementary signals are called atoms. Since thenumber of the atoms is more than the dimension of the signalspace, any signal can be represented by linear combinationsof these atoms and the representations are not unique.</p><p>Sparse representation is to use the minimum number of atoms to express the signals and this is actually an $L_{0}$ norm optimization problem. That is for a given overcomplete dictionary $A\\in\\Re^{N \\times M}$ and a signal $x\\in R^{N \\times 1}$, where $N&lt;M$ and $R^{a \\times b}$ denotes the space of $a \\times b$ real valued matrices, the representation problem is to find $z\\in\\Re^{M \\times 1}$ such that $x=Az$ and $\\lVert z\\lVert_{0}$ is minimized. That is \\(z_{0}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{0} \\text{  subject to  } x=Az \\tag{1}\\)</p><p>Here $\\lVert z\\lVert_{0}$ denotes $L_{0}$ norm of $z$, which is equivalent to the number of nonzero elements in $z$.</p><p>The problem defined in (1) is nonconvex, nonsmooth and NP hard, it requires an exhaustive search for finding the solution. An approximate solution can be obrtained by solving the corresponding $L_{1}$ norm optimization problem if the isometry condition is satisfied. The $L_{1}$ norm optimization is as follows</p>\\[z_{1}^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } x=Az \\tag{2}\\]<p>Although $z_{1}^{\\ast}$ is a good approximation of $z_{0}^{\\ast}$ when the isometry condition is satisifed, these two solutions will be very different if $x$ contains significant amount of noise. Nevertheless, this is the typical case in practical circumstances. Hence the exact equality constraint is usually related to an inequality constraint as follows.</p>\\[z^{\\ast} = \\mathrm{argmin}_{z} \\lVert z\\lVert_{1} \\text{  subject to  } \\lVert Az-x\\lVert_{\\infty} \\le \\epsilon \\tag{3}\\]<p>where $\\epsilon$ is the specification on the maximium absolute difference between $Az$ and $x$. This problem can be efficiently solved via reformulating the $L_{1}$ norm optimization problem to a linear programming problem.</p>",
            "url": "http://localhost:4000/2017/04/11/sparse-representation",
            
            
            
            "tags": ["representation","sparse_binary_programming"],
            
            "date_published": "2017-04-11T00:00:00+01:00",
            "date_modified": "2017-04-11T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/10/ssa",
            "title": "Single Spectrum Analysis",
            "summary": "SSA",
            "content_text": "The main steps of SSA can be summarized as follows:For a time series $x(n)$ for $n=1,2\\dots N$, let the window length be $L$ , where  $1&lt;L&lt;N$. The first step of SSA is to construct a trajectory matrix as follows. Define the $L$ dimensional vectors as\\[X_{n}=\\begin{bmatrix} x(n)\\\\ \\vdots \\\\ x(n+L-1)\\end{bmatrix}\\]for $n =1,2,\\dots, N − L +1$.Denote $K = N − L +1$. These $K$ vectors are put into a matrix and the $L \\times K$ trajectory matrix is constructed as follows:\\[X = \\begin{bmatrix}X_{1} &amp; X_{2} &amp; \\dots &amp; X_{K}\\end{bmatrix}\\]The second step is to express $X$ as the sum of component matrices. Let $S=XX^T$ and the eigenvalues of $S$ be $\\lambda_{1}\\ge\\lambda_{2}\\dots\\ge\\lambda_{L}\\ge 0$. Define $D=\\max{j:\\lambda_{j}&gt;0}$. Let $U_{1},\\dots,U_{D}$ be the corresponding eigenvectors.Denote $V_{j}=\\frac{X^TU_{j}}{\\sqrt{ \\lambda_{j} }}$ for $j=1,2,\\dots,D$  be the factor vectors.Define:\\[\\tilde{X}_{j} =\\sqrt{\\lambda_{j}}U_{j}V_{j}^T\\]for $j=1,2,\\dots,D$. It can be shown that $X$ can be represented as\\[X = \\tilde{X}_{1}+\\dots+\\tilde{X}_{D}\\]The third step is to represent $X$ as the sum of grouped matrix components as follows. The indices set ${1,\\dots,D}$ is partitioned into $M$ disjoint subsets $I_{1}\\dots I_{M}$. Let $I_{m} ={i_{m_{1}},\\dots,{i_{m_{c}}}}$ for $m=1,\\dots, M$ and\\[\\tilde{X}_{I_{m}}=\\tilde{X}_{i_{m_{1}}}+\\dots+\\tilde{X}_{i_{m_{C}}}\\]Hence we have\\[X=\\tilde{X}_{I_{1}}+\\dots+\\tilde{X}_{I_{M}}\\]The final step is to reconstruct the signal by the diagonal averaging method.First, transform  new text $\\tilde{X}_{I_m}$ into new one dimensional signals of length $N$ by the hankelization like procedure.The vectors and the transform operator are denoted as $\\tilde{x}_{I_m}$  for $m=1,\\dots,M$ and  $\\Im(.)$ respectievely. That is\\[\\tilde{x}_{I_{m}} = \\Im(\\tilde{X}_{I_{m}}) ; m=1,\\dots,M\\]Thus the original time series can be expressed as a sum of $M$ series,\\[x(n)=\\tilde{x}_{I_{1}}(n)+\\dots+\\tilde{x}_{I_{M}}(n) ; n=1,\\dots,N\\]",
            "content_html": "<p>The main steps of SSA can be summarized as follows:</p><p>For a time series $x(n)$ for $n=1,2\\dots N$, let the window length be $L$ , where  $1&lt;L&lt;N$. The first step of SSA is to construct a trajectory matrix as follows. Define the $L$ dimensional vectors as</p>\\[X_{n}=\\begin{bmatrix} x(n)\\\\ \\vdots \\\\ x(n+L-1)\\end{bmatrix}\\]<p>for $n =1,2,\\dots, N − L +1$.</p><p>Denote $K = N − L +1$. These $K$ vectors are put into a matrix and the $L \\times K$ trajectory matrix is constructed as follows:</p>\\[X = \\begin{bmatrix}X_{1} &amp; X_{2} &amp; \\dots &amp; X_{K}\\end{bmatrix}\\]<p>The second step is to express $X$ as the sum of component matrices. Let $S=XX^T$ and the eigenvalues of $S$ be $\\lambda_{1}\\ge\\lambda_{2}\\dots\\ge\\lambda_{L}\\ge 0$. Define $D=\\max{j:\\lambda_{j}&gt;0}$. Let $U_{1},\\dots,U_{D}$ be the corresponding eigenvectors.</p><p>Denote $V_{j}=\\frac{X^TU_{j}}{\\sqrt{ \\lambda_{j} }}$ for $j=1,2,\\dots,D$  be the factor vectors.</p><p>Define:</p>\\[\\tilde{X}_{j} =\\sqrt{\\lambda_{j}}U_{j}V_{j}^T\\]<p>for $j=1,2,\\dots,D$. It can be shown that $X$ can be represented as</p>\\[X = \\tilde{X}_{1}+\\dots+\\tilde{X}_{D}\\]<p>The third step is to represent $X$ as the sum of grouped matrix components as follows. The indices set ${1,\\dots,D}$ is partitioned into $M$ disjoint subsets $I_{1}\\dots I_{M}$. Let $I_{m} ={i_{m_{1}},\\dots,{i_{m_{c}}}}$ for $m=1,\\dots, M$ and</p>\\[\\tilde{X}_{I_{m}}=\\tilde{X}_{i_{m_{1}}}+\\dots+\\tilde{X}_{i_{m_{C}}}\\]<p>Hence we have</p>\\[X=\\tilde{X}_{I_{1}}+\\dots+\\tilde{X}_{I_{M}}\\]<p>The final step is to reconstruct the signal by the diagonal averaging method.</p><p>First, transform  new text $\\tilde{X}_{I_m}$ into new one dimensional signals of length $N$ by the hankelization like procedure.</p><p>The vectors and the transform operator are denoted as $\\tilde{x}_{I_m}$  for $m=1,\\dots,M$ and  $\\Im(.)$ respectievely. That is</p>\\[\\tilde{x}_{I_{m}} = \\Im(\\tilde{X}_{I_{m}}) ; m=1,\\dots,M\\]<p>Thus the original time series can be expressed as a sum of $M$ series,</p>\\[x(n)=\\tilde{x}_{I_{1}}(n)+\\dots+\\tilde{x}_{I_{M}}(n) ; n=1,\\dots,N\\]",
            "url": "http://localhost:4000/2017/04/10/ssa",
            
            
            
            "tags": ["trend_extraction","SSA"],
            
            "date_published": "2017-04-10T00:00:00+01:00",
            "date_modified": "2017-04-10T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming",
            "title": "Trend extraction with SSA and Sparse Binary Programming",
            "summary": "Trend extraction with SSA and Sparse Binary Programming",
            "content_text": "The underlying trend is approximated by the sum of a part of SSA components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem canbe approximated by a linear programming problem when theisometry condition is satisfied, and the solution of the linearprogramming problem can be obtained via existing simplexmethods or interior point methods.By applying the binaryquantization to the obtained solution of the linear programmingproblem, the approximated solution of the original sparsebinary programming problem is obtained. Unlike previouslyreported techniques that require a pre-cursor model orparameter specifications, the proposed method is completelyadaptive.DetailsThe conventional approach for selecting SSAcomponents for extracting the underlying trend is to employonly the first several SSA components. However, thisselection rule fails when the underlying trend of a signal has acomplicated structure such as a high order polynomialstructure which cannot be characterized by only the firstseveral SSA components.The idea is to formulate theselection problem as a sparse binary programming problem and proposes an efficient methodology for approximatingthe solution of the problem. In particular, the selectionproblem is formulated as follows. The number of thecomponents to be selected is minimized subject to aspecification on the maximum absolute difference betweenthe approximated underlying trend and the original signal aswell as the binary valued constraint on the selectioncoefficients. Since the sparse binary programming problem isnonsmooth, nonconvex and NP hard, it requires an exhaustivesearch for finding the solution. As a result, the computationaleffort for finding the solution is very large and an efficientalgorithm for approximating the solution is very useful andimportant. To address these issues, the corresponding continuous valued optimization problem (the sameoptimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective functionsubject to an $L_{\\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem isobtained.MethodologySSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.The selection problem is formulated as the following sparse sparse binary programming problem.\\[z^{*}= \\begin{bmatrix}z_{1}^{*}&amp; \\dots, z_{D}^*\\end{bmatrix}^{T}= \\mathrm{argmin}_{z} \\lVert z\\lVert_{0}\\]subject to $\\lVert Az-x\\lVert_{\\infty} \\le \\epsilon$ and $z_{i}\\in{0,1}$ for $i=1,\\dots,D$.Here\\[A=[\\tilde{x}_1,\\dots,\\tilde{x}_D]\\in R^{N \\times D}\\]and\\[\\epsilon=0.5\\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\\]where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.If $z_{i}^{\\ast}=1$ (or $z_{i}^{\\ast}=0$), then the corresponding component $\\tilde{x}_i$ for $i=1,\\dots,D$ is selected (or excluded) for the representation of the underlying trend.Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimizationproblem is considered first. The solution of the $L_{0}$ normcontinuous valued optimization problem is approximated bythe solution of the corresponding $L_{1}$ norm continuous valuedoptimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:\\[y^{*}= [y_{1}^{*},\\dots,y_{D}^{*}]= \\mathrm{arg}\\min_{y}\\lVert y\\lVert_{1} \\text{  subject to } \\lVert Ay-x\\lVert_{\\infty}\\le \\epsilon\\]By further applying the quantization on $y_{i}^{*}$ for $i=1,\\dots,D$ to either $0$ or $1$ via the following operator.\\[W_{i}^{*}=\\begin{cases}1 &amp;  y_{i}^{*}\\ge 0.5\\\\0 &amp;  y_{i}^{*}&lt; 0.5\\end{cases}\\]the corresponding component $\\tilde{x}_{i}$for is selected (excluded) for the representation of the underlying trend if$W_{i}^{\\ast}=1( \\text{ or } W_{i}^{\\ast}=0)$Finally the underlying trend of the signal is obtained by\\[\\Gamma = AW^*\\]where $W^{\\ast}= [W_1^{\\ast},\\dots,W_D^{\\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original sparse binary programming problem. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programmingproblem. That is, $W^{\\ast}$ in is very close to $z^{\\ast}$ above.Therefore, the subset $I_{1}$ can be obtained by\\[I_{1} = \\{i\\vert W_{i}^{*}= 1, 1\\le i \\le D\\}\\]After obtaining the underlying trend $\\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.Denote $\\Gamma =[\\gamma(1), \\dots, \\gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume thatthere is an underlying structure in the time series and thisstructure is preserved for the time period to be predicted. Aprediction model based on the linear recurrent formulae (LRF)is employed.That is, the points $\\gamma(N-L+2), \\dots, \\gamma(N)$ are employed for the prediction of $\\gamma(N+1)$, and so on. In other words,\\[\\gamma(n+1) = \\sum_{k=0}^{L-2}a_{k}\\gamma(n-k) \\text{   for  } n\\ge N,\\]where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \\dots, a_{0})^T$ is given by\\[R = \\frac{1}{1-\\nu^{2}}\\sum_{i\\in I_{1}}\\pi_{i}U_{i}^{\\nabla}\\]Here, $\\nu^{2=}\\sum_{i\\in I_{1}}\\pi_{i}^2$, $\\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\\nabla }\\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \\in I_{1}.$",
            "content_html": "<p align=\"justify\">The underlying trend is approximated by the sum of a part of <a class=\"internal-link\" href=\"/2017/04/10/ssa\">SSA</a> components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.</p><p>As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.</p><p align=\"justify\">This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem canbe approximated by a linear programming problem when theisometry condition is satisfied, and the solution of the linearprogramming problem can be obtained via existing simplexmethods or interior point methods.</p><p align=\"justify\">By applying the binaryquantization to the obtained solution of the linear programmingproblem, the approximated solution of the original sparsebinary programming problem is obtained. Unlike previouslyreported techniques that require a pre-cursor model orparameter specifications, the proposed method is completelyadaptive.</p><h3 id=\"details\">Details</h3><p align=\"justify\">The conventional approach for selecting SSAcomponents for extracting the underlying trend is to employonly the first several SSA components. However, thisselection rule fails when the underlying trend of a signal has acomplicated structure such as a high order polynomialstructure which cannot be characterized by only the firstseveral SSA components.</p><p align=\"justify\">The idea is to formulate theselection problem as a sparse binary programming problem and proposes an efficient methodology for approximatingthe solution of the problem. In particular, the selectionproblem is formulated as follows. The number of thecomponents to be selected is minimized subject to aspecification on the maximum absolute difference betweenthe approximated underlying trend and the original signal aswell as the binary valued constraint on the selectioncoefficients. </p><p align=\"justify\">Since the sparse binary programming problem isnonsmooth, nonconvex and NP hard, it requires an exhaustivesearch for finding the solution. As a result, the computationaleffort for finding the solution is very large and an efficientalgorithm for approximating the solution is very useful andimportant. </p><p>To address these issues, the corresponding continuous valued optimization problem (the sameoptimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective functionsubject to an $L_{\\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.</p><p>By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem isobtained.</p><h2 id=\"methodology\">Methodology</h2><p>SSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.</p><p>Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.</p><p>However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.</p><p>In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.</p><p>The selection problem is formulated as the following sparse sparse binary programming problem.</p>\\[z^{*}= \\begin{bmatrix}z_{1}^{*}&amp; \\dots, z_{D}^*\\end{bmatrix}^{T}= \\mathrm{argmin}_{z} \\lVert z\\lVert_{0}\\]<p>subject to $\\lVert Az-x\\lVert_{\\infty} \\le \\epsilon$ and $z_{i}\\in{0,1}$ for $i=1,\\dots,D$.</p><p>Here</p>\\[A=[\\tilde{x}_1,\\dots,\\tilde{x}_D]\\in R^{N \\times D}\\]<p>and</p>\\[\\epsilon=0.5\\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\\]<p>where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.</p><p>If $z_{i}^{\\ast}=1$ (or $z_{i}^{\\ast}=0$), then the corresponding component $\\tilde{x}_i$ for $i=1,\\dots,D$ is selected (or excluded) for the representation of the underlying trend.</p><p>Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.</p><p>In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimizationproblem is considered first. The solution of the $L_{0}$ normcontinuous valued optimization problem is approximated bythe solution of the corresponding $L_{1}$ norm continuous valuedoptimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:</p>\\[y^{*}= [y_{1}^{*},\\dots,y_{D}^{*}]= \\mathrm{arg}\\min_{y}\\lVert y\\lVert_{1} \\text{  subject to } \\lVert Ay-x\\lVert_{\\infty}\\le \\epsilon\\]<p>By further applying the quantization on $y_{i}^{*}$ for $i=1,\\dots,D$ to either $0$ or $1$ via the following operator.</p>\\[W_{i}^{*}=\\begin{cases}1 &amp;  y_{i}^{*}\\ge 0.5\\\\0 &amp;  y_{i}^{*}&lt; 0.5\\end{cases}\\]<p>the corresponding component $\\tilde{x}_{i}$</p><p>for is selected (excluded) for the representation of the underlying trend if</p><p>$W_{i}^{\\ast}=1( \\text{ or } W_{i}^{\\ast}=0)$</p><p>Finally the underlying trend of the signal is obtained by</p>\\[\\Gamma = AW^*\\]<p>where $W^{\\ast}= [W_1^{\\ast},\\dots,W_D^{\\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original <a class=\"internal-link\" href=\"/2017/04/11/sparse-representation\">sparse binary programming problem</a>. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programmingproblem. That is, $W^{\\ast}$ in is very close to $z^{\\ast}$ above.</p><p>Therefore, the subset $I_{1}$ can be obtained by</p>\\[I_{1} = \\{i\\vert W_{i}^{*}= 1, 1\\le i \\le D\\}\\]<p>After obtaining the underlying trend $\\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.</p><p>Denote $\\Gamma =[\\gamma(1), \\dots, \\gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume thatthere is an underlying structure in the time series and thisstructure is preserved for the time period to be predicted. Aprediction model based on the linear recurrent formulae (LRF)is employed.</p><p>That is, the points $\\gamma(N-L+2), \\dots, \\gamma(N)$ are employed for the prediction of $\\gamma(N+1)$, and so on. In other words,</p>\\[\\gamma(n+1) = \\sum_{k=0}^{L-2}a_{k}\\gamma(n-k) \\text{   for  } n\\ge N,\\]<p>where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \\dots, a_{0})^T$ is given by</p>\\[R = \\frac{1}{1-\\nu^{2}}\\sum_{i\\in I_{1}}\\pi_{i}U_{i}^{\\nabla}\\]<p>Here, $\\nu^{2=}\\sum_{i\\in I_{1}}\\pi_{i}^2$, $\\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\\nabla }\\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \\in I_{1}.$</p>",
            "url": "http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming",
            
            
            
            "tags": ["trend_extraction","SSA","sparse_binary_programming"],
            
            "date_published": "2017-04-09T00:00:00+01:00",
            "date_modified": "2017-04-09T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/08/ica-with-cdwt",
            "title": "ICA with CDWT",
            "summary": "ICA with CDWT",
            "content_text": "It is well known that if the original sounds are mixed inthe real environment (in the time domain) then the observedsounds are a convolution mixture between the originalsounds with a delay and a reverberation. In order to simplifythis convolution mixture, it is a good idea to convert thesignal from the time domain into the time–frequencydomain and transform the convolution mixture into thelinear mixture by a time–frequency analysis method. Bydoing this the drawback of poor performance with unsteadysounds of the ICA also can be improved.The time-frequency analysis method is usually acombination of the ICA, the Short Time Fourier Transform(STFT) and the Discrete Wavelet Transform (DWT).The STFT is probably the most common approach fortime–frequency analysis. It subdivides the signal into shorttime segments (it is the same as using a small window todivide the signal), and a discrete Fourier transform iscomputed for each of these. For each frequency component,however, the window length is fixed. So it is impossible tochoose an optimal window for each frequency component,that is, the short time Fourier transform is unable to obtainoptimal analysis results for individual frequencycomponents.On the other hand, the DWT that wascarried out by Mattal’s fast algorithm also has a drawbackof lacking shift invariance although it can solve the problemof the window width and obtain optimal frequencyresolution for each frequency component. Fortunately,in order to improve the fault, a Complex Discrete WaveletTransform (CDWT) was proposed and it has been appliedwidely to signal and image analysisICA and CDWT for blind source seperationIn this method, the signals first were transformed from the time–domain into thetime–frequency domain by using the CDWT and then the ICA was carried out in the time–frequency domain. As in traditional methods, such as the STFT + ICA and the DWT + ICA, the following two problems when the ICA processing was carry out in the time–frequency domainoccurred. \t1. Scaling problem: the signal’s amplitude and phase obtained by the ICA was changed, \t2. Permutation problem: the separated signals are replaced at every frequency level mutually.This method discuss the technique for solving the scaling and the permutation problems. Finally, the separated signals are obtained by the inverse CDWT.Here, the case of two sound sources and two mikes has been considered for simplicity. First of all,the two sound signals $x_n(t) (n=1,2,$ where $n$ is the number of channels and $t$ is the time) were observed by mikes from two sound sources $s_{i}(t) ( i =1, 2)$. The relation between the observed signal $x_n(t)$ and the sound source $s_{i}(t)$ is as follows:\\[x_{n}(t) = \\mathbf{A_{n,i}}(t) \\ast  S_{i}(t) \\tag{1}\\]where $\\ast$ denotes convolution, the $\\mathbf{A_{n,i}}(t)$ , is the impulse response that is from the sound sources to the mikes and can be shown as the following equation:\\[\\mathbf{A_{n,i}}(t) = \\begin{bmatrix} a_{11}(t) &amp; a_{12}(t)\\\\ a_{21}(t) &amp; a_{22}(t)\\\\\\end{bmatrix}, n,i = 1,2\\]If one transforms the signal $x_{n}(t)$ from the time domain to the time–frequency domain by the CDWT then the (1) can be expressed as (2), in which the convolution of the sound source and the impulse response was changed to simple multiplication.\\[x_{n}(\\omega , T) = \\mathbf{A_{n,i}}(\\omega) S_{i}(\\omega,T) \\tag{2}\\]where, $\\omega$ is the frequency and $T$ the time in the time–frequency domain.Next, whitening of the observed signal was carried out as follows:\\[\\hat{x}_{n}(\\omega , T) = Q(\\omega) x_{n}(\\omega,T) \\tag{3}\\]where $\\hat{x}_{n}(\\omega ,T)$is a whitened signal matrix and $Q(\\omega)$ a whitened mixture, which can be obtained from the observed mixture signal $x_{n}(\\omega , T)$ in each frequency.Finally, the ICA was carried out by using the whitened signal matrix $\\hat{x}_{n}(\\omega , T)$, in which the separation matrix $W(\\omega)$ can be presumed. As a result, the separated signal $u_{i}(\\omega , T)$ shown in (4) can be obtained.\\[u_{i}(\\omega , T) = W(\\omega) \\hat{x}_{n}(\\omega,T) \\tag{4}\\]The convolution mixture signal $x_{n}(t)$  shown in (1) can be transformed into the linear mixture signal$x_{n}(\\omega, T)$ shown in (2) by using the CDWT which simplifies the preprocessing of the ICA. A complex wavelet like, RI-Daubechies 6 wavelet can be applied as the mother wavelet.Problem and correction rule of ICA processingThe ICA is a technique for presuming the sound source $s_{i}(\\omega,T)$  and the inverse matrix of $A_{ni}(\\omega)$ from statistics without any former information of the observed signal. In this case, the amplitude of the separated signal $u_{i}(\\omega,T)$ is a constant times the amplitude of the sound source $s_{i}(\\omega,T)$ and a correction is needed.In this method, a similar amplitude change at each frequency level also occurred in the preprocessing by CDWT introduced. This phenomenon is called a scaling irregularity. Furthermore,same as in the traditional method, it is also possible that the separated sound is replaced with noise at every frequencylevel mutually. This phenomenon is called the permutationproblem. Therefore, after these two problems are solved, theinverse complex discrete wavelet transform is performed,and it is necessary to restore the sound signal observed witheach mike.Scaling problem and correction ruleNot only the amplitude of the restored signal but the phase also differs depending on the frequency by making for signal whitening (making no correlation). To take an arbitrary complex value by processing, this is caused. In order to solve the scaling irregularity problem, a method hasbeen proposed by Murata that uses the independent element $u_{i}(\\omega,T)$ , which is obtained at each frequency and divides the spectrum. In this study, the method of correcting scaling is adopted from the divided spectrum and can be shown as follows:\\[v_{1}(\\omega,T)=\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{12}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix}u_{1}(\\omega,T) \\\\0\\end{bmatrix}\\]\\[v_{2}(\\omega,T)=\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix} 0 \\\\u_{2}(\\omega,T) \\end{bmatrix}\\]where $v_{11}(\\omega,T),v_{22}(\\omega,T)$ are divided spectrums. If the sum $v_{1}(\\omega,T)+v_{2}(\\omega,T)$ is calculated then the sum $v_{11}(\\omega,T)+v_{21}(\\omega,T)$ is the mixture signal $x_{1}(\\omega,T)$ and the sum $v_{11}(\\omega,T)+v_{22}(\\omega,T)$ is the mixture signal $x_{2}(\\omega,T)$Permutation problem and correction ruleThe complex Fast-ICA performs separation based onnon-Gaussian characteristics. Therefore, the possibility ofthe separating signal with higher non-Gaussiancharacteristics being output as the first channel is very high.However, the height of the frequency is not determined andnoise with low non-Gaussian characteristics might also beoutput. Therefore, there is a possibility that the output of each frequency level is separated without being united by the sound. The separated signal $u_{1}(\\omega,T), u_{2}(\\omega,T)$ of the 1st and 2nd channel without permutation is shown as follows.\\[u_{1}(\\omega ,k) \\approx s_{1}(\\omega,T) \\tag{5} ; u_{2}(\\omega ,k) \\approx s_{2}(\\omega,T)\\]The above expression can be shown as the following equation by using the divided spectrum of the scaling correction.\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]On the other hand, when permutation occurs, (5) becomes the next expression\\[u_{1}(\\omega ,k) \\approx s_{2}(\\omega,T) \\tag{6} ; u_{2}(\\omega ,k) \\approx s_{1}(\\omega,T)\\]\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]From these, we can know that the divided spectrum can be shown as a multiplication of the separated signal $u_{i}(\\omega,T)$ and the transfer function $g_{ni}(\\omega)$ , which is from the sound source to the mike.The separation matrix $W(\\omega)$ and the whitening matrix $Q(\\omega)$ presumed by the ICA processing are used and the next equation can be obtained.\\[D = (W(\\omega)Q(\\omega))^{-1} = \\begin{bmatrix}g_{11}(\\omega) &amp; g_{12}(\\omega) \\\\g_{21}(\\omega) &amp; g_{22}(\\omega)\\end{bmatrix} = e\\begin{bmatrix}a_{11}(\\omega) &amp; a_{12}(\\omega) \\\\a_{21}(\\omega) &amp; a_{22}(\\omega)\\end{bmatrix} = eA_{ni}(\\omega), e \\in \\mathbb{R}\\]ConclusionCDWT +Complex Value Fast ICA outperforms, STFT+ Complex value Fast ICA and DWT+Real Value Fast ICAReference : BLIND SOURCE SEPARATION BY COMBINING INDEPANDENT COMPONENT ANALYSIS WITH COMPLEX DISCRETE WAVELET TRANSFORM ZHONG ZHANG , TAKESHI ENOMOTO , TETSUO MIYAKE , TAKASHI IMAMURA",
            "content_html": "<p align=\"justify\">It is well known that if the original sounds are mixed inthe real environment (in the time domain) then the observedsounds are a convolution mixture between the originalsounds with a delay and a reverberation. In order to simplifythis convolution mixture, it is a good idea to convert thesignal from the time domain into the time–frequencydomain and transform the convolution mixture into thelinear mixture by a time–frequency analysis method. Bydoing this the drawback of poor performance with unsteadysounds of the ICA also can be improved.</p><p align=\"justify\">The time-frequency analysis method is usually acombination of the <a class=\"internal-link\" href=\"/2017/04/06/blind-source-seperation-ica\">ICA</a>, the Short Time Fourier Transform(STFT) and the Discrete Wavelet Transform (DWT).</p><p align=\"justify\">The STFT is probably the most common approach fortime–frequency analysis. It subdivides the signal into shorttime segments (it is the same as using a small window todivide the signal), and a discrete Fourier transform iscomputed for each of these. For each frequency component,however, the window length is fixed. So it is impossible tochoose an optimal window for each frequency component,that is, the short time Fourier transform is unable to obtainoptimal analysis results for individual frequencycomponents.</p><p align=\"justify\">On the other hand, the DWT that wascarried out by Mattal’s fast algorithm also has a drawbackof lacking shift invariance although it can solve the problemof the window width and obtain optimal frequencyresolution for each frequency component. Fortunately,in order to improve the fault, a Complex Discrete WaveletTransform (CDWT) was proposed and it has been appliedwidely to signal and image analysis</p><h2 id=\"ica-and-cdwt-for-blind-source-seperation\">ICA and CDWT for blind source seperation</h2><p>In this method, the signals first were transformed from the time–domain into thetime–frequency domain by using the CDWT and then the ICA was carried out in the time–frequency domain. As in traditional methods, such as the STFT + ICA and the DWT + ICA, the following two problems when the ICA processing was carry out in the time–frequency domainoccurred. \t1. Scaling problem: the signal’s amplitude and phase obtained by the ICA was changed, \t2. Permutation problem: the separated signals are replaced at every frequency level mutually.</p><p>This method discuss the technique for solving the scaling and the permutation problems. Finally, the separated signals are obtained by the inverse CDWT.</p><p><img src=\"/assets/snips/ica_cdwt.png\" alt=\"ICA_CDWT\" /></p><p>Here, the case of two sound sources and two mikes has been considered for simplicity. First of all,the two sound signals $x_n(t) (n=1,2,$ where $n$ is the number of channels and $t$ is the time) were observed by mikes from two sound sources $s_{i}(t) ( i =1, 2)$. The relation between the observed signal $x_n(t)$ and the sound source $s_{i}(t)$ is as follows:</p>\\[x_{n}(t) = \\mathbf{A_{n,i}}(t) \\ast  S_{i}(t) \\tag{1}\\]<p>where $\\ast$ denotes convolution, the $\\mathbf{A_{n,i}}(t)$ , is the impulse response that is from the sound sources to the mikes and can be shown as the following equation:</p>\\[\\mathbf{A_{n,i}}(t) = \\begin{bmatrix} a_{11}(t) &amp; a_{12}(t)\\\\ a_{21}(t) &amp; a_{22}(t)\\\\\\end{bmatrix}, n,i = 1,2\\]<p>If one transforms the signal $x_{n}(t)$ from the time domain to the time–frequency domain by the CDWT then the (1) can be expressed as (2), in which the convolution of the sound source and the impulse response was changed to simple multiplication.</p>\\[x_{n}(\\omega , T) = \\mathbf{A_{n,i}}(\\omega) S_{i}(\\omega,T) \\tag{2}\\]<p>where, $\\omega$ is the frequency and $T$ the time in the time–frequency domain.</p><p>Next, whitening of the observed signal was carried out as follows:</p>\\[\\hat{x}_{n}(\\omega , T) = Q(\\omega) x_{n}(\\omega,T) \\tag{3}\\]<p>where $\\hat{x}_{n}(\\omega ,T)$</p><p>is a whitened signal matrix and $Q(\\omega)$ a whitened mixture, which can be obtained from the observed mixture signal $x_{n}(\\omega , T)$ in each frequency.</p><p>Finally, the ICA was carried out by using the whitened signal matrix $\\hat{x}_{n}(\\omega , T)$</p><p>, in which the separation matrix $W(\\omega)$ can be presumed. As a result, the separated signal $u_{i}(\\omega , T)$ shown in (4) can be obtained.</p>\\[u_{i}(\\omega , T) = W(\\omega) \\hat{x}_{n}(\\omega,T) \\tag{4}\\]<p>The convolution mixture signal $x_{n}(t)$  shown in (1) can be transformed into the linear mixture signal$x_{n}(\\omega, T)$ shown in (2) by using the CDWT which simplifies the preprocessing of the ICA. A complex wavelet like, RI-Daubechies 6 wavelet can be applied as the mother wavelet.</p><h2 id=\"problem-and-correction-rule-of-ica-processing\">Problem and correction rule of ICA processing</h2><p>The ICA is a technique for presuming the sound source $s_{i}(\\omega,T)$  and the inverse matrix of $A_{ni}(\\omega)$ from statistics without any former information of the observed signal. In this case, the amplitude of the separated signal $u_{i}(\\omega,T)$ is a constant times the amplitude of the sound source $s_{i}(\\omega,T)$ and a correction is needed.</p><p align=\"justify\">In this method, a similar amplitude change at each frequency level also occurred in the preprocessing by CDWT introduced. This phenomenon is called a scaling irregularity. Furthermore,same as in the traditional method, it is also possible that the separated sound is replaced with noise at every frequencylevel mutually. This phenomenon is called the permutationproblem. Therefore, after these two problems are solved, theinverse complex discrete wavelet transform is performed,and it is necessary to restore the sound signal observed witheach mike.</p><h3 id=\"scaling-problem-and-correction-rule\">Scaling problem and correction rule</h3><p>Not only the amplitude of the restored signal but the phase also differs depending on the frequency by making for signal whitening (making no correlation). To take an arbitrary complex value by processing, this is caused. In order to solve the scaling irregularity problem, a method hasbeen proposed by Murata that uses the independent element $u_{i}(\\omega,T)$ , which is obtained at each frequency and divides the spectrum. In this study, the method of correcting scaling is adopted from the divided spectrum and can be shown as follows:</p>\\[v_{1}(\\omega,T)=\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{12}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix}u_{1}(\\omega,T) \\\\0\\end{bmatrix}\\]\\[v_{2}(\\omega,T)=\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = (W(\\omega)Q(\\omega))^{-1}\\begin{bmatrix} 0 \\\\u_{2}(\\omega,T) \\end{bmatrix}\\]<p>where $v_{11}(\\omega,T),v_{22}(\\omega,T)$ are divided spectrums. If the sum $v_{1}(\\omega,T)+v_{2}(\\omega,T)$ is calculated then the sum $v_{11}(\\omega,T)+v_{21}(\\omega,T)$ is the mixture signal $x_{1}(\\omega,T)$ and the sum $v_{11}(\\omega,T)+v_{22}(\\omega,T)$ is the mixture signal $x_{2}(\\omega,T)$</p><h3 id=\"permutation-problem-and-correction-rule\">Permutation problem and correction rule</h3><p align=\"justify\">The complex Fast-ICA performs separation based onnon-Gaussian characteristics. Therefore, the possibility ofthe separating signal with higher non-Gaussiancharacteristics being output as the first channel is very high.However, the height of the frequency is not determined andnoise with low non-Gaussian characteristics might also beoutput. </p><p>Therefore, there is a possibility that the output of each frequency level is separated without being united by the sound. The separated signal $u_{1}(\\omega,T), u_{2}(\\omega,T)$ of the 1st and 2nd channel without permutation is shown as follows.</p>\\[u_{1}(\\omega ,k) \\approx s_{1}(\\omega,T) \\tag{5} ; u_{2}(\\omega ,k) \\approx s_{2}(\\omega,T)\\]<p>The above expression can be shown as the following equation by using the divided spectrum of the scaling correction.</p>\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]<p>On the other hand, when permutation occurs, (5) becomes the next expression</p>\\[u_{1}(\\omega ,k) \\approx s_{2}(\\omega,T) \\tag{6} ; u_{2}(\\omega ,k) \\approx s_{1}(\\omega,T)\\]\\[\\begin{bmatrix}v_{11}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{12}(\\omega,T)u_{2}(\\omega,T) \\\\g_{22}(\\omega,T)u_{2}(\\omega,T)\\end{bmatrix}\\]\\[\\begin{bmatrix}v_{21}(\\omega,T)\\\\ v_{22}(\\omega,T)\\end{bmatrix} = \\begin{bmatrix}g_{11}(\\omega,T)u_{1}(\\omega,T) \\\\g_{21}(\\omega,T)u_{1}(\\omega,T)\\end{bmatrix}\\]<p>From these, we can know that the divided spectrum can be shown as a multiplication of the separated signal $u_{i}(\\omega,T)$ and the transfer function $g_{ni}(\\omega)$ , which is from the sound source to the mike.</p><p>The separation matrix $W(\\omega)$ and the whitening matrix $Q(\\omega)$ presumed by the ICA processing are used and the next equation can be obtained.</p>\\[D = (W(\\omega)Q(\\omega))^{-1} = \\begin{bmatrix}g_{11}(\\omega) &amp; g_{12}(\\omega) \\\\g_{21}(\\omega) &amp; g_{22}(\\omega)\\end{bmatrix} = e\\begin{bmatrix}a_{11}(\\omega) &amp; a_{12}(\\omega) \\\\a_{21}(\\omega) &amp; a_{22}(\\omega)\\end{bmatrix} = eA_{ni}(\\omega), e \\in \\mathbb{R}\\]<h2 id=\"conclusion\">Conclusion</h2><p>CDWT +Complex Value Fast ICA outperforms, STFT+ Complex value Fast ICA and DWT+Real Value Fast ICA</p><p>Reference : BLIND SOURCE SEPARATION BY COMBINING INDEPANDENT COMPONENT ANALYSIS WITH COMPLEX DISCRETE WAVELET TRANSFORM ZHONG ZHANG , TAKESHI ENOMOTO , TETSUO MIYAKE , TAKASHI IMAMURA</p>",
            "url": "http://localhost:4000/2017/04/08/ica-with-cdwt",
            
            
            
            "tags": ["BlindSourceSeperation","ICA","CDWT","STFT","DWT","wavelet","whitening","ICAissues"],
            
            "date_published": "2017-04-08T00:00:00+01:00",
            "date_modified": "2017-04-08T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/07/piecewise-linear-representation",
            "title": "Piecewise linear representation of time series",
            "summary": "Piecewise linear representation of time series",
            "content_text": "The time series data usually fluctuate frequently and exit a lot of noise. So data mining in the original sequence data directly will not only cost highly in the storage and computation, but also probably affect the accuracy and reliability of the data mining algorithms. Therefore, many time series models are proposed, which can transform original series to new series. Modeling may not only compress the data, but also keep the main form and ignore fine changes. Accordingly, it can help improve the efficiency and accuracy of the data mining algorithms, which will provide policy support for data analysts.  Piecewise Linear Representation Based on Important Point: Pratt and Fink proposed a piecewise linear representation based on the important points. The important points are defined as the points which are the extreme points within the local scope and the ratio of the important point and the endpoint exceeds the parameters $R$. After extracting the important points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series and get various piecewise linear representation with different fine and granularity by selecting different parameters  $R$.  Piecewise Aggregate Approximation (PAA): Keogh and Yi proposed the method of the piecewise aggregate approximation independently. The algorithm divides the time series by the same time width and each sub-segment is represent by the average of the sub-segment. The method is simple, intuitionistic. It not only can support the similarity queries, all the Minkowski metric and the weighted Euclidean distance, but also can be used to index to improve query efficiency.  Piecewise Linear Representation based on the characteristic points: Xiao  proposed a method of piecewise linear representation based on the characteristic points. After extracting the characteristic points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series.  Piecewise Linear Representation Based on Slope Extract Edge Point (SEEP):** ZHAN Yan-Yan brought forward a new piecewise linear representation combining slope with the characteristics of time series. The algorithm can select some change points according to the rate of slope change firstly, and then combines the points with the line sequentially. Finally it will generate a new time series.The literatures above are analyzed as follows:The piecewise linear representation gets some characteristics (e.g., extreme point, trend, etc.) of each section by segmenting the series mainly. The above methods not only have the advantages of simple and intuitive, but also can support dynamic incremental updates, clustering, fast similarity search, and so on. But the cost and fitting error is different.Piecewise Linear Representation of Time Series based on Slope Change Threshold (SCT):Firstly, the algorithm calculates the two segments’ slope of the certain point connecting with thetwo adjacent points(except the two endpoints of time series).Secondly, it determines the change points by the ratio of slope. And then it combines the points with the line orderly. In this way, a new time series arises.The key of the algorithm is determining the change points. The change points must follow thefollowing principles:  The first point and last point are both change point;  When the slope of the line combining the certain point with its left neighboring point is zero, welook on the point as change point if the slope of the line combining the certain point with its rightneighboring point is out the range of$(-d,+d);$  When the slope of the line combining the certain point with its left neighboring point is not zero, we look on the point as change point if the slope ratio of two lines is beyond the range of $(1-d,1+d).$ The two lines refer to the line which combines the certain point with its right neighboring point and the line which combines the certain point with its left neighboring point. Above $d$ is a threshold parameter.",
            "content_html": "<p align=\"justify\">The time series data usually fluctuate frequently and exit a lot of noise. So data mining in the original sequence data directly will not only cost highly in the storage and computation, but also probably affect the accuracy and reliability of the data mining algorithms. Therefore, many time series models are proposed, which can transform original series to new series. Modeling may not only compress the data, but also keep the main form and ignore fine changes. Accordingly, it can help improve the efficiency and accuracy of the data mining algorithms, which will provide policy support for data analysts.</p><ol>  <li><u><b>Piecewise Linear Representation Based on Important Point:</b></u> Pratt and Fink proposed a piecewise linear representation based on the important points. The important points are defined as the points which are the extreme points within the local scope and the ratio of the important point and the endpoint exceeds the parameters $R$. After extracting the important points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series and get various piecewise linear representation with different fine and granularity by selecting different parameters  $R$.</li>  <li><u><b>Piecewise Aggregate Approximation (PAA):</b></u> Keogh and Yi proposed the method of the piecewise aggregate approximation independently. The algorithm divides the time series by the same time width and each sub-segment is represent by the average of the sub-segment. The method is simple, intuitionistic. It not only can support the similarity queries, all the Minkowski metric and the weighted Euclidean distance, but also can be used to index to improve query efficiency.</li>  <li><u><b>Piecewise Linear Representation based on the characteristic points:</b></u> Xiao  proposed a method of piecewise linear representation based on the characteristic points. After extracting the characteristic points from the time series, the algorithm then combines the points with the line orderly. Thus it will generate a new time series.</li>  <li><u><b>Piecewise Linear Representation Based on Slope Extract Edge Point (SEEP)</b></u>:** ZHAN Yan-Yan brought forward a new piecewise linear representation combining slope with the characteristics of time series. The algorithm can select some change points according to the rate of slope change firstly, and then combines the points with the line sequentially. Finally it will generate a new time series.</li></ol><p>The literatures above are analyzed as follows:</p><p align=\"justify\">The piecewise linear representation gets some characteristics (e.g., extreme point, trend, etc.) of each section by segmenting the series mainly. The above methods not only have the advantages of simple and intuitive, but also can support dynamic incremental updates, clustering, fast similarity search, and so on. But the cost and fitting error is different.</p><p align=\"justify\"><u><b>Piecewise Linear Representation of Time Series based on Slope Change Threshold (SCT):</b></u>Firstly, the algorithm calculates the two segments’ slope of the certain point connecting with thetwo adjacent points(except the two endpoints of time series).Secondly, it determines the change points by the ratio of slope. And then it combines the points with the line orderly. In this way, a new time series arises.</p><p>The key of the algorithm is determining the change points. The change points must follow thefollowing principles:</p><ol>  <li>The first point and last point are both change point;</li>  <li>When the slope of the line combining the certain point with its left neighboring point is zero, welook on the point as change point if the slope of the line combining the certain point with its rightneighboring point is out the range of$(-d,+d);$</li>  <li>When the slope of the line combining the certain point with its left neighboring point is not zero, we look on the point as change point if the slope ratio of two lines is beyond the range of $(1-d,1+d).$ The two lines refer to the line which combines the certain point with its right neighboring point and the line which combines the certain point with its left neighboring point. Above $d$ is a threshold parameter.</li></ol>",
            "url": "http://localhost:4000/2017/04/07/piecewise-linear-representation",
            
            
            
            "tags": ["timeseries","representation","piecewiseLinearRepresentation","changepoint","slope"],
            
            "date_published": "2017-04-07T00:00:00+01:00",
            "date_modified": "2017-04-07T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/06/blind-source-seperation-ica",
            "title": "ICA",
            "summary": "Blind source seperation based on ICA",
            "content_text": "The main idea can be briefly expressed by the following mixed model:\\(x(t) = \\mathbf{A} s(t) + n(t)\\)The statistical model in the above equation is called ICA model, which describes how the observed data are mixed through the components $s(t)$ . The $m$ dimension column vector $x(t)$ is the observed data. $\\mathbf{A}$ is a $m \\times n$ mixing matrix; $n(t)$ denotes the additive noise vector. The matrix $\\mathbf{A}$ is assumed to be unknown. All we observe is the random vector x(t) , and we must estimate both Aand s(t) . Since $\\mathbf{A}$ is unknown, so $s(t)$ seems to be unsolvable.Fortunately, there are many mathematical methods for calculating the coefficients of $\\mathbf{A}$ by requiring the High-Order Statistics (HOS) information during the search for independentcomponents.",
            "content_html": "<p>The main idea can be briefly expressed by the following mixed model:\\(x(t) = \\mathbf{A} s(t) + n(t)\\)The statistical model in the above equation is called ICA model, which describes how the observed data are mixed through the components $s(t)$ . The $m$ dimension column vector $x(t)$ is the observed data. $\\mathbf{A}$ is a $m \\times n$ mixing matrix; $n(t)$ denotes the additive noise vector. The matrix $\\mathbf{A}$ is assumed to be unknown. All we observe is the random vector x(t) , and we must estimate both Aand s(t) . Since $\\mathbf{A}$ is unknown, so $s(t)$ seems to be unsolvable.</p><p>Fortunately, there are many mathematical methods for calculating the coefficients of $\\mathbf{A}$ by requiring the High-Order Statistics (HOS) information during the search for independentcomponents.</p>",
            "url": "http://localhost:4000/2017/04/06/blind-source-seperation-ica",
            
            
            
            "tags": ["BlindSourceSeperation","ICA"],
            
            "date_published": "2017-04-06T00:00:00+01:00",
            "date_modified": "2017-04-06T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition",
            "title": "Extreme Point Symmetric Mode Decomposition",
            "summary": "How to extract trend based on Extreme Point Symmetric Mode Decomposition",
            "content_text": "Considering the extraction methods of trend item, including the difference method, average slope method, moving average method, low pass filtering method, and least square fitting method, the type of trend term often needs to be presupposed. These methods are not suitable for processing the nonstationary signals with complex or random change trends.According to previous studies, the wavelet transform-based method is required for preselecting the wavelet basis and decomposition levels. This method is influenced easily byartificial factors and has no self-adaptability. The method based on Empirical mode decomposition (EMD) can adaptively decompose non-stationary signals regardless of the type of trend term. But, EMD is affected by mode mixing andend effect, causing the decomposed trend function is rough and the extraction accuracy is restricted. Professor Wang et al. recently proposed a self-adaptive method called ESMD. The ESMD is a novel development derived from Hilbert Huang transform that can be used to process non-stationary signal. ESMD has been applied to many studies.ESMD  Step 1: Identify all local extreme points, including maxima and minima points, of the data $Y$. Mark them as $E_{i} (1 ≤ i ≤ n);$  Step 2: Connect all adjacent Ei with line segments, and mark their midpoints as $F_{i} (1 ≤ i ≤ n-1);$  Step 3: Add left and right boundary midpoints $F_{0}$ and $F_{n}$ using linear interpolation method;  Step 4: Construct $p$ interpolating curves $L_{1}, L_{2},\\dots L_{p} (p≥1)$ with all $n+1$ midpoints and calculate their mean value by using equation $L^{\\ast}$.\\[L^{*}= \\frac{L_{1}+L_{2}+\\dots + L_{p}}{p}\\]  Step 5: Repeat steps $1$ to $4$ on $Y - L^{\\ast}$ until $||L^{\\ast}|| \\le \\epsilon$ ($\\epsilon$ is a permitted error), or until the sifting times attain a preset maximum number $K$. Then, the first mode $M_{1}$ is obtained.  Step 6: Repeat steps $1$ to $5$ on the residual $Y - M_1$ and obtain $M_2, M_3 \\dots$ until the last residual $R$ with no more than a certain number of extreme points.  Step 7: Change the maximum number $K$ on a finite integer interval $[K_{min}, K_{max}],$ and repeat all previous steps. Calculate the variance $\\sigma^2$ of $Y - R$ and plot a figure with $\\frac{\\sigma}{\\sigma_{0}}$ and $K$, $\\sigma_{0}$ is the standard deviation of $Y$.\\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-r_{i})^2\\]\\[\\sigma_{0}^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-\\bar{Y})^2\\]  Step 8 : Identify the number $K_0$ which corresponds to the minimum  $\\frac{\\sigma}{\\sigma_{0}}$  in $[K_{min}, K_{max}].$  Use this $K_{0}$ to repeat steps $1$ to $6$ and obtain the whole modes. Then last residual $R$ is an optimal Adaptive global mean (AGM) curve.Based on the steps above, ESMD can decompose signal into limited intrinsic mode functions and a residual component. The residual component $R$ is an optimal AGM curve that can be considered as the trend term of the original signal.The extraction error of EMD is larger than that of ESMD.   The EMD-based extraction method can adaptively extract the signal trend. Whereas, the extracted trend curve limits the number of its extreme points (no more than 1), and no optimal strategy to find it. Therefore, the extraction error of EMD is relatively larger than that of ESMD.  The ESMD-based extraction method has commendable self-adaptability. It can obtain the signal trend with high precision using adaptive decomposition and optimization. The trend type of signal does not need to be preset. And, the extraction results of ESMD are better than that of EMD.Reference: Adaptive extraction method for trend term of machinery signal based onextreme-point symmetric mode decomposition - Yong Zhu, Wan-lu Jiang and Xiang-dong Kong",
            "content_html": "<p align=\"justify\">Considering the extraction methods of trend item, including the difference method, average slope method, moving average method, low pass filtering method, and least square fitting method, the type of trend term often needs to be presupposed. These methods are not suitable for processing the nonstationary signals with complex or random change trends.</p><p align=\"justify\">According to previous studies, the wavelet transform-based method is required for preselecting the wavelet basis and decomposition levels. This method is influenced easily byartificial factors and has no self-adaptability. </p><p align=\"justify\">The method based on Empirical mode decomposition (EMD) can adaptively decompose non-stationary signals regardless of the type of trend term. But, EMD is affected by mode mixing andend effect, causing the decomposed trend function is rough and the extraction accuracy is restricted. </p><p align=\"justify\">Professor Wang et al. recently proposed a self-adaptive method called ESMD. The ESMD is a novel development derived from Hilbert Huang transform that can be used to process non-stationary signal. ESMD has been applied to many studies.</p><h2 id=\"esmd\">ESMD</h2><ul>  <li><strong>Step 1:</strong> Identify all local extreme points, including maxima and minima points, of the data $Y$. Mark them as $E_{i} (1 ≤ i ≤ n);$</li>  <li><strong>Step 2:</strong> Connect all adjacent Ei with line segments, and mark their midpoints as $F_{i} (1 ≤ i ≤ n-1);$</li>  <li><strong>Step 3:</strong> Add left and right boundary midpoints $F_{0}$ and $F_{n}$ using linear interpolation method;</li>  <li><strong>Step 4:</strong> Construct $p$ interpolating curves $L_{1}, L_{2},\\dots L_{p} (p≥1)$ with all $n+1$ midpoints and calculate their mean value by using equation $L^{\\ast}$.</li></ul>\\[L^{*}= \\frac{L_{1}+L_{2}+\\dots + L_{p}}{p}\\]<ul>  <li><strong>Step 5:</strong> Repeat steps $1$ to $4$ on $Y - L^{\\ast}$ until $||L^{\\ast}|| \\le \\epsilon$ ($\\epsilon$ is a permitted error), or until the sifting times attain a preset maximum number $K$. Then, the first mode $M_{1}$ is obtained.</li>  <li><strong>Step 6:</strong> Repeat steps $1$ to $5$ on the residual $Y - M_1$ and obtain $M_2, M_3 \\dots$ until the last residual $R$ with no more than a certain number of extreme points.</li>  <li><strong>Step 7:</strong> Change the maximum number $K$ on a finite integer interval $[K_{min}, K_{max}],$ and repeat all previous steps. Calculate the variance $\\sigma^2$ of $Y - R$ and plot a figure with $\\frac{\\sigma}{\\sigma_{0}}$ and $K$, $\\sigma_{0}$ is the standard deviation of $Y$.</li></ul>\\[\\sigma^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-r_{i})^2\\]\\[\\sigma_{0}^2 = \\frac{1}{N}\\sum_{i=1}^N(y_{i}-\\bar{Y})^2\\]<ul>  <li><strong>Step 8 :</strong> Identify the number $K_0$ which corresponds to the minimum  $\\frac{\\sigma}{\\sigma_{0}}$  in $[K_{min}, K_{max}].$  Use this $K_{0}$ to repeat steps $1$ to $6$ and obtain the whole modes. Then last residual $R$ is an optimal Adaptive global mean (AGM) curve.</li></ul><p>Based on the steps above, ESMD can decompose signal into limited intrinsic mode functions and a residual component. The residual component $R$ is an optimal AGM curve that can be considered as the trend term of the original signal.</p><p><u>The extraction error of EMD is larger than that of ESMD. </u></p><ol>  <li>The EMD-based extraction method can adaptively extract the signal trend. Whereas, the extracted trend curve limits the number of its extreme points (no more than 1), and no optimal strategy to find it. Therefore, the extraction error of EMD is relatively larger than that of ESMD.</li>  <li>The ESMD-based extraction method has commendable self-adaptability. It can obtain the signal trend with high precision using adaptive decomposition and optimization. The trend type of signal does not need to be preset. And, the extraction results of ESMD are better than that of EMD.</li></ol><p>Reference: Adaptive extraction method for trend term of machinery signal based onextreme-point symmetric mode decomposition - Yong Zhu, Wan-lu Jiang and Xiang-dong Kong</p>",
            "url": "http://localhost:4000/2017/04/05/extreme-point-symmetric-mode-decomposition",
            
            
            
            "tags": ["trend_extraction","EMD","ESMD"],
            
            "date_published": "2017-04-05T00:00:00+01:00",
            "date_modified": "2017-04-05T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/04/03/pca-ica-1d-timeseries",
            "title": "PCA ICA for 1D timeseries",
            "summary": "How to apply ICA, PCA for fault detection from single sensor?",
            "content_text": "IdeaPrincipal component analysis (PCA) isa method that transforms multiple data series into uncorrelated data series. Independent component analysis ICA is a method that separates multiple data series into independent data series. However, both require signals from at least two separate sensors. To overcome this requirement and utilize the fault detection capability of ICA and PCA, we propose to use wavelet transform to pre-process the data collected from a single sensor and then use the coefficients of the wavelet transforms at different scales as input to ICA and PCA. Independent components analysis (ICA) requires little prior knowledge about the components to be isolated; however, at least two sensors must be available for signal collection and the number of sensors must be at least equal to the number of sources to be separated and this method cannot be applied directly when there is only one sensor collecting signals.Principal component analysis (PCA) is a multivariate data analysis technique that transforms a set of correlated variables into a set of uncorrelated variables. Each member of the resulting set of uncorrelated variables is called a principal component. We are interested in determining its suitability for fault detection because one of the identified principal components may reveal the signature of a hidden fault. As with ICA, however, this method cannot be applied directly when only a single variable is observed.Wavelet transform may be considered as a series of band pass filters when applied to the datacollected from a single sensor. The results of the transform, which exist in different frequencyregions, say $N$ regions, may be considered as different mixtures of the sources that havegenerated the collected signals.These $N$ groups of data can then be used as input to ICA or PCAfor identification of the hidden sources.ICA and PCAICA is a technique for separating independent sources linearly mixed in signals. Suppose thatthere are $N$ independent sources of vibration, and $N$ sensors at different locations are used torecord vibration signals. The signals recorded by each sensor come from different sources withdifferent mixing ratios. Let $s_{1}(t),s_{2}(t),  \\dots ,s_{N}(t)$ be the signals produced by the $N$ independentsources and $x_{1}(t),x_{2}(t),  \\dots ,x_{N}(t)$ be the observations from the $N$ sensors. The sensors record these signals simultaneously. The task of ICA is to estimate the mixing ratios of the source signals in the collected signals and obtain the independent source signals.To identify the independent components successfully, we need a rule for evaluating theindependency of the identified components. According to the Central Limit Theorem, thedistribution of the sum of a large number of independent random variables tends to a Gaussiandistribution. Since the collected signals are weighted sums of the independent sources, the sourcesto be isolated must have less Gaussianity than the collected signals. Thus, non-Gaussianity can beused for separating independent components. Hyvarinen and Oja proposed to use negentropyto evaluate the non-Gaussianity of the separated components so as to evaluate separationperformance. With this concept, we can seek the separation that provides the least Gaussianness of the separated components. The popular FastICA algorithm proposed by Havarinen and Ojaisoften used to carry out the ICA procedure.PCA is a technique that obtains linear transformations of a group of correlated variables suchthat the transformed variables are uncorrelated. For example, consider two variables, $x_{1}$ and$x_{2}$. For each variable, we have obtained the following $N$ observations:\\(x_{11}, x_{12}, \\dots , x_{1N}; x_{21}, x_{22}, \\dots; x_{2N}\\)where $x_{1i}$ and $x_{2j}$ denote the $i^{th}$ and the $j^{th}$ observations of variables $x_{1}$ and $x_{2}$, respectively. The PCA method seeks two new axes, D1 and D2, that make the projectionsof the collected data ontoD1 have the largest variability and at the same time, the projections of the collected data onto D2have the smallest variability. This way, we have expressed the collected data as their two principalcomponents. Most of the variation in the original data is explained by the first principalcomponent, D1, and the remaining variation in the original data isexplained by the secondprincipal component, D2.ICA renders the separated components independent of one another while PCA renderstheseparated components uncorrelated with one another. PCA separates the components based onlyon the second-order cumulant while ICA separates the components on high-order cumulants.Therefore, ICA can be considered a generalization of PCA.Method of preprocessing to apply ICA or PCA on 1D Time seriesThe available data is a single time series. To apply ICA or PCA for feature extraction, we needto have more than one time series. A method to generate multiple time series from the single available time series is given below.Wavelet transform decomposes a signal series in the time domain into a two-dimensionalfunction in the time-scale (frequency) plane. The wavelet coefficients measure the time-scale (frequency) content in a signal indexed by the scale parameter and the translation parameter. Let$\\varphi(t)$ be the mother wavelet. The wavelet family consists of a series of daughter wavelets that aregenerated by dilation and translation from the mother wavelet $\\varphi(t)$\\[\\varphi_{a, b}(t)=\\sqrt{|a|} \\varphi[(t-b) / a]\\]where $a$ is the scale parameter, $b$ isthe location parameter, and $\\sqrt{  |a|}$isused to guarantee energy preservation. The wavelet transform of signal $x(t)$ isdefined as the inner product of $\\varphi_{a, b}(t)$ and $x(t)$ in the Hilbert space of $L^2$ norm defined as:\\[W(a, b)=\\left\\langle\\varphi_{a, b}(t), x(t)\\right\\rangle=\\int x(t) \\varphi_{a, b}^*(t) \\mathrm{d} t\\]where the symbol * stands for the complex conjugate.Wavelet transform can be thought of as a series of band pass filters. The results of thetransform, which exist in different frequency regions, may be thought of as different mixtures of the independent sources. These different mixtures may be considered to be signals collected atdifferent ‘‘locations’’, or more accurately, through different ‘‘sensors’’ with different frequencyranges. This way, the one-dimensional signal is transformed into multidimensional datathat satisfy the requirements of ICA and PCA. The preprocessing of the one-dimensional data with wavelet transform makes ICA and PCA usable for identification of a hiddensource.Reference :Feature separation using ICA for a one-dimensional time series and its application in fault detection - Ming J. Zuo, Jing Lin, Xianfeng Fan",
            "content_html": "<h3 id=\"idea\">Idea</h3><p align=\"justify\">Principal component analysis (PCA) isa method that transforms multiple data series into uncorrelated data series. Independent component analysis <a class=\"internal-link\" href=\"/2017/04/06/blind-source-seperation-ica\">ICA</a> is a method that separates multiple data series into independent data series. However, both require signals from at least two separate sensors. To overcome this requirement and utilize the fault detection capability of ICA and PCA, we propose to use wavelet transform to pre-process the data collected from a single sensor and then use the coefficients of the wavelet transforms at different scales as input to ICA and PCA. </p><p align=\"justify\">Independent components analysis (ICA) requires little prior knowledge about the components to be isolated; however, at least two sensors must be available for signal collection and the number of sensors must be at least equal to the number of sources to be separated and this method cannot be applied directly when there is only one sensor collecting signals.</p><p align=\"justify\">Principal component analysis (PCA) is a multivariate data analysis technique that transforms a set of correlated variables into a set of uncorrelated variables. Each member of the resulting set of uncorrelated variables is called a principal component. We are interested in determining its suitability for fault detection because one of the identified principal components may reveal the signature of a hidden fault. As with ICA, however, this method cannot be applied directly when only a single variable is observed.</p><p>Wavelet transform may be considered as a series of band pass filters when applied to the datacollected from a single sensor. The results of the transform, which exist in different frequencyregions, say $N$ regions, may be considered as different mixtures of the sources that havegenerated the collected signals.These $N$ groups of data can then be used as input to ICA or PCAfor identification of the hidden sources.</p><h3 id=\"ica-and-pca\">ICA and PCA</h3><p>ICA is a technique for separating independent sources linearly mixed in signals. Suppose thatthere are $N$ independent sources of vibration, and $N$ sensors at different locations are used torecord vibration signals. The signals recorded by each sensor come from different sources withdifferent mixing ratios. Let $s_{1}(t),s_{2}(t),  \\dots ,s_{N}(t)$ be the signals produced by the $N$ independentsources and $x_{1}(t),x_{2}(t),  \\dots ,x_{N}(t)$ be the observations from the $N$ sensors. The sensors record these signals simultaneously. The task of ICA is to estimate the mixing ratios of the source signals in the collected signals and obtain the independent source signals.</p><p align=\"justify\">To identify the independent components successfully, we need a rule for evaluating theindependency of the identified components. According to the Central Limit Theorem, thedistribution of the sum of a large number of independent random variables tends to a Gaussiandistribution. Since the collected signals are weighted sums of the independent sources, the sourcesto be isolated must have less Gaussianity than the collected signals. Thus, non-Gaussianity can beused for separating independent components. Hyvarinen and Oja proposed to use negentropyto evaluate the non-Gaussianity of the separated components so as to evaluate separationperformance. With this concept, we can seek the separation that provides the least Gaussianness of the separated components. The popular FastICA algorithm proposed by Havarinen and Ojaisoften used to carry out the ICA procedure.</p><p>PCA is a technique that obtains linear transformations of a group of correlated variables suchthat the transformed variables are uncorrelated. For example, consider two variables, $x_{1}$ and$x_{2}$. For each variable, we have obtained the following $N$ observations:\\(x_{11}, x_{12}, \\dots , x_{1N}; x_{21}, x_{22}, \\dots; x_{2N}\\)where $x_{1i}$ and $x_{2j}$ denote the $i^{th}$ and the $j^{th}$ observations of variables $x_{1}$ and $x_{2}$, respectively. The PCA method seeks two new axes, D1 and D2, that make the projectionsof the collected data ontoD1 have the largest variability and at the same time, the projections of the collected data onto D2have the smallest variability. This way, we have expressed the collected data as their two principalcomponents. Most of the variation in the original data is explained by the first principalcomponent, D1, and the remaining variation in the original data isexplained by the secondprincipal component, D2.</p><p align=\"justify\">ICA renders the separated components independent of one another while PCA renderstheseparated components uncorrelated with one another. PCA separates the components based onlyon the second-order cumulant while ICA separates the components on high-order cumulants.Therefore, ICA can be considered a generalization of PCA.</p><h3 id=\"method-of-preprocessing-to-apply-ica-or-pca-on-1d-time-series\">Method of preprocessing to apply ICA or PCA on 1D Time series</h3><p>The available data is a single time series. To apply ICA or PCA for feature extraction, we needto have more than one time series. A method to generate multiple time series from the single available time series is given below.</p><p>Wavelet transform decomposes a signal series in the time domain into a two-dimensionalfunction in the time-scale (frequency) plane. The wavelet coefficients measure the time-scale (frequency) content in a signal indexed by the scale parameter and the translation parameter. Let$\\varphi(t)$ be the mother wavelet. The wavelet family consists of a series of daughter wavelets that aregenerated by dilation and translation from the mother wavelet $\\varphi(t)$</p>\\[\\varphi_{a, b}(t)=\\sqrt{|a|} \\varphi[(t-b) / a]\\]<p>where $a$ is the scale parameter, $b$ isthe location parameter, and $\\sqrt{  |a|}$isused to guarantee energy preservation. The wavelet transform of signal $x(t)$ isdefined as the inner product of $\\varphi_{a, b}(t)$ and $x(t)$ in the Hilbert space of $L^2$ norm defined as:</p>\\[W(a, b)=\\left\\langle\\varphi_{a, b}(t), x(t)\\right\\rangle=\\int x(t) \\varphi_{a, b}^*(t) \\mathrm{d} t\\]<p>where the symbol * stands for the complex conjugate.</p><p>Wavelet transform can be thought of as a series of band pass filters. The results of thetransform, which exist in different frequency regions, may be thought of as different mixtures of the independent sources. These different mixtures may be considered to be signals collected atdifferent ‘‘locations’’, or more accurately, through different ‘‘sensors’’ with different frequencyranges. This way, the one-dimensional signal is transformed into multidimensional datathat satisfy the requirements of ICA and PCA. The preprocessing of the one-dimensional data with wavelet transform makes ICA and PCA usable for identification of a hiddensource.</p><p><img src=\"/assets/snips/img1.png\" alt=\"Flowchart\" /></p><h3 id=\"reference-\">Reference :</h3><p>Feature separation using ICA for a one-dimensional time series and its application in fault detection - Ming J. Zuo, Jing Lin, Xianfeng Fan</p>",
            "url": "http://localhost:4000/2017/04/03/pca-ica-1d-timeseries",
            
            
            
            "tags": ["ICA","faultdetection","wavelet","PCA","sensors","preprocessing"],
            
            "date_published": "2017-04-03T00:00:00+01:00",
            "date_modified": "2017-04-03T00:00:00+01:00",
            
                "author": "Pavan Donthireddy"
            
        },
    
        {
            "id": "http://localhost:4000/2017/03/21/latex-cheat-sheet",
            "title": "Latex cheatsheet",
            "summary": null,
            "content_text": "DescriptionCheatsheet for LaTex, using Markdown for markup. I use this with atom.ioand :package:markdown-preview-plus to write math stuff. :package:keyboard-localizationis necessary when using an international layout (like [swiss] german).$\\mathrm{abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ23456}$$\\text{abcdeABCDEF ASDFASDF} \\alpha, \\beta, \\gamma$\\[E = mc^2\\]$\\text{This is a text for math} \\Bigg(\\frac{a}{b} \\Bigg)$Further Reference and source: ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdfExample expressions / functions            Input      Rendered                  $a = b + c − d$      $a = b + c − d$              $\\sqrt{?\\frac{\\pi}{2}}$      $\\sqrt{\\frac{\\pi}{2}}$              $y = a x_1^2 + b x_2 + c$      $y = a x_1^2 + b x_2 + c$      Special characters / Symbols###Latin:#####No dot:\\imath $\\rightarrow$ $\\imath$,\\jmath $\\rightarrow$ $\\jmath$#####Hat:\\hat{\\imath}  $\\rightarrow$ $\\hat{\\imath}$,\\hat{\\jmath}  $\\rightarrow$ $\\hat{\\jmath}$###Greek Letters:#####Capital:LaTex      |   | LaTex    |   |———-:|–:|———:|–:|\\Gamma   | Γ | \\Delta | ∆ |\\Lambda  | Λ | \\Phi   | Φ |\\Pi      | Π | \\Psi   | Ψ |\\Sigma   | Σ | \\Theta | Θ |\\Upsilon | Υ | \\Xi    | Ξ |\\Omega   | Ω |          |   |#####Lowercase:LaTex      |   | LaTex     |   |———-:|–:|———-:|–:|\\alpha   | α | \\nu     | ν |\\beta    | β | \\kappa  | κ |\\gamma   | γ | \\lambda | λ |\\delta   | δ |  \\mu    | µ |  \\epsilon | ϵ | \\zeta   | ζ |\\eta     | η | \\theta  | θ |\\iota    | ι | \\xi     | ξ |\\pi      | π | \\rho    | ρ |\\sigma   | σ | \\tau    | τ |\\upsilon | υ | \\phi    | φ |\\chi     | χ | \\psi    | ψ |\\omega   | ω |           |   |#####Other:LaTex       |   | LaTex       |   |———–:|—|————:|–:|\\digamma  | ϝ | varepsilon| ε       |\\varkappa | ϰ | \\varphi   | ϕ       |\\varpi    | ϖ | \\varrho   | ϱ       |\\varsigma | ς | \\vartheta | ϑ       |\\eth      | ð | \\hbar     | $\\hbar$ |###Other:####Other SymbolsLaTex         |   | LaTex            |   |————-:|—|—————–:|–:|\\partial    | ∂ | \\infty         | ∞ |\\wedge      | ∧ | \\vee           | ∨ |\\neg \\not | ¬ |                  |   |\\bot        | ⊥ | \\top           | ⊤ |\\nabla      | ∇ | \\varnothing    | ∅ |\\angle      | ∠ | \\measuredangle | ∡ |\\surd       | √ | \\forall        | ∀ |\\exists     | ∃ | \\nexists       | ∄ |####Relational SymbolsLaTex             |   | LaTex              |          |—————–:|—|——————-:|———:|\\hookrightarrow | ↪      | \\Rightarrow     | ⇒         |\\rightarrow     | →      | \\Leftrightarrow | ⇔         |\\nrightarrow    | ↛      | \\mapsto         | $\\mapsto$ |\\geq            | ≥      | \\leq            | ≤         |\\equiv          | ≡      | \\sim            | ∼         |\\gg             | ≫      | \\ll            | ≪          |\\subset          | ⊂     | \\subseteq     | ⊆           |\\in             | ∈      | \\notin         | ∉          |\\mid            | $\\mid$ | \\propto        | ∝          |\\perp            | ⊥     | ` \\parallel     | ∥          |\\vartriangle`     | $\\vartriangle$####Binary operatorsLaTex        |   | LaTex  |   |————:|—|——-:|–:|\\wedge     | ∧ | \\vee | ∨ |\\neg\\not | ¬ |        |   |####Cumulative operatorsLaTex     |           | LaTex       |             |———:|———–|————:|————:|\\int    | ∫         | \\iint     | $\\iint$     |\\iiint  | $\\iiint$  | \\idotsint | $\\idotsint$ |\\prod   | $\\prod$   | \\sum      | $\\sum$      |\\bigcup | $\\bigcup$ | \\bigcap   | $\\bigcap$   |####Named operators$\\arccos$,$\\arcsin$,$\\arctan$,$\\arg$,$\\cos$,$\\cosh$,$\\cot$,$\\coth$,$\\deg$,$\\det$,$\\dim$,$\\exp$,$\\gcd$,$\\hom$,$\\inf$,$\\injlim$,$\\lg$,$\\lim$,$\\liminf$,$\\limsup$,$\\ln$,$\\log$,$\\max$,$\\min$,$\\Pr$,$\\projlim$,$\\sec$,$\\sin$,$\\sinh$,$\\sup$",
            "content_html": "<h1 id=\"description\">Description</h1><p>Cheatsheet for LaTex, using Markdown for markup. I use this with <a href=\"https://atom.io/\">atom.io</a>and :package:<code>markdown-preview-plus</code> to write math stuff. :package:<code>keyboard-localization</code>is necessary when using an international layout (like [swiss] german).</p><p>$\\mathrm{abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVWXYZ23456}$</p><p>$\\text{abcdeABCDEF ASDFASDF} \\alpha, \\beta, \\gamma$</p>\\[E = mc^2\\]<p>$\\text{This is a text for math} \\Bigg(\\frac{a}{b} \\Bigg)$</p><p>Further Reference and source: ftp://ftp.ams.org/pub/tex/doc/amsmath/short-math-guide.pdf</p><h1 id=\"example-expressions--functions\">Example expressions / functions</h1><table>  <thead>    <tr>      <th style=\"text-align: right\">Input</th>      <th style=\"text-align: right\">Rendered</th>    </tr>  </thead>  <tbody>    <tr>      <td style=\"text-align: right\"><code>$a = b + c − d$</code></td>      <td style=\"text-align: right\">$a = b + c − d$</td>    </tr>    <tr>      <td style=\"text-align: right\"><code>$\\sqrt{?\\frac{\\pi}{2}}$</code></td>      <td style=\"text-align: right\">$\\sqrt{\\frac{\\pi}{2}}$</td>    </tr>    <tr>      <td style=\"text-align: right\"><code>$y = a x_1^2 + b x_2 + c$</code></td>      <td style=\"text-align: right\">$y = a x_1^2 + b x_2 + c$</td>    </tr>  </tbody></table><h1 id=\"special-characters--symbols\">Special characters / Symbols</h1><p>###Latin:#####No dot:<br /><code>\\imath</code> $\\rightarrow$ $\\imath$,<code>\\jmath</code> $\\rightarrow$ $\\jmath$</p><p>#####Hat:<br /><code>\\hat{\\imath}</code>  $\\rightarrow$ $\\hat{\\imath}$,<code>\\hat{\\jmath}</code>  $\\rightarrow$ $\\hat{\\jmath}$</p><p>###Greek Letters:#####Capital:LaTex      |   | LaTex    |   |———-:|–:|———:|–:|<code>\\Gamma</code>   | Γ | <code>\\Delta</code> | ∆ |<code>\\Lambda</code>  | Λ | <code>\\Phi</code>   | Φ |<code>\\Pi</code>      | Π | <code>\\Psi</code>   | Ψ |<code>\\Sigma</code>   | Σ | <code>\\Theta</code> | Θ |<code>\\Upsilon</code> | Υ | <code>\\Xi</code>    | Ξ |<code>\\Omega</code>   | Ω |          |   |</p><p>#####Lowercase:LaTex      |   | LaTex     |   |———-:|–:|———-:|–:|<code>\\alpha</code>   | α | <code>\\nu</code>     | ν |<code>\\beta</code>    | β | <code>\\kappa</code>  | κ |<code>\\gamma</code>   | γ | <code>\\lambda</code> | λ |<code>\\delta</code>   | δ |  <code>\\mu</code>    | µ |  <br /><code>\\epsilon</code> | ϵ | <code>\\zeta</code>   | ζ |<code>\\eta</code>     | η | <code>\\theta</code>  | θ |<code>\\iota</code>    | ι | <code>\\xi</code>     | ξ |<code>\\pi</code>      | π | <code>\\rho</code>    | ρ |<code>\\sigma</code>   | σ | <code>\\tau</code>    | τ |<code>\\upsilon</code> | υ | <code>\\phi</code>    | φ |<code>\\chi</code>     | χ | <code>\\psi</code>    | ψ |<code>\\omega</code>   | ω |           |   |</p><p>#####Other:LaTex       |   | LaTex       |   |———–:|—|————:|–:|<code>\\digamma</code>  | ϝ | <code>varepsilon</code>| ε       |<code>\\varkappa</code> | ϰ | <code>\\varphi</code>   | ϕ       |<code>\\varpi</code>    | ϖ | <code>\\varrho</code>   | ϱ       |<code>\\varsigma</code> | ς | <code>\\vartheta</code> | ϑ       |<code>\\eth</code>      | ð | <code>\\hbar</code>     | $\\hbar$ |</p><p>###Other:####Other SymbolsLaTex         |   | LaTex            |   |————-:|—|—————–:|–:|<code>\\partial</code>    | ∂ | <code>\\infty</code>         | ∞ |<code>\\wedge</code>      | ∧ | <code>\\vee</code>           | ∨ |<code>\\neg</code> <code>\\not</code> | ¬ |                  |   |<code>\\bot</code>        | ⊥ | <code>\\top</code>           | ⊤ |<code>\\nabla</code>      | ∇ | <code>\\varnothing</code>    | ∅ |<code>\\angle</code>      | ∠ | <code>\\measuredangle</code> | ∡ |<code>\\surd</code>       | √ | <code>\\forall</code>        | ∀ |<code>\\exists</code>     | ∃ | <code>\\nexists</code>       | ∄ |</p><p>####Relational SymbolsLaTex             |   | LaTex              |          |—————–:|—|——————-:|———:|<code>\\hookrightarrow</code> | ↪      | <code>\\Rightarrow</code>     | ⇒         |<code>\\rightarrow</code>     | →      | <code>\\Leftrightarrow</code> | ⇔         |<code>\\nrightarrow</code>    | ↛      | <code>\\mapsto</code>         | $\\mapsto$ |<code>\\geq</code>            | ≥      | <code>\\leq</code>            | ≤         |<code>\\equiv</code>          | ≡      | <code>\\sim</code>            | ∼         |<code>\\gg</code>             | ≫      | <code>\\ll</code>            | ≪          |<code>\\subset</code>          | ⊂     | <code>\\subseteq</code>     | ⊆           |<code>\\in</code>             | ∈      | <code>\\notin</code>         | ∉          |<code>\\mid</code>            | $\\mid$ | <code>\\propto</code>        | ∝          |<code>\\perp</code>            | ⊥     | ` \\parallel<code>     | ∥          |</code>\\vartriangle`     | $\\vartriangle$</p><p>####Binary operatorsLaTex        |   | LaTex  |   |————:|—|——-:|–:|<code>\\wedge</code>     | ∧ | <code>\\vee</code> | ∨ |<code>\\neg</code><code>\\not</code> | ¬ |        |   |</p><p>####Cumulative operatorsLaTex     |           | LaTex       |             |———:|———–|————:|————:|<code>\\int</code>    | ∫         | <code>\\iint</code>     | $\\iint$     |<code>\\iiint</code>  | $\\iiint$  | <code>\\idotsint</code> | $\\idotsint$ |<code>\\prod</code>   | $\\prod$   | <code>\\sum</code>      | $\\sum$      |<code>\\bigcup</code> | $\\bigcup$ | <code>\\bigcap</code>   | $\\bigcap$   |</p><p>####Named operators$\\arccos$,$\\arcsin$,$\\arctan$,$\\arg$,$\\cos$,$\\cosh$,$\\cot$,$\\coth$,$\\deg$,$\\det$,$\\dim$,$\\exp$,$\\gcd$,$\\hom$,$\\inf$,$\\injlim$,$\\lg$,$\\lim$,$\\liminf$,$\\limsup$,$\\ln$,$\\log$,$\\max$,$\\min$,$\\Pr$,$\\projlim$,$\\sec$,$\\sin$,$\\sinh$,$\\sup$</p>",
            "url": "http://localhost:4000/2017/03/21/latex-cheat-sheet",
            
            
            
            "tags": ["latex","css"],
            
            "date_published": "2017-03-21T00:00:00+00:00",
            "date_modified": "2017-03-21T00:00:00+00:00",
            
                "author": "Bart SimpsonNelson Mandela Muntz"
            
        }
    
    ]
}