<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Trend extraction with SSA and Sparse Binary Programming" /><meta property="og:locale" content="en_US" /><meta name="description" content="Trend extraction with SSA and Sparse Binary Programming" /><meta property="og:description" content="Trend extraction with SSA and Sparse Binary Programming" /><link rel="canonical" href="http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming" /><meta property="og:url" content="http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2017-04-09T00:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Trend extraction with SSA and Sparse Binary Programming" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2017-04-09T00:00:00+01:00","datePublished":"2017-04-09T00:00:00+01:00","description":"Trend extraction with SSA and Sparse Binary Programming","headline":"Trend extraction with SSA and Sparse Binary Programming","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming"},"url":"http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming"}</script><title> Trend extraction with SSA and Sparse Binary Programming - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="The underlying trend is approximated by the sum of a part of [[Single Spectrum Analysis|SSA]] components, in which the total number of the SSA components in ..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="The underlying trend is approximated by the sum of a part of [[Single Spectrum Analysis|SSA]] components, in which the total number of the SSA components in ..."/><meta property="og:title" content="Trend extraction with SSA and Sparse Binary Programming"><meta property="og:type" content="article"><meta property="article:published_time" content="2017-04-09T00:00:00+01:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2017/04/09/trend-extraction-ssa-sparse-binary-programming" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Trend extraction with SSA and Sparse Binary Programming</h1><div style="text-align: center;"><span class="meta"><time datetime="2017-04-09T00:00:00+01:00">April 9, 2017</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/trend_extraction">trend_extraction</a>, <a href="/tag/SSA">SSA</a>, <a href="/tag/sparse_binary_programming">sparse_binary_programming</a></span></div><p align="justify">The underlying trend is approximated by the sum of a part of <a class="internal-link" href="/2017/04/10/ssa">SSA</a> components, in which the total number of the SSA components in the sum is minimized subject to a specification on the maximum absolute difference between the original signal and the approximated underlying trend.</p><p>As the selection of the SSA components is binary, this selection problem is to minimize the $L_{0}$ norm of the selection vector subject to the $L_{\infty}$ norm constraint on the difference between the original signal and the approximated underlying trend as well as the binary valued constraint on the elements of the selection vector.</p><p align="justify">This problem is actually a sparse binary programming problem. To solve this problem, first the corresponding continuous valued sparse optimization problem is solved. That is, to solve the same problem without the consideration of the binary valued constraint. This problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be obtained via existing simplex methods or interior point methods.</p><p align="justify">By applying the binary quantization to the obtained solution of the linear programming problem, the approximated solution of the original sparse binary programming problem is obtained. Unlike previously reported techniques that require a pre-cursor model or parameter specifications, the proposed method is completely adaptive.</p><h3 id="details">Details</h3><p align="justify">The conventional approach for selecting SSA components for extracting the underlying trend is to employ only the first several SSA components. However, this selection rule fails when the underlying trend of a signal has a complicated structure such as a high order polynomial structure which cannot be characterized by only the first several SSA components.</p><p align="justify">The idea is to formulate the selection problem as a sparse binary programming problem and proposes an efficient methodology for approximating the solution of the problem. In particular, the selection problem is formulated as follows. The number of the components to be selected is minimized subject to a specification on the maximum absolute difference between the approximated underlying trend and the original signal as well as the binary valued constraint on the selection coefficients.</p><p align="justify">Since the sparse binary programming problem is nonsmooth, nonconvex and NP hard, it requires an exhaustive search for finding the solution. As a result, the computational effort for finding the solution is very large and an efficient algorithm for approximating the solution is very useful and important.</p><p>To address these issues, the corresponding continuous valued optimization problem (the same optimization problem without the consideration of the binary valued constraint) is considered. Although this continuous valued optimization problem is with an $L_{0}$ objective function subject to an $L_{\infty}$ norm constraint, this problem can be approximated by a linear programming problem when the isometry condition is satisfied, and the solution of the linear programming problem can be efficiently obtained via existing simplex methods or interior point methods.</p><p>By applying the binary quantization to the obtained solution of this linear programming problem, the approximated solution of the original sparse binary programming problem is obtained.</p><h2 id="methodology">Methodology</h2><p>SSA is a nonparametric approach which does not need a priori specification on the model of the time series. It is very useful for extracting the underlying trend of a signal by selecting a subgroup of all $D$ SSA components and representing the underlying trend as the sum of the selected components.</p><p>Here, it is required to determine how to partition the index set into $2$ disjoint subsets $I_{1}$ and $I_{2}$ , in which they represent the underlying trend and the residual of the signal, respectively.</p><p>However, how to adaptively select the SSA components corresponding to the underlying trend is still an unsolved problem.</p><p>In order not to select the irregularities in the original signal, only the most important SSA components corresponding to the underlying trend of the signal are selected.</p><p>The selection problem is formulated as the following sparse sparse binary programming problem.</p>\[z^{*}= \begin{bmatrix} z_{1}^{*}&amp; \dots, z_{D}^* \end{bmatrix}^{T}= \mathrm{argmin}_{z} \lVert z\lVert_{0}\]<p>subject to $\lVert Az-x\lVert_{\infty} \le \epsilon$ and $z_{i}\in{0,1}$ for $i=1,\dots,D$.</p><p>Here</p>\[A=[\tilde{x}_1,\dots,\tilde{x}_D]\in R^{N \times D}\]<p>and</p>\[\epsilon=0.5\mathrm{max}_{n}(e_{up}(n)-e_{low}(n))\]<p>where $e_{up}(n)$ and $e_{low}(n)$ are the upper and lower envelopes of $x(n)$, respectievely.</p><p>If $z_{i}^{\ast}=1$ (or $z_{i}^{\ast}=0$), then the corresponding component $\tilde{x}_i$ for $i=1,\dots,D$ is selected (or excluded) for the representation of the underlying trend.</p><p>Since the total number of the selected components is minimized, the obtained solution is sparse and only the important SSA components corresponding to the underlying trend are selected. On the other hand, $L_{\infty}$ norm specification forces the underlying trend to follow the global change of the original signal.</p><p>In order to solve this sparse binary optimization problem, the corresponding $L_{0}$ norm continuous valued optimization problem is considered first. The solution of the $L_{0}$ norm continuous valued optimization problem is approximated by the solution of the corresponding $L_{1}$ norm continuous valued optimization problem when the isometry condition is satisfied. That is, to solve the following optimization problem:</p>\[y^{*}= [y_{1}^{*},\dots,y_{D}^{*}]= \mathrm{arg}\min_{y}\lVert y\lVert_{1} \text{ subject to } \lVert Ay-x\lVert_{\infty}\le \epsilon\]<p>By further applying the quantization on $y_{i}^{*}$ for $i=1,\dots,D$ to either $0$ or $1$ via the following operator.</p>\[W_{i}^{*}= \begin{cases} 1 &amp; y_{i}^{*}\ge 0.5\\ 0 &amp; y_{i}^{*}&lt; 0.5 \end{cases}\]<p>the corresponding component $\tilde{x}_{i}$</p><p>for is selected (excluded) for the representation of the underlying trend if</p><p>$W_{i}^{\ast}=1( \text{ or } W_{i}^{\ast}=0)$</p><p>Finally the underlying trend of the signal is obtained by</p>\[\Gamma = AW^*\]<p>where $W^{\ast}= [W_1^{\ast},\dots,W_D^{\ast} ]^T$. The quantized solution is employed for the approximation of the solution of original <a class="internal-link" href="/2017/04/11/sparse-representation">sparse binary programming problem</a>. It is found that the solution obtained by the proposed method is very close to the actual solution of the original sparse binary programming problem. That is, $W^{\ast}$ in is very close to $z^{\ast}$ above.</p><p>Therefore, the subset $I_{1}$ can be obtained by</p>\[I_{1} = \{i\vert W_{i}^{*}= 1, 1\le i \le D\}\]<p>After obtaining the underlying trend $\Gamma$ for the first $N$ points, the above SSA procedure can be applied for the prediction of the underlying trend for future time indices.</p><p>Denote $\Gamma =[\gamma(1), \dots, \gamma(N)]^T$. In order to predict the underlying trend in the future time indices, we assume that there is an underlying structure in the time series and this structure is preserved for the time period to be predicted. A prediction model based on the linear recurrent formulae (LRF) is employed.</p><p>That is, the points $\gamma(N-L+2), \dots, \gamma(N)$ are employed for the prediction of $\gamma(N+1)$, and so on. In other words,</p>\[\gamma(n+1) = \sum_{k=0}^{L-2}a_{k}\gamma(n-k) \text{ for } n\ge N,\]<p>where the coefficient vector of the LRF denoted as $R=(a_{L-2}, \dots, a_{0})^T$ is given by</p>\[R = \frac{1}{1-\nu^{2}}\sum_{i\in I_{1}}\pi_{i}U_{i}^{\nabla}\]<p>Here, $\nu^{2=}\sum_{i\in I_{1}}\pi_{i}^2$, $\pi_{i}$ is the last element in $U_{i}$, and $U_{i}^{\nabla }\in R^{L-1}$ is the vector only containing the first $L-1$ elements of $U_{i}$ for $i \in I_{1}.$</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
