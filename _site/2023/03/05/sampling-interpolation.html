<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Sampling and interpolation" /><meta property="og:locale" content="en_US" /><meta name="description" content="Sampling and interpolation" /><meta property="og:description" content="Sampling and interpolation" /><link rel="canonical" href="http://localhost:4000/2023/03/05/sampling-interpolation" /><meta property="og:url" content="http://localhost:4000/2023/03/05/sampling-interpolation" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-05T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Sampling and interpolation" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-05T00:00:00+00:00","datePublished":"2023-03-05T00:00:00+00:00","description":"Sampling and interpolation","headline":"Sampling and interpolation","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/03/05/sampling-interpolation"},"url":"http://localhost:4000/2023/03/05/sampling-interpolation"}</script><title> Sampling and interpolation - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/03/05/sampling-interpolation" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Nowadays, a lot of time series data is produced, which represents the state of the environment over a periodof time [15]. These data points are generally cap..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Nowadays, a lot of time series data is produced, which represents the state of the environment over a periodof time [15]. These data points are generally cap..."/><meta property="og:title" content="Sampling and interpolation"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-03-05T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/03/05/sampling-interpolation" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Sampling and interpolation</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-03-05T00:00:00+00:00">March 5, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/sampling">sampling</a>, <a href="/tag/interpolation">interpolation</a>, <a href="/tag/numerical_methods">numerical_methods</a></span></div><p align="justify">Nowadays, a lot of time series data is produced, which represents the state of the environment over a period of time [15]. These data points are generally captured by a piece of equipment called a sensor. The sensor can detect di erent events or changes in the environment and quantify the changes in the form of temperature, pressure, noise, or light intensity, among others. A limitation of collecting data points is the frequency at which the sensor records the changes or events. The more frequently a sensor records a reading, the more expensive the running cost is. Likewise, the less frequent the sensor records the reading, the more difficult it is to capture and reconstruct the original behaviour of the event. In practice, all signals have to be sampled because the number of points in a continuous environment is infinite</p><p align="justify">Sampling is the mechanism that collects the information by setting the frequency of the collected points over a time period. Capturing readings more often is economically more expensive due to the amount of data being stored, transmitted, and processed. The challenge while performing sampling is to preserve the vital information in the less amount of data points so that the objective of recording changes is met. The periodic or Riemann sampling [15] is a conventional approach of sampling in the time series data. In this approach, the data is captured periodically, i.e. at an equidistant time intervals (such as each second or each microsecond). Even though the approach is simple to implement, the shortcoming is that, when the sampled data fails to indicate changes that happen between the interval (also known as frequency aliasing), sampling needs to be readjusted at a higher frequency, resulting into more data collection. Firstly, making such an adjustment requires manual assessment, and in addition to that, it bears the additional cost</p><p align="justify">concerning more data being generated. Due to this pitfall, many research findings advocated for the use of Lebesgue sampling, instead of Riemann sampling [24]. Furthermore, some authors [2] have demonstrated Lebesgue sampling being a more ecient strategy compared to Riemann sampling. The Lebesgue sampling [4], also known as Event-based sampling, is an alternative sampling strategy to the more popular Riemann sampling strategy. In the Lebesgue sampling, the time-series data is sampled whenever a signi cant change takes place or when the measurement passes a certain limit [17]. A few motivating examples of sampling strategy would be: whenever a speci c value of the sensor reading crosses a limit, when a data packet arrives at a node on a computer network, or when the system output has changed with a speci ed amount.</p><p align="justify"> The overall intuition of the Lebesgue sampling is to save the unnecessary data from being stored, processed, or transmitted which represents either no change or a trivial change compared to the previous data point. The nature of sampling based on events in the Lebesgue sampling is very appealing and natural in many domains where the systems remain constant for an extended period such as wireless communications [19] or systems with an on-o mechanism like those in the satellite control [2]. Increasing the battery life of the sensors and reducing their use [25], reducing network trac by decreasing the amount of information transferred [32], or using fewer computer resources while maintaining the same performance [31], are some of the advantages of event-based control over control based in time.</p><p align="justify">By contrast, the management of the systems that implement Lebesgue sampling becomes more complicated [2]. When a time series signal is sampled, generally, the subsequent step is to reconstruct the original signal as accurately as possible [14]. The interpolation method is one of the well-known criteria to reconstruct the signal by lling the missing values between the range of the discrete set of data points. Despite the signi cance of the interpolation methods, the challenge of reconstructing the signal remains an important area of research.</p><p align="justify">Moreover, common interpolation methods do not perform well on Lebesgue sampling as demonstrated in this contribution. In this contribution, it is proposed interpolation methods to reconstruct the time-series data sampled using Lebesgue sampling. To the best of our knowledge, this is the rst interpolation method designed exclusively for Lebesgue sampling. The proposed methods have a higher performance because they exploit the particular properties of this kind of sampling. Two novel interpolation methods are proposed: ZeLiC and ZeChipC. ZeLiC uses Zero-order hold and Linear interpolation with speci c shape approximation on the basis of Concavity/Convexity1. On the other hand, ZeChipC uses Zero-order hold and PChip interpolation with the same Concavity/Convexity improvement as ZeLiC.</p><h3 id="lebesgue-sampling">Lebesgue Sampling</h3><p align="justify">Lebesgue sampling is an alternative to the traditional approach of sampling time series at a constant frequency. Instead of periodically taking samples from a system like in Riemann sampling, the event-based method takes samples only when a prede ned event happens as shown in Figure 1. Some examples of typical events could be a sudden change in the signal, the signal reaching a preset limit, the arrival of a data package, or a change in the state of a system [2].</p><p align="justify">Even though Lebesgue sampling is more accurate than Riemann sampling it is less extended because those systems are more dicult to implement [2]. In recent years, a great interest has aroused in applications implementing event-based sampling. For example, the \Send on Delta" algorithm takes advantage of Lebesgue sampling to reduce the information transmitted by wireless networks in order to increase the lifetime of the sensors batteries. Under this scheme, the sampling is performed only when there is a deviation in the signal higher than the delta value. Results show that using this approach it is possible to increase the lifetime of the sensors without any loss of quality in the resolution of the signals [25].</p><p align="justify">We can find another positive example in the domain of Networked Control Systems (NCSs), where the advantages of Lebesgue sampling become clear. In this type of systems increasing the sampling frequency can be counterproductive since the information load increases and the trac of the network can collapse the whole system functioning. In the last decade, many NCS have successfully implemented event-triggered control reducing the required resources and the bandwidth of the network [32]. It is also interesting pointing out the convenience of using event-based sampling in the Fault Diagnosis and Prognosis (FDP). In the last years, it has been increasingly dicult to manage microcontrollers and embedded systems due to the volume of the information collected by sensors and to the complexity of the programs that they implement. Increasing computational resources is not a good solution in the long term since it increases economic costs. Yan et al. [31] found an ecient solution to this problem applying the philosophy of \execution only when necessary" based on Lebesgue sampling, which reduces computational costs substantially without diminishing the performance of the system.</p><p align="justify"> Some research has been done to nd the optimal balance between the number of samples and the performance of the system. For example, Andren, et al. [1] studied this balance for a linear-quadratic-Gaussian (LQG) control problem setting with output feedback. However, sampling based on changes works well with signals that remain constant for some time and present sudden variations. This is because this kind of sampling captures a higher number of points when the signal has abrupt changes while it does not take points when the signal remains constant. For example, when a natural phenomenon like an earthquake takes place, the sensor values can go from zero to a positive high value in an instant. With sampling based on events, more points of the critical moments would be captured, which gives important information about the behaviour of the phenomenon while if nothing occurs no information will be captured. Lebesgue sampling can minimise energy consumption, storage space, computation time and the amount of data to be transmitted as it has been claimed in many investigations [30].</p><h3 id="time-series-interpolation-methods">Time series interpolation methods</h3><p>The downsampled time series data can be reconstructed using different interpolation techniques. The interpolation function estimates the missing data points within the range of the discrete set of known data points with the objective of preserving the shape of the original signal before the application of downsampling [22]. Let $\left(x_i, y_i\right), i=0, \ldots, n$ be pairs of real values, where $f$ is the interpolation function, $x_i$ are the indexes of the downsampled data points, and $y_i$ are the values of those points. The objective of the optimal interpolation technique is to satisfy the condition where $f\left(x_i\right)$ verifies $y_i$. \(f\left(x_i\right)=y_i \text { for } i=0, \ldots, n\) There are a number of interpolation techniques ranging from simpler ones such as Zero-order hold [11] and Linear [11], to a more complex ones such as Multiquadric [18] which is based on radial basis function, Shannon [23] which is based on Nyquist-Shannon sampling theorem, Lasso [29] which is based on regression, Natural Neighbour [8] which is a spatial method and provides smoother approximation compared to simple interpolation technique. Cubic Hermite spline [21] and Piecewise Cubic Hermite Interpolating Polynomial (PCHIP) [20] are interpolation techniques based on splines and cubic function respectively. They are often a preferred choice in the polynomial interpolation.</p><p>The relation 2 describes the objective function of an interpolation method. Let $\left(x_i, y_i\right), i=0, \ldots, n$ pairs of real values. We want to find a function $f$ (easy to calculate), where $f$ verifies \(f\left(x_i\right)=y_i \text { for } i=0, \ldots, n\)</p><h4 id="zero-order-hold-interpolation">Zero-order hold interpolation</h4><p>The zero-order hold (ZOH) interpolation is one of the simplest signal reconstruction techniques [11]. In this technique, the missing values between two sampled points are interpolated with a constant value. This constant value is the same value of the preceding known point before encountering missing values. This technique has several applications in electrical communication and the main advantage is its low computational complexity. However, this interpolation strategy fails to reconstruct continuity or trend in time series with non zero values for the first derivative.</p><p>In ZOH, the polynomial $C_i(x)$ is of 0 th degree. Therefore, $C_i(x)=c$ where $c$ is constant. Because of (2), $C_i\left(x_{i-1}\right)=y_{i-1}$. If in $x_i$, a sudden change is presented, we cannot represent it because $C_{i+1}\left(x_i\right)=y_i \neq y_{i-1}$. Therefore, this interpolation cannot be used neither to represent continuous functions nor to produce a natural curves.</p><h4 id="first-order-or-linear-interpolation">First order or Linear interpolation</h4><p>Linear interpolation is also another popular choice for the reconstruction of a signal due to its simplicity and low computational complexity . In this method, missing values are reconstructed by fitting a straight line between successive known points. The shortcoming is that the reconstruction of a signal fails to capture any non-linear trend even though the overall known values follow a non-linear trend.</p><p>The polynomial $C_i(x)$ is of 1st degree, which means $C_i(x)=a x+b$, and because of (2), $C_i\left(x_{i-1}\right)=y_{i-1}$ and $C_i\left(x_i\right)=y_i$, where each two consecutive pair of sampled points are connected using a straight line. The following formulas can be applied to calculate $a$ and $b, \forall i=1, \ldots, n$ \(\left\{\begin{array}{l} a=\frac{y_i-y_{i-1}}{x_i-x_{i-1}} \\ b=y_i-a x_i \end{array}\right.\)</p><p>This interpolation gives a continuous but non-differentiable function $f$. Let $C_i(x)=a_i x+b_i$ and $C_{i+1}=$ $a_{i+1} x+b_{i+1}$, we have \(\begin{aligned} &amp; \lim _{x^{-} \rightarrow x_i} f^{\prime}(x)=a_i \\ &amp; \lim _{x^{+} \rightarrow x_i} f^{\prime}(x)=a_{i+1} \end{aligned}\) Therefore, the function is differentiable only if $a_i=a_{i+1}, \quad \forall i=1, \ldots, n$. We can conclude that for non-linear functions, $f$ is not differentiable with linear interpolation.</p><p>Linear interpolation is fast to compute and very intuitive. However, the drawback is the non-derivability of the interpolation at each node, which makes it produce sharp changes in the reconstructed signal.</p><h4 id="spline-interpolation-methods">Spline interpolation methods</h4><p>In the spline interpolation methods, the interpolation function is a particular case of piecewise polynomial. The advantage of this type of method over the first order interpolation is that it reconstructs the signal using non-linear functions (which makes the transitions smoother) and also avoids the Runge phenomenon [5, 28]. The Runge phenomenon refers to the oscillation at the edges of a given interval while interpolating missing values.</p><p>We can define $f$ as $f(x)=C_i(x)$ for $x \in\left[x_{i-1}, x_i\right], i=1, \ldots, n$ where $C_i$ is a polynomial of small degree. Where $C_i\left(x_i\right)=C_{i+1}\left(x_i\right)=f\left(x_i\right)=y_i$. The most common degrees in terms of interpolation are the first and third degree, which are Linear and Cubic interpolation. 2.2.4 Third order or Cubic interpolation The cubic function is one of the most commonly used spline interpolation methods [21]. It uses a third-degree polynomial in the Hermite form for interpolating missing values as follows $C_i(x)=a x^3+b x^2+c x+d$. This interpolation strategy inherits the conditions of Linear interpolation and adds conditions on its first and second derivatives: \(\begin{aligned} C_i\left(x_i\right) &amp; =C_{i+1}=f\left(x_i\right) \\ C_i^{\prime}\left(x_i\right) &amp; =C_{i+1}^{\prime}=f^{\prime}\left(x_i\right) \\ C_i^{\prime \prime}\left(x_i\right) &amp; =C_{i+1}^{\prime \prime}=f^{\prime \prime}\left(x_i\right) \end{aligned}\) The strength of this interpolation strategy is the fact that it produces smooth curves in the region of missing values which makes the signal look natural. The drawback of this method is that it can lead to significant errors in the region of reconstruction when there is an abrupt change at the end of an interval. Where the derivative on the nodes (sampled points) must be equal, this change will be presented in the beginning of the next interpolated interval.</p><h4 id="piecewise-cubic-interpolating-polynomial-pchip">Piecewise Cubic Interpolating Polynomial (PCHIP)</h4><p>The PCHIP (Piecewise Cubic Hermite Interpolating Polynomial) interpolation method is based on the same principle as the spline interpolation, but between each point, it fits a cubic polynomial in Hermite’s form [20]. The sampled points are known as “knots”, PCHIP connects those “knows” independently making a good performance in time and results. The given points have first derivative at the interpolated points, although the second derivative is not guaranteed to be continuous.</p><h3 id="other-interpolation-methods-for-lebesgue-sampling">Other interpolation methods for lebesgue sampling</h3><p>Lebesgue sampling is typically used in conjunction with interpolation techniques to reconstruct the continuous signal from the irregularly sampled data. Here are some interpolation techniques that are commonly used with Lebesgue sampling:</p><ol><li><p>Sinc interpolation: Sinc interpolation is a commonly used technique for reconstructing a continuous signal from its samples. It involves using the sinc function, which is the Fourier transform of the rectangular window function used to sample the signal, to interpolate between samples.</p></li><li><p>Polynomial interpolation: Polynomial interpolation involves fitting a polynomial to the sampled data points and using the polynomial to interpolate between samples. This technique can be computationally efficient, but may not provide accurate interpolation for signals with high-frequency components.</p></li><li><p>Radial basis function (RBF) interpolation: RBF interpolation involves using a set of radial basis functions, such as Gaussian or inverse multiquadric functions, to interpolate between samples. This technique can be effective for signals with complex structure and can provide accurate interpolation with relatively few basis functions.</p></li><li><p>Wavelet interpolation: Wavelet interpolation involves using wavelet transforms to decompose the signal into different scales and interpolate the signal at each scale using a suitable interpolation technique. This technique can be effective for signals with complex structure and can provide accurate interpolation at different scales.</p></li></ol><p>Overall, the choice of interpolation technique depends on the specific characteristics of the signal and the desired accuracy and computational efficiency of the interpolation. It’s important to carefully evaluate the performance of each technique on a given signal and choose the one that provides the best results.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
