<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM" /><meta property="og:locale" content="en_US" /><meta name="description" content="ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM" /><meta property="og:description" content="ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM" /><link rel="canonical" href="http://localhost:4000/2023/03/07/whitening-for-kpnlms-algorithm" /><meta property="og:url" content="http://localhost:4000/2023/03/07/whitening-for-kpnlms-algorithm" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-07T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-07T00:00:00+00:00","datePublished":"2023-03-07T00:00:00+00:00","description":"ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM","headline":"ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/03/07/whitening-for-kpnlms-algorithm"},"url":"http://localhost:4000/2023/03/07/whitening-for-kpnlms-algorithm"}</script><title> ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/03/07/whitening-for-kpnlms-algorithm" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Introduction"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Introduction"/><meta property="og:title" content="ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-03-07T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/03/07/whitening-for-kpnlms-algorithm" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><section class="post"><h1 style="text-align: center;">ON WHITENING FOR KRYLOV-PROPORTIONATE NORMALIZED LMS ALGORITHM</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-03-07T00:00:00+00:00">March 7, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/adaptive_filtering">adaptive_filtering</a>, <a href="/tag/feedback">feedback</a>, <a href="/tag/subspace_methods">subspace_methods</a></span></div><h2 id="introduction">Introduction</h2><p align="justify">The tradeoffproblem between convergence speed and computational complexity has been the most important and challenging issue in adaptive filtering [1]. Since the birth of the classical least mean square (LMS) algorithm, a variety ofalgorithms have been proposed, such as the normalized LMS (NLMS) algorithm [2], the affine projection algorithm (APA) [3,4], and the adaptive parallel subgradient projection (PSP) algorithm [5].</p><p align="justify">Recently, the Krylov-proportionate normalized least-meansquare (KPNLMS) algorithm has been proposed [6], which extends the well-known (sparsity-based) proportionate normalized leastmean- square (PNLMS) algorithm [7, 8] to a nonsparse estimandum (system to be estimated). Here, sparse means that most components have negligibly small magnitudes. The KPNLMS algorithm constructs a set of orthonormal basis vectors, with which the estimandum has sparse coefficients, by using a certain Krylov subspace, and utilizes the sparsity for raising the speed of convergence. The gain offered by KPNLMS depends on the energy ofthe estimandum in the Krylov subspace.</p><h2 id="preliminaries">PRELIMINARIES</h2><h3 id="problem-formulation">Problem Formulation</h3><p>We consider a simple linear system model: \(d_k:=\boldsymbol{u}_k^T \boldsymbol{h}^*+n_k, k \in \mathbb{N}\) where $u_k:=\left[u_k, u_{k-1}, \cdots, u_{k-N+1}\right]^T \in \mathbb{R}^N$ is the input vector at time $k$ with the input process $\left(u_k\right)<em>{k \in \mathbb{N}}, \boldsymbol{h}^* \in \mathbb{R}^N$ the estimandum, $\left(d_k\right)</em>{k \in \mathbf{N}}$ the output process, and $\left(n_k\right)<em>{k \in \mathbf{N}}$ the noise process $\left(N \in \mathbb{N}^*:=\mathbb{N} \backslash{0}\right)$. Assume that the input and output data are available. An adaptive filter $\left(\boldsymbol{h}_k\right)</em>{k \in \mathrm{N}}$ is controlled in a recursive way for minimizing the mean-square error (MSE): \(\operatorname{MSE}(\boldsymbol{h}):=E\left\{e_k^2(\boldsymbol{h})\right\}, \boldsymbol{h} \in \mathbb{R}^N \text {. }\) Here, $E{\cdot}$ denotes expectation and $e_k: \mathbb{R}^N \rightarrow \mathbb{R}, \boldsymbol{h} \mapsto \boldsymbol{u}<em>k^T \boldsymbol{h}-d_k$, the error function at time $k$. The filter $\boldsymbol{h}</em>{\text {MMSE }} \in \mathbb{R}^N$ minimizing (2) is called the minimum MSE (MMSE) filter, characterized by the the so-called Wiener-Hopf equation: $\boldsymbol{R} \boldsymbol{h}<em>{\mathrm{MMSE}}=\boldsymbol{p}$, where $\boldsymbol{R}:=$ $E\left{\boldsymbol{u}_k \boldsymbol{u}_k^T\right} \in \mathbb{R}^{N \times N}$ and $\boldsymbol{p}:=E\left{\boldsymbol{u}_k d_k\right} \in \mathbb{R}^N$. The matrix $\boldsymbol{R}$ is mostly positive definite due to the presence of noise, and in this case, the MMSE filter is uniquely given by $\boldsymbol{h}</em>{\mathrm{MMSE}}=\boldsymbol{R}^{-1} \boldsymbol{p}$. In the following, the MMSE filter is denoted by $\boldsymbol{h}^<em>$, because $\boldsymbol{h}_{\mathrm{MMSE}}=\boldsymbol{h}^</em>$ under the natural assumption $E\left{\boldsymbol{u}_k n_k\right}=0$, where 0 denotes the zero vector.</p><h3 id="brief-review-of-the-kpnlms-algorithm">Brief Review of the KPNLMS Algorithm</h3><p>The update equation of the KPNLMS algorithm is given as [6] \(\boldsymbol{h}_{k+1}=\boldsymbol{h}_k-\lambda_k e_k\left(\boldsymbol{h}_k\right) \frac{\boldsymbol{\Omega}_k \boldsymbol{u}_k}{\boldsymbol{u}_k^T \boldsymbol{\Omega}_k \boldsymbol{u}_k}, k \in \mathbb{N}\) Here, $\lambda_k \in[0,2]$ is the step size, and the matrix $\boldsymbol{\Omega}_k:=\boldsymbol{U} \boldsymbol{\Lambda}_k \boldsymbol{U}^T \in$ $\mathbb{R}^{N \times N}, k \in \mathbb{N}$, is positive definite with the orthogonal matrix $U \in$ $\mathbb{R}^{N \times N}$ and the positive diagonal matrix $\Lambda_k \in \mathbb{R}^{N \times N}, k \in \mathbb{N}$. The key is how to construct $\boldsymbol{U}$ and $\boldsymbol{\Lambda}_k$.</p><p>To explain the construction of the matrix $\boldsymbol{U}$, let us define, for a given $(N \geq) D \in \mathbb{N}^*$, a matrix-valued function $\boldsymbol{K}_D: \mathbb{R}^{N \times N} \times$ $\mathbb{R}^N \rightarrow \mathbb{R}^{N \times D}$ as \(\boldsymbol{K}_D(\boldsymbol{A}, \boldsymbol{b}):=\left[\boldsymbol{b}, \boldsymbol{A} \boldsymbol{b}, \cdots, \boldsymbol{A}^{D-1} \boldsymbol{b}\right], \forall \boldsymbol{A} \in \mathbb{R}^{N \times N}, \boldsymbol{b} \in \mathbb{R}^N .\) Then, \(\mathcal{K}_D(\boldsymbol{A}, \boldsymbol{b}):=\mathcal{R}\left\{\boldsymbol{K}_D(\boldsymbol{A}, \boldsymbol{b})\right\} \subset \mathbb{R}^N\) is called the $D$ th Krylov subspace associated with $\boldsymbol{A}$ and $\boldsymbol{b}$, where $\mathcal{R}{\cdot}$ stands for range.</p><p>The matrix $U \in \mathbb{R}^{N \times N}$ is constructed by orthogonalizing the columns of $\boldsymbol{K}_N(\widehat{\boldsymbol{R}}, \widehat{\boldsymbol{p}})$, where $\widehat{\boldsymbol{R}} \in \mathbb{R}^{N \times N}$ and $\widehat{\boldsymbol{p}} \in \mathbb{R}^N$ are estimates of $\boldsymbol{R}$ and $\boldsymbol{p}$, respectively. The construction of $\boldsymbol{\Lambda}_k$ borrows the idea of the PNLMS algorithm $[7,8]$. If the improved PNLMS (IPNLMS) algorithm [11] is adopted, the diagonal matrix is given as \(\boldsymbol{\Lambda}_k:=\operatorname{diag}\left\{\boldsymbol{\theta}^{(k)}\right\} \in \mathbb{R}^{N \times N}, k \in \mathbb{N}\) with \(\begin{aligned} \boldsymbol{\theta}^{(k)} &amp; :=\frac{1-\eta}{N} \mathbf{1}_N+\frac{\eta}{\left\|\tilde{\boldsymbol{h}}_k\right\|_1+\varepsilon}\left|\tilde{\boldsymbol{h}}_k\right| \in \mathbb{R}^N, k \in \mathbb{N}, \\ \tilde{\boldsymbol{h}}_k &amp; :=\boldsymbol{U}^T \boldsymbol{h}_k \in \mathbb{R}^N, k \in \mathbb{N}, \\ \mathbf{1}_N &amp; :=[1,1, \cdots, 1]^T \in \mathbb{R}^N . \end{aligned}\) Here, $|\cdot|$ and $|\cdot|_1$ denote the elementwise absolute-value operation and 1-norm, respectively, $\eta \in(0,1)$ a parameter to control the amount of proportionality in the update, and $\varepsilon&gt;0$ a small positive constant for regularization. If $\boldsymbol{U}=\boldsymbol{I}(\boldsymbol{I}$ : identity matrix), then KPNLMS coincides with PNLMS.</p><p>It should be mentioned that another key point of KPNLMS is simplification to keep $O(N)$ computational complexity per iteration, which is practically important but is omitted in this paper for conciseness (see [6]). Moreover, an extension of KPNLMS to complexvalued signals and its application to wireless communication systems will be presented at this workshop [12], in which the whitening is not used because the correlation of the input signals is fairly low.</p><h2 id="whitening-in-the-kpnlms-algorithm">WHITENING IN THE KPNLMS ALGORITHM</h2><p>The KPNLMS algorithm realizes fast convergence when \(\tilde{\boldsymbol{h}}^*:=\boldsymbol{U}^T \boldsymbol{h}^* \in \mathbb{R}^N\) is sparse; the sparsity is measured by a certain energy. In [6], only an intuitive discussion is given about the motivation for whitening in case of highly colored input signals.</p><h3 id="on-whitening-for-kpnlms">On Whitening for KPNLMS</h3><p>Let $\boldsymbol{V} \in \mathbb{R}^{N \times N}$ (or $\boldsymbol{V} \in \mathbb{C}^{N \times N}$ ) be a prespecified orthogonal (or unitary) matrix, such as the DCT (discrete cosine transform) or the DFT (discrete Fourier transform) matrix. For simplicity, $\boldsymbol{V}$ is assumed to be real-valued in the following (an extention to the complex-valued case is straightforward). As in TDAF, the diagonal matrix is defined as \(\boldsymbol{D}_k:=\operatorname{diag}^{-1}\left\{\sigma_1^{(k)}, \sigma_2^{(k)}, \cdots, \sigma_N^{(k)}\right\}, k \in \mathbb{N}\) where, for given initial estimates $\sigma_n^{(0)}&gt;0, n=1,2, \cdots, N$, $\sigma_n^{(k)}:=\zeta \sigma_n^{(k-1)}+(1-\zeta)\left[\boldsymbol{V} \boldsymbol{u}<em>k\right]_n^2&gt;0, k \in \mathbb{N}$, with the forgetting factor $\zeta \in(0,1)$. Here, $[\cdot]_n$ is the $n$th element of a vector. Then, the whitened input vector is generated as \(\phi_k:=\boldsymbol{\Phi}_k^{1 / 2} \boldsymbol{u}_k \in \mathbb{R}^N, k \in \mathbb{N}\) with $\boldsymbol{\Phi}_k:=\boldsymbol{V}^T \boldsymbol{D}_k \boldsymbol{V} \in \mathbb{R}^{N \times N}, k \in \mathbb{N}$. The standard NLMS algorithm with the whitened input $\phi_k$ tracks the modified solution $\boldsymbol{\omega}^*$ characterized by $\boldsymbol{R}</em>\phi \boldsymbol{\omega}^<em>=\boldsymbol{p}_\phi\left(\Leftrightarrow \boldsymbol{R} \boldsymbol{\Phi}_k^{1 / 2} \boldsymbol{\omega}^</em>=\boldsymbol{p}\right)$, where $\boldsymbol{R}<em>\phi:=E\left{\phi_k \phi_k^T\right} \in \mathbb{R}^{N \times N}$ and $p</em>\phi:=E\left{\phi_k d_k\right} \in \mathbb{R}^N$. Hence, for letting the algorithm track the original solution $\boldsymbol{h}^*$, the update equation (12) with $\boldsymbol{v}<em>k=\phi_k\left(\boldsymbol{G}_k=\boldsymbol{\Phi}_k\right)$ should be left-multiplied by $\boldsymbol{\Phi}_k^{1 / 2}$, leading to \(\boldsymbol{h}_{k+1}=\boldsymbol{h}_k-\lambda_k e_k\left(\boldsymbol{h}_k\right) \frac{\boldsymbol{\Phi}_k \boldsymbol{u}_k}{\boldsymbol{u}_k^T \boldsymbol{\Phi}_k \boldsymbol{u}_k}, k&lt;K_0,\) where $\boldsymbol{h}_k:=\boldsymbol{\Phi}_k^{1 / 2} \boldsymbol{w}_k ;$ (16) is nothing but (11) for $\boldsymbol{G}_k=\boldsymbol{\Phi}_k$. Once obtaining adequate estimates of $\boldsymbol{R}</em>{\boldsymbol{\phi}}$ and $\boldsymbol{p}_\phi$, we can construct the orthogonal matrix $\boldsymbol{U}$ as in the original KPNLMS algorithm. Next is the construction of the diagonal matrix $\boldsymbol{\Lambda}_k$. Note that</p><ol><li>$\boldsymbol{h}_k$ tracks $\boldsymbol{h}^*$;</li><li>(not the original solution $\boldsymbol{h}^<em>$ but) the modified solution $\boldsymbol{\omega}^</em>(=$ $\left.\boldsymbol{\Phi}<em>k^{-1 / 2} \boldsymbol{h}^*\right)$ tends to have large energy in $\mathcal{K}_D\left(\boldsymbol{R}</em>\phi, \boldsymbol{p}_\phi\right)$. Thus, $\boldsymbol{\Lambda}_k$ should be constructed according, instead of $\tilde{\boldsymbol{h}}_k$, to \(\ddot{\boldsymbol{h}}_k:=\boldsymbol{U}^T \boldsymbol{\Phi}_k^{-1 / 2} \boldsymbol{h}_k\) The final and important point is how to combine $\boldsymbol{\Phi}_k$ and $\boldsymbol{\Omega}_k$. The original KPNLMS algorithm can be viewed as follows: (i) the input is modified by $\Omega_k^{1 / 2}$, and then (ii) the update equation (12) is left-multiplied by $\Omega_k^{1 / 2}\left(=G_k^{1 / 2}\right)$ for tracking the original solution $h^<em>$. In case that whitening is involved, the input $\phi_k$ is further modified by $\boldsymbol{\Omega}_k^{1 / 2}\left(=\boldsymbol{U} \boldsymbol{\Lambda}_k^{1 / 2} \boldsymbol{U}^T\right)$, implying that the solution $\boldsymbol{\omega}^</em>$ is also modified further by $\Omega_k^{1 / 2}$. Therefore, for tracking $\boldsymbol{h}^*$, the update equation with the doubly-modified input $\boldsymbol{v}_k=\Omega_k^{1 / 2} \boldsymbol{\phi}_k$ should be left-multiplied by $\boldsymbol{\Phi}_k^{1 / 2} \boldsymbol{\Omega}_k^{1 / 2}$, leading to \(\boldsymbol{h}_{k+1}=\boldsymbol{h}_k-\lambda_k e_k\left(\boldsymbol{h}_k\right) \frac{\boldsymbol{\Pi}_k \boldsymbol{u}_k}{\boldsymbol{u}_k^T \boldsymbol{\Pi}_k \boldsymbol{u}_k}, k \geq K_0,\) where $\boldsymbol{h}_k:=\boldsymbol{\Phi}_k^{1 / 2} \boldsymbol{\Omega}_k^{1 / 2} \boldsymbol{w}_k$ and $\boldsymbol{\Pi}_k:=\boldsymbol{\Phi}_k^{1 / 2} \boldsymbol{\Omega}_k \boldsymbol{\Phi}_k^{1 / 2}$. Simplification is possible to attain $O(N)$ complexity by following the way in [6].</li></ol><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main></body></html>
