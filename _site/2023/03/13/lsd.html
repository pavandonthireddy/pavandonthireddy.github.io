<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Low rank plus sparse decomposition" /><meta property="og:locale" content="en_US" /><meta name="description" content="Low rank plus sparse decomposition" /><meta property="og:description" content="Low rank plus sparse decomposition" /><link rel="canonical" href="http://localhost:4000/2023/03/13/lsd" /><meta property="og:url" content="http://localhost:4000/2023/03/13/lsd" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-03-13T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Low rank plus sparse decomposition" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-13T00:00:00+00:00","datePublished":"2023-03-13T00:00:00+00:00","description":"Low rank plus sparse decomposition","headline":"Low rank plus sparse decomposition","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/03/13/lsd"},"url":"http://localhost:4000/2023/03/13/lsd"}</script><title> Low rank plus sparse decomposition - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/03/13/lsd" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Low-rank and sparse decomposition is a technique used in signal processing, image processing, and machine learning to decompose a matrix into two components:..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Low-rank and sparse decomposition is a technique used in signal processing, image processing, and machine learning to decompose a matrix into two components:..."/><meta property="og:title" content="Low rank plus sparse decomposition"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-03-13T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/03/13/lsd" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Low rank plus sparse decomposition</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-03-13T00:00:00+00:00">March 13, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/signal_extraction">signal_extraction</a>, <a href="/tag/subspace_methods">subspace_methods</a></span></div><p>Low-rank and sparse decomposition is a technique used in signal processing, image processing, and machine learning to decompose a matrix into two components: a low-rank component and a sparse component.</p><p>A matrix is said to be low-rank if it can be approximated by a matrix of much lower rank. For example, a rank-10 matrix can be approximated by a rank-3 matrix if the approximation is good enough. A matrix is said to be sparse if most of its elements are zero.</p><p>The low-rank and sparse decomposition technique seeks to find a low-rank matrix L and a sparse matrix S such that the sum of L and S is equal to the original matrix M. This can be expressed as:</p><p>M = L + S</p><p>The low-rank component L represents the underlying structure or pattern in the data, while the sparse component S represents the noise or outliers in the data.</p><p>This technique has many applications, such as in image denoising, video compression, and anomaly detection. It is often used in combination with other techniques such as principal component analysis (PCA) and singular value decomposition (SVD) to extract meaningful information from complex data sets.</p><h2 id="introduction">Introduction</h2><p>Let us consider the problem of the enhancement of a speech signal contaminated by an independent additive noise. Let $x(t)$ and $d(t)$ denote the sampled clean speech and noise signal, respectively. The observed noisy speech signal $y(t)$ is \(y(t)=x(t)+d(t)\) Suppose $y(t)$ was framed with the length $N$. Arranging the $N$-dimensional vectors into a $(M-1+l) \times l$ Toeplitz structure matrix, we can get \(Y=X+D\) Assuming that the rank of matrix $Y$ is $r$, the optimal enhanced speech matrix $\hat{X}$ can be estimated according to the following least-square criterion \(\min _{\hat{X}}\|Y-\hat{X}\|_F^2, \operatorname{rank}(\hat{X}) \leq r\) where symbol $|<em>F$ denotes the Frobenius norm of a matrix and $|X|_F=\sqrt{X</em>{i j}^2}$</p><p>If $d(t)$ is a white Gaussian noise, it satisfies the conditions $D^T D=\sigma_d^2 I$ and $X^T D=0$. Where $\sigma_d^2$ is the variance of noise. The optimal solution of (4) can be obtained by applying singular value decomposition (SVD) of $Y$. \(\begin{aligned} &amp; Y=U \Sigma V^T \\ &amp; \hat{X}=\sum_{i=1}^r \lambda_i U_i V_i^T . \end{aligned}\) Here, $U$ and $V$ are two orthogonal matrices holding the left and right (approximate) singular vectors of given matrix, and $\Lambda$ is a diagonal matrix holding the singular values: $\lambda_1 \geq \lambda_2 \geq \cdots \lambda_{r-1} \geq \lambda_r$.</p><p>The above low-rank matrix $\hat{X}$ represents the original speech matrix $\mathrm{X}$ in the sense of least-square minimization. This may get the optimal estimate when the noise is small, independent, and identically distributed Gaussian.</p><p>However, PCA is highly sensitive to the presence of large corruptions. Even a single outlier in the data matrix can render the estimation of the low-rank component arbitrarily far from the true model. In [16], a new theory called Robust PCA was developed for this shortcoming. The basic idea of Robust PCA is to decompose the data matrix $M$ as $M=L+S$, where $S \hat{\mathrm{I}} \mathrm{i}^{N K}$ is a sparse matrix with a sparse number of non-zero coefficients with arbitrarily large magnitude. RPCA can be solved by minimizing the following convex program \(\min \|L\|_*+\lambda\|S\|_1 \text {, s.t. } M=L+S\) where $|-|_*$ denotes the matrix nuclear norm, which is defined as the sum of all singular values and is suggested as a convex surrogate to the rank function [18]. ||$_1$ denotes the $l_1$-norm of a matrix, which is defined as the sum of the absolute values of matrix elements. This problem is known to have a stable solution provided $L$ and $S$ are sufficiently incoherent [19], i. e., the low-rank matrix is not sparse and the sparse matrix is not low-rank. More recently, RPCA theory was introduced into the s peech enhancement task in [20], where a constrained low-rank and sparse matrix decomposition (CLSMD) algorithm is designed for noise reduction.</p><h3 id="lsd-based-speech-denoising-method">LSD based speech denoising method</h3><p>In this work, we propose a new subspace decomposition algorithm based on the LSD, which is less sensitive to the large noise interferences.</p><p>Firstly, we formulate the speech enhancement problem as the following optimization problem, \(\begin{aligned} &amp; \min _{\mathrm{L}, \mathrm{S}}\|Y-L-S\|_F^2, \\ &amp; \text { s.t. } \operatorname{rank}(L) \leq r,|S|_0 \leq h . \end{aligned}\) The above formula can be solved by alternatively solving the following two formulas until convergence \(\left\{\begin{array}{l} L_i=\underset{\operatorname{rank}(L) \leq r}{\arg \min }\left\|Y-L-S_{i-1}\right\|_F^2 \\ S_i=\underset{\mid S b \leq h}{\arg \min }\left\|Y-L_i-S\right\|_F^2 \end{array}\right.\) Given an estimate of sparse matrix $S_{i-1}$, the minimization in (7-a) over $L$ is to learn a rank- $r$ low-rank matrix from partial observations. This is a fixed-rank approximation problem, we can solve it use bilateral random projections (BRP) based fast low-rank matrix approximation. \(L_t=M_1\left(A_2^T M_1\right)^{-1} M_2^T\) Where $M_1=Y A_1, M_2=Y^T A_2$. Both $A_1 \in R^{n \times r}$ and $A_2 \in R^{m \times r}$ are Gaussian random matrices.</p><p>The minimization in (7-b) over $S$ is to learn a sparse matrix from partial observations. This can be computed via entry-wise hard thresholding function [21], $\varphi_T(x)=x \cdot 1(|x|&gt;u)$ which keeps the input if it is larger than the threshold; otherwise, it is set to zero. In summary, we have following optimization algorithm for LSD.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
