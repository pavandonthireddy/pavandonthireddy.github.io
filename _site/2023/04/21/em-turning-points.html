<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Expectation Maximization for Change Point Detection in Time Series" /><meta name="author" content="Pavan Donthireddy" /><meta property="og:locale" content="en_US" /><meta name="description" content="This article describes the Expectation Maximization algorithm for finding change points in time series data. The algorithm is explained for both batch and online mode, and Python implementation is provided." /><meta property="og:description" content="This article describes the Expectation Maximization algorithm for finding change points in time series data. The algorithm is explained for both batch and online mode, and Python implementation is provided." /><link rel="canonical" href="http://localhost:4000/2023/04/21/em-turning-points" /><meta property="og:url" content="http://localhost:4000/2023/04/21/em-turning-points" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-04-21T00:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Expectation Maximization for Change Point Detection in Time Series" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Pavan Donthireddy" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Pavan Donthireddy"},"dateModified":"2023-04-21T00:00:00+01:00","datePublished":"2023-04-21T00:00:00+01:00","description":"This article describes the Expectation Maximization algorithm for finding change points in time series data. The algorithm is explained for both batch and online mode, and Python implementation is provided.","headline":"Expectation Maximization for Change Point Detection in Time Series","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/04/21/em-turning-points"},"url":"http://localhost:4000/2023/04/21/em-turning-points"}</script><title> Expectation Maximization for Change Point Detection in Time Series - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/04/21/em-turning-points" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Introduction"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Introduction"/><meta property="og:title" content="Expectation Maximization for Change Point Detection in Time Series"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-04-21T00:00:00+01:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/04/21/em-turning-points" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Expectation Maximization for Change Point Detection in Time Series</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-04-21T00:00:00+01:00">April 21, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/Expectation_Maximization">Expectation_Maximization</a>, <a href="/tag/turning_points">turning_points</a>, <a href="/tag/changepoint">changepoint</a></span></div><h2 id="introduction">Introduction</h2><p>Change point detection in time series data is a fundamental problem in statistics and machine learning. A change point is a time at which some property of the time series changes abruptly. For example, a change point in stock prices could indicate a shift in market trends or a significant world event. Change point detection is therefore useful in many applications such as finance, weather forecasting, and medical research.</p><p>One approach for detecting change points is the Expectation Maximization (EM) algorithm. The EM algorithm is a powerful tool for finding the underlying structure of data that has been generated from an unknown source. It works by iteratively fitting a statistical model to the data, estimating the parameters of the model, and then using those parameters to predict the underlying structure of the data.</p><p>In this article, we will start by introducing the EM algorithm and its basic principles. We will then explain how it can be used to detect change points in time series data. Finally, we will demonstrate the algorithm through a Python implementation.</p><h2 id="the-expectation-maximization-algorithm">The Expectation Maximization Algorithm</h2><p>The Expectation Maximization algorithm is a general algorithm for finding maximum likelihood estimates of parameters in statistical models with hidden variables. It is often used when there is missing or incomplete information in the data.</p><p>In essence, the algorithm works by iteratively updating two sets of variables: the “expectation” variables (E-step) and the “maximization” variables (M-step). The expectation variables represent the expected value of the hidden variables given the observed data and the current estimate of the model parameters. The maximization variables represent the parameter estimates that maximize the likelihood of the observed data given the current estimate of the hidden variables.</p><p>The EM algorithm can be summarized in the following steps:</p><ol><li>Initialize the parameters of the model.</li><li>Compute the expectation variables given the current estimate of the parameters.</li><li>Compute the maximization variables by maximizing the likelihood function given the expectation variables.</li><li>Repeat steps 2 and 3 until convergence.</li></ol><p>This algorithm is guaranteed to converge to a local maximum of the likelihood function. However, there is no guarantee that this maximum is the global maximum. Therefore, it is often run multiple times with different initial conditions to ensure that the global maximum is found.</p><h2 id="em-algorithm-for-change-point-detection">EM Algorithm for Change Point Detection</h2><p>Now that we have covered the basic principles of the EM algorithm, let us explain how it can be used for change point detection in time series data.</p><p>Suppose we have a time series { $x_1$, $x_2$, …, $x_n$ } and we would like to detect any change points in the data. The EM algorithm can be used to fit a statistical model to the data that assumes that there are a fixed number of segments with different means.</p><p>Let us assume that the data is generated from the following statistical model:</p>\[x_i = \mu_{z_i} + \epsilon_i\]<p>where $z_i$ is the segment index, $\mu_{z_i}$ is the mean of segment $z_i$, and $\epsilon_i$ is the noise term for observation $i$.</p><p>We can think of this statistical model as a hidden Markov model, where the segment index $z_i$ is the hidden state and the observation $x_i$ is the observed variable.</p><p>To apply the EM algorithm, we need to define the expectation and maximization steps. Let $\theta = { \mu_1, …, \mu_k, \pi_1, …, \pi_k }$ be the set of parameters of the model, where $k$ is the number of segments in the data and $\pi_i$ is the probability of being in segment $i$.</p><h3 id="batch-mode">Batch Mode</h3><p>In the batch mode of the EM algorithm, we assume that we have access to the entire time series data at once. The algorithm proceeds as follows:</p><h4 id="expectation-step">Expectation Step:</h4><p>We compute the probability of being in segment $i$ given the observed data and the current estimate of the parameters:</p>\[\gamma_{i,j} = p(z_i = j \mid x_1, ..., x_n, \theta) = \frac{\pi_j f(x_i \mid \mu_j)}{\sum_{r=1}^k \pi_r f(x_i \mid \mu_r)}\]<p>where $f(x_i \mid \mu_j)$ is the probability density function of a normal distribution with mean $\mu_j$ and variance $\sigma^2$, and $\sigma$ is assumed to be constant for all segments.</p><h4 id="maximization-step">Maximization Step:</h4><p>We update the parameters by maximizing the log-likelihood of the observed data given the expectation variables, subject to the constraint that the probabilities of being in the different segments add up to one:</p>\[\begin{aligned} \pi_j &amp;= \frac{1}{n} \sum_{i=1}^n \gamma_{i,j} \\ \mu_j &amp;= \frac{\sum_{i=1}^n \gamma_{i,j} x_i}{\sum_{i=1}^n \gamma_{i,j}} \end{aligned}\]<p>We repeat the expectation and maximization steps until convergence.</p><h4 id="interpretation">Interpretation:</h4><p>The EM algorithm in batch mode provides us with the maximum likelihood estimates of the segment means and probabilities of being in each segment. We can use this information to identify the change points in the data. A change point is identified as the time where the segment mean changes.</p><h3 id="online-mode">Online Mode</h3><p>In the online mode of the EM algorithm, we assume that we have access to the time series data one observation at a time. We update the model parameters after each observation is received. The algorithm proceeds as follows:</p><h4 id="expectation-step-1">Expectation Step:</h4><p>The expectation step remains the same as in the batch mode:</p>\[\gamma_{i,j} = p(z_i = j \mid x_1, ..., x_n, \theta) = \frac{\pi_j f(x_i \mid \mu_j)}{\sum_{r=1}^k \pi_r f(x_i \mid \mu_r)}\]<h4 id="maximization-step-1">Maximization Step:</h4><p>We update the parameters after each observation by maximizing the log-likelihood of the observed data up to that point subject to the same constraint as in the batch mode:</p>\[\begin{aligned} \pi_{j}^{(t)} &amp;= \pi_{j}^{(t-1)} + \alpha_j \gamma_{n,j} \\ \mu_{j}^{(t)} &amp;= \mu_{j}^{(t-1)} + \beta_j \gamma _{n,j} (x_{n} - \mu_{j}^{(t-1)}) \end{aligned}\]<p>where $t$ is the time step, $\alpha_j$ and $\beta_j$ are the learning rates for the mixture weight and the mean parameter respectively.</p><p>We repeat the expectation and maximization steps after each observation is received.</p><h4 id="interpretation-1">Interpretation:</h4><p>The EM algorithm in online mode provides the same information as in batch mode. However, it is more useful in applications where online learning is critical. For example, in a stock trading application where stock prices change frequently and rapidly, online change point detection using the EM algorithm can help predict future trends and alter trading strategies on the fly.</p><h2 id="python-implementation">Python Implementation</h2><p>We present a Python implementation of the EM algorithm for change point detection in time series data. The implementation is optimized for the batch mode of the algorithm.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">em_cp_detection</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
    <span class="s">"""
    Expectation Maximization algorithm for change point detection
    Args:
        X: 1D numpy array of observations
        K: int, number of segments to fit
        max_iter: int, maximum number of EM iterations
        tol: float, tolerance for convergence
    Returns:
        tuple of numpy arrays:
            * Z: 1D np array of segment assignments (length N)
            * mu: np array of segment means (length K)
            * pi: np array of segment probabilities (length K)
    """</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span> <span class="o">/</span> <span class="n">K</span>
    <span class="n">sig_sq</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># assume constant variance for all segments
</span>    <span class="n">ll_old</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="c1"># E-step
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pi</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mu</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">sig_sq</span><span class="p">))</span>
            <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">likelihood</span><span class="p">)</span>
        <span class="c1"># M-step
</span>        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">Z</span> <span class="o">==</span> <span class="n">j</span>
            <span class="n">n_j</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
            <span class="n">pi</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">n_j</span> <span class="o">/</span> <span class="n">n</span>
            <span class="k">if</span> <span class="n">n_j</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">mu</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">mask</span><span class="p">])</span> <span class="o">/</span> <span class="n">n_j</span>
        <span class="c1"># Compute log-likelihood
</span>        <span class="n">ll_new</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">ll_new</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">pi</span> <span class="o">*</span> <span class="n">norm</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">mu</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">sig_sq</span><span class="p">))))</span>
        <span class="c1"># Check for convergence
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">ll_new</span> <span class="o">-</span> <span class="n">ll_old</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">ll_old</span> <span class="o">=</span> <span class="n">ll_new</span>
    <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">pi</span>
</code></pre></div></div><h2 id="conclusion">Conclusion</h2><p>The Expectation Maximization algorithm is a powerful tool for finding maximum likelihood estimates of parameters in statistical models with hidden variables. Its applicability in batch and online mode makes it useful in many applications, including change point detection in time series data.</p><p>In this article, we have presented the EM algorithm for change point detection in time series data. We have explained the algorithm in both batch and online mode, and provided a Python implementation for the batch mode. We hope that this article will serve as a useful reference for those interested in implementing the EM algorithm for change point detection.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
