<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Recursive Algorithms for Trailing Stop Stochastic Approximation Approach" /><meta name="author" content="Pavan Donthireddy" /><meta property="og:locale" content="en_US" /><meta name="description" content="Recursive Algorithms for Trailing Stop Stochastic Approximation Approach" /><meta property="og:description" content="Recursive Algorithms for Trailing Stop Stochastic Approximation Approach" /><link rel="canonical" href="http://localhost:4000/2023/04/19/recursive-algorithms-for-trailing-stop-stochastic-approximation-approach" /><meta property="og:url" content="http://localhost:4000/2023/04/19/recursive-algorithms-for-trailing-stop-stochastic-approximation-approach" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-04-19T00:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Recursive Algorithms for Trailing Stop Stochastic Approximation Approach" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Pavan Donthireddy" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Pavan Donthireddy"},"dateModified":"2023-04-19T00:00:00+01:00","datePublished":"2023-04-19T00:00:00+01:00","description":"Recursive Algorithms for Trailing Stop Stochastic Approximation Approach","headline":"Recursive Algorithms for Trailing Stop Stochastic Approximation Approach","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/04/19/recursive-algorithms-for-trailing-stop-stochastic-approximation-approach"},"url":"http://localhost:4000/2023/04/19/recursive-algorithms-for-trailing-stop-stochastic-approximation-approach"}</script><title> Recursive Algorithms for Trailing Stop Stochastic Approximation Approach - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/04/19/recursive-algorithms-for-trailing-stop-stochastic-approximation-approach" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Abstract"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Abstract"/><meta property="og:title" content="Recursive Algorithms for Trailing Stop Stochastic Approximation Approach"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-04-19T00:00:00+01:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/04/19/recursive-algorithms-for-trailing-stop-stochastic-approximation-approach" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Recursive Algorithms for Trailing Stop Stochastic Approximation Approach</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-04-19T00:00:00+01:00">April 19, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/stochastic_optimization">stochastic_optimization</a>, <a href="/tag/trading">trading</a>, <a href="/tag/execution">execution</a></span></div><h2 id="abstract">Abstract</h2><p align="justify">Trailing stops are often used in stock trading to limit the maximum of a possible loss and to lock in a profit. This work develops stochastic approximation algorithms to estimate the optimal trailing stop percentage. A stochastic optimization approach is proposed to recursively estimate the desired trailing stop percentage. A modification using projection is developed to ensure that the approximation sequence constructed stays in a reasonable range. Convergence of the algorithm is obtained. Moreover, interval estimates are constructed. Simulation examples are presented to compare our algorithm with Monte Carlo methods. Finally, we use real market data to demonstrate the algorithms.</p><h2 id="formulation">Formulation</h2><p>In our formulation, we shall not require the stock price $S(t)$ be any specific stochastic process or follow any specific distributions; we only assume the stock price being observable. Based on the observed stock price, for a given time $t$, we define the stop price at a trailing stop percentage $h$ with $0&lt;h&lt;1$ as \(T_h(t)=(1-h) S_{\max }(t)\) where \(S_{\max }(t)=\max \{S(u): 0 \leq u \leq t\}\) Let \(\tau=\inf \left\{t&gt;0: S(t) \leq T_h(t)\right\}\) Then $\tau$ is the first time the stock price reaches the stop price. We aim to find the optimal trailing stop percentage $h_* \in[a, 1]$ with $a&gt;0$ that maximize a suitable objective function. Thus the problem is $(\mathcal{P})$ Find $\operatorname{argmax} J(h)=E[\Phi(S(\tau)) \exp (-\rho \tau)], \quad h \in[a, 1]$ Here $\rho&gt;0$ is an appropriate discount rate and the reward function is \(\Phi(S)=\frac{S-S_0}{S_0}\) In practice, a trailing stop $h$ has to be greater than 0 because $h=0$ means selling right after bought the stock. In view of this, the lower bound $a$ is assumed to be greater than 0 , which represents a reasonable lower bound for the trailing stop percentage. In general, an analytic solution is difficult to obtain even if $S(t)$ is a specific process, e.g., a geometric Brownian motion. This is mainly due to the pathdependence nature of the problem. Our contribution is to devise a numerical approximation procedure that estimates the optimal trailing stop percentage $h$. We will use a stochastic approximation procedure to resolve the problem by constructing a sequence of estimates of the optimal trailing stop percentage $h$, using \(\left.h_{n+1}=h_n+\{\text { stepsize }\} \text { gradient estimate of } J(h)\right\} .\) Moreover, in accordance with (4), we need to make sure the iterate $h_n \in[a, 1]$.</p><h3 id="recursive-algorithm">Recursive Algorithm</h3><p>Let us begin with a simple noisy finite difference scheme. The only provision is that $S(t)$ can be observed. Associated with the iteration number $n$, denote the trailing stop percentage by $h_n$. Beginning at an arbitrary initial guess, we construct a sequence of estimates $\left{h_n\right}$ recursively as follows. We figure out $\tau_n$, the first time when the stock price declines under the stop price as \(\tau_n=\inf \left\{t&gt;0: S(t) \leq T_{h_n}(t)\right\} .\)</p><p>Define a combined process $\xi_n$ that includes the random effect from $S(t)$ and the stopping time $\tau_n$ as \(\xi_n=\left(S\left(\tau_n\right), \tau_n\right)^{\prime}\) where $S\left(\tau_n\right)$ denotes the stock price process $S(t)$ stopped at stopping time $\tau_n$. Henceforth, we call $\left{\xi_n\right}$ the sequence of collective noise. Let $\hat{J}(h, \xi)$ be the observed value of $J(h)$ with collective noise $\xi$. With the values $h \pm \delta_n$, define $Y_n^{ \pm}$as \(Y_n^{ \pm}\left(h, \xi_n^{ \pm}\right)=\hat{J}\left(h \pm \delta_n, \xi_n^{ \pm}\right) .\) $\xi_n^{ \pm}$being the two different collective noises taken at the trailing stop percentages $h \pm$ $\delta_n$, where $\delta_n$ is the finite difference sequence satisfying $\delta_n \rightarrow 0$ as $n \rightarrow \infty$. We shall write $Y_n^{ \pm}=Y_n^{ \pm}\left(h, \xi_n^{ \pm}\right)$. For simplicity, in what follows, we often use $\xi_n$ to represent both $\xi_n^{+}$and $\xi_n^{-}$if there is no confusion. The gradient estimate at iteration $n$ is given by \(D \hat{J}\left(h_n, \xi_n\right) \stackrel{\text { def }}{=}\left(Y_n^{+}-Y_n^{-}\right) /\left(2 \delta_n\right)\) Then the recursive algorithm is \(h_{n+1}=h_n+\varepsilon_n D \hat{J}\left(h_n, \xi_n\right),\) where $\varepsilon_n$ is a sequence of real numbers known as stepsizes. A frequently used choice of step size and finite difference sequences is $\varepsilon_n=O(1 / n)$ and $\delta_n=O\left(1 / n^{1 / 6}\right)$. Throughout this paper, this is our default choice of stepsize and finite difference sequences. To proceed, define \(\begin{aligned} &amp; \rho_n=\left(Y_n^{+}-Y_n^{-}\right)-E_n\left(Y_n^{+}-Y_n^{-}\right), \\ &amp; \eta_n=\left[E_n Y_n^{+}-J\left(h_n+\delta_n\right)\right]-\left[E_n Y_n^{-}-J\left(h_n-\delta_n\right)\right], \\ &amp; \beta_n=\frac{J\left(h_n+\delta_n\right)-J\left(h_n-\delta_n\right)}{2 \delta_n}-J_h\left(h_n\right), \end{aligned}\) where $E_n$ denotes the conditional expectation with respect to $\mathcal{F}_n$, the $\sigma$-algebra generated by $\left{h_j, \xi_j^{ \pm}: j&lt;n\right}, J_h\left(h_n\right)=(\partial / \partial h) J\left(h_n\right)$. In the above, $\eta_n$ and $\beta_n$ represent the noise and bias, and $\left{\rho_n\right}$ is a martingale difference sequence. We separate the noise into two parts, uncorrelated noise $\rho_n$ and correlated noise $\eta_n$. It is reasonable to assume that after taking the conditional expectations, the resulting function is smooth. With the above definitions, algorithm (9) can be rewritten as \(h_{n+1}=h_n+\varepsilon_n J_h\left(h_n\right)+\varepsilon_n \frac{\rho_n}{2 \delta_n}+\varepsilon_n \beta_n+\varepsilon_n \frac{\eta_n\left(h_n, \xi_n\right)}{2 \delta_n} .\)</p><h3 id="projection-algorithms">Projection Algorithms</h3><p>The use of projections in the algorithms stems from two reasons. First, for the purpose of computations, it is more convenient if one uses projections to force the iterates to remain in a bounded region. In addition, the problems under consideration may well be constrained so that the iterates will be in a given set. Current problem under consideration is such an example (the iterates need to stay in the interval $[a, 1]$ ). For example, one might choose a lowest trailing stop percentage of $10 \%$ to ensure the holding position will not be closed due to the normal fluctuations of daily stock price. Obviously, there is a upper bound for the optimal trailing stop percentage, $100 \%$. To solve the problem (4) with constrains, we construct the following stochastic approximation algorithm with a projection \(h_{n+1}=\Pi\left[h_n+\varepsilon_n D \hat{J}\left(h_n, \xi_n\right)\right]\) where $\varepsilon_n=1 / n, \delta_n=\delta /\left(n^{1 / 6}\right)$ and $\Pi[x]$ is a projection given by \(\Pi[h]= \begin{cases}a, &amp; \text { if } h&lt;a, \\ 1, &amp; \text { if } h&gt;1, \\ h, &amp; \text { otherwise. }\end{cases}\) As in Kushner and Yin [4], the projection algorithm (11) can be rewritten as \(h_{n+1}=h_n+\varepsilon_n D \hat{J}\left(h_n, \xi_n\right)+\varepsilon_n r_n,\) where $\varepsilon_n r_n=h_{n+1}-h_n-\varepsilon_n D \hat{J}\left(h_n, \xi_n\right)$ is the real number with the shortest distance needed to bring $h_n+\varepsilon_n D \hat{J}\left(h_n, \xi_n\right)$ back $[a, 1]$ if it is outside this set.</p><h2 id="interval-estimates">Interval Estimates</h2><p>This section is devoted to obtaining interval estimates as well as a piratically useful stopping rule for the recursive computation. Roughly, with prescribed confidence level, we wish to show that with large probability (probability close to 1 ), a sequence of scaled and centered estimates and a stopped sequence converge weakly to a diffusion process. Based on this result, we will then be able to build confidence interval for the iterates.</p><p>To proceed, for simplicity of notation, we take $\varepsilon_n=1 / n$ and $\delta_n=\delta_0 / n^{1 / 6}$. In the analysis to follow, for simplicity and without loss of generality, we take $\delta_0=1$. We assume all the conditions of Theorem 3.1 holds. To carry out the subsequent study, we also assume an additional condition. (A3) $J_h(h)=J_{h h}\left(h_<em>\right)\left(h-h_</em>\right)+o\left(\left|h-h_<em>\right|^2\right)$, where $J_{h h}\left(h_</em>\right)-(1 / 2)&lt;0$. In addition, $k^{2 / 3} E\left(h_k-h_*\right)^2=O(1)$ and the bound holds uniformly in $k$.</p><p>Remark 4.1 The first condition in (A3) indicates that $J_h(h)$ is linearizable. The second condition is a moment estimate. Sufficient conditions guaranteeing this can be provided by means of perturbed Lyapunov function methods; see for example [7] for liquidation related issues and the more extensive discussion in [4] for general setting. For simplicity, here we assume this condition. Define \(\rho_n^*=\left[Y\left(h_*, \xi_n^{+}\right)-Y\left(h_*, \xi_n^{-}\right)\right]-E_n\left[Y\left(h_*, \xi_n^{+}\right)-Y\left(h_*, \xi_n^{-}\right)\right] .\) That is, $\rho_n^<em>$ is $\rho_n$ with the argument $h_n$ replaced by $h_</em>$. The detailed development of the interval estimates can be outlined as follows. Suppose that we can show that $n^{1 / 3}\left(h_n-h_<em>\right)$ is asymptotically normal with mean zero and asymptotic variance $\sigma^2$. Choose $\alpha$, such that $0&lt;\alpha&lt;1$ and $1-\alpha$ is the desired confidence coefficient. Given $\varepsilon&gt;0$, then the asymptotic normality implies that \(P\left(\frac{n^{1 / 3}\left|h_n-h_*\right|}{\sigma} \leq z_{\alpha / 2}\right) \rightarrow 1-\alpha, \text { as } n \rightarrow \infty .\) This will lead to the desired confidence interval estimator. Then we require the length of the interval $\left|h_n-h_</em>\right|$ be small enough in that for any $\varepsilon&gt;0$, for sufficiently large</p><p>$n$, we can make $\sigma z_{\alpha / 2} / n^{1 / 3}&lt;\varepsilon$ or equivalently $n&gt;\left\lfloor\sigma z_{\alpha / 2} / \varepsilon\right\rfloor$. Define \(M_{\varepsilon, \alpha}^n=\left\lfloor\frac{\sigma z_{\alpha / 2}}{\varepsilon}\right\rfloor, \quad \mu_{\varepsilon, \alpha}=\inf \left\{n: M_{\varepsilon, \alpha}^n \leq n\right\},\) where $\lfloor z\rfloor$ denotes the greatest integer that is less than or equal to $z$. Then $\mu_{\varepsilon, \alpha}$ is a stopping rule for the iterating sequence $\left{h_n\right}$. Denote \(I_{\mu_{\varepsilon, \alpha}}=\left[h_{\mu_{\varepsilon, \alpha}}-\sigma \frac{z_{\alpha / 2}}{n^{2 / 3}}, h_{\mu_{\varepsilon, \alpha}}+\sigma \frac{z_{\alpha / 2}}{n^{2 / 3}}\right] .\) We shall show that as the length of the interval shrinks, i.e., $\varepsilon \rightarrow 0$, \(P\left\{h \in I_{\mu_{\varepsilon, \alpha}} \text { and }\left|I_{\mu_{\varepsilon, \alpha}}\right| \leq \varepsilon\right\} \rightarrow 1-\alpha,\) where $\left|I_{\mu_{\varepsilon, \alpha}}\right|$ denotes the length of the interval $I_{\mu_{\varepsilon, \alpha}}$.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
