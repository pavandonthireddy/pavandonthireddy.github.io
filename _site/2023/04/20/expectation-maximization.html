<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Expectation Maximization: A Powerful Tool for Trading Analysis" /><meta name="author" content="Pavan Donthireddy" /><meta property="og:locale" content="en_US" /><meta name="description" content="Explore how the Expectation Maximization algorithm can be applied to financial trading analysis in both batch and online mode. A comprehensive mathematical article with Python implementation." /><meta property="og:description" content="Explore how the Expectation Maximization algorithm can be applied to financial trading analysis in both batch and online mode. A comprehensive mathematical article with Python implementation." /><link rel="canonical" href="http://localhost:4000/2023/04/20/expectation-maximization" /><meta property="og:url" content="http://localhost:4000/2023/04/20/expectation-maximization" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-04-20T00:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Expectation Maximization: A Powerful Tool for Trading Analysis" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Pavan Donthireddy" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Pavan Donthireddy"},"dateModified":"2023-04-20T00:00:00+01:00","datePublished":"2023-04-20T00:00:00+01:00","description":"Explore how the Expectation Maximization algorithm can be applied to financial trading analysis in both batch and online mode. A comprehensive mathematical article with Python implementation.","headline":"Expectation Maximization: A Powerful Tool for Trading Analysis","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/04/20/expectation-maximization"},"url":"http://localhost:4000/2023/04/20/expectation-maximization"}</script><title> Expectation Maximization: A Powerful Tool for Trading Analysis - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/04/20/expectation-maximization" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="IntroductionExpectation Maximization, or EM, is a powerful algorithm for solving a wide range of statistical problems, particularly in the field of machine l..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="IntroductionExpectation Maximization, or EM, is a powerful algorithm for solving a wide range of statistical problems, particularly in the field of machine l..."/><meta property="og:title" content="Expectation Maximization: A Powerful Tool for Trading Analysis"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-04-20T00:00:00+01:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/04/20/expectation-maximization" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style><link rel="stylesheet" href="/_includes/prism.css"> <script src="/assets/js/prism.js"></script></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"> <script src="/assets/js/prism.js"></script><section class="post"> <script src="/assets/js/prism.js"></script><h1 style="text-align: center;">Expectation Maximization: A Powerful Tool for Trading Analysis</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-04-20T00:00:00+01:00">April 20, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/Expectation_Maximization">Expectation_Maximization</a></span></div><h2 id="introduction">Introduction</h2><p>Expectation Maximization, or EM, is a powerful algorithm for solving a wide range of statistical problems, particularly in the field of machine learning. The algorithm estimates the parameters of a model, assuming that some of the data is missing or unobserved. EM is an iterative algorithm with two steps: the expectation step, which computes the expected values of the unobserved data or missing values, given the observed data and the estimated parameters, and the maximization step, which maximizes the likelihood of the observed data while updating the parameter estimates. EM can be applied in both batch and online modes, depending on how the data is processed. This article explains the theoretical background and algorithmic approach of EM and demonstrates its potential use cases in financial trading analysis, highlighting both modes.</p><h2 id="theoretical-background">Theoretical background</h2><p>The EM algorithm is based on the maximum likelihood estimation principle, which is used to estimate the parameters of a statistical model that maximizes the likelihood function. The likelihood function calculates the probability of the observed data given the parameters of the model. However, in some cases, part of the data may be unobserved or missing, which makes it impossible to maximize the likelihood function directly. The EM algorithm provides a way to find the maximum likelihood estimates of the parameters in such cases.</p><p>The EM algorithm solves this problem by introducing a latent or unobserved variable, which represents the missing data. The algorithm then iteratively estimates the parameters of the model by computing the expected values of the missing data given the observed data and the current estimate of the parameters, and then computes the maximum likelihood estimate of the parameters using the expected values. This process repeats until convergence is achieved, or a stopping criterion is satisfied.</p><p>EM has several advantages over other parameter estimation algorithms, such as gradient descent or Newton-Raphson, particularly in cases where the likelihood function is not easily differentiable. Furthermore, EM provides a way to estimate the parameters of models with incomplete data or models that depend on unobserved variables.</p><h2 id="the-algorithm">The Algorithm</h2><p>The EM algorithm can be described in two steps: the expectation step (E-step) and the maximization step (M-step).</p><ol><li>E-step: Given the observed data $X$ and the current parameter estimates $\theta^{(t)}$, we compute the expected value of the unobserved or missing data $Z$ using the conditional probability distribution $p(Z|X,\theta^{(t)})$. This distribution is called the posterior distribution of $Z$ given $X$ and $\theta^{(t)}$, and it tells us the probability of the missing data given the observed data and the current parameter estimates.</li></ol>\[\gamma(z_i)=p(z_i|x_i,\theta^{(t)})=\frac{p(x_i|z_i,\theta^{(t)})p(z_i|\theta^{(t)})}{\sum_{z_i}p(x_i|z_i,\theta^{(t)})p(z_i|\theta^{(t)})}\]<p>where $z_i$ is the missing data for the $i^{th}$ observation $x_i$, and $\gamma(z_i)$ is the probability of $z_i$ given $x_i$ and the current parameter estimates $\theta^{(t)}$. The denominator in the equation normalizes the probabilities over all possible values of $z_i$.</p><ol><li>M-step: In the M-step, we compute the maximum likelihood estimate of the parameters $\theta$ given the observed data and the expected values of the missing data obtained in the E-step.</li></ol>\[\theta^{(t+1)}=\arg\max_{\theta}\sum_{i=1}^N\sum_{z_i}\gamma(z_i)\log p(x_i,z_i|\theta)\]<p>where $N$ is the number of observations, and $\theta^{(t+1)}$ is the updated estimate of the parameters. The expression inside the logarithm is the joint probability of the observed data $x_i$ and its missing data $z_i$ given the parameter estimates $\theta$.</p><p>The EM algorithm iterates between the E-step and the M-step until convergence is achieved. The convergence criterion can be based on the change in the log-likelihood of the observed data or on the change in the parameter estimates.</p><h2 id="potential-use-cases-in-trading-analysis">Potential use cases in trading analysis</h2><p>EM has several potential use cases in financial trading analysis, particularly in cases where the data is incomplete, missing, or noisy. EM can be applied in both batch and online modes depending on the trading strategy and data formats.</p><h3 id="batch-mode">Batch mode</h3><p>In the batch mode, the algorithm processes a fixed set of historical data all at once. The algorithm can be used for various purposes including:</p><h4 id="portfolio-optimization">Portfolio optimization</h4><p>EM can be used for portfolio optimization by estimating the asset returns and covariances from historical data. The estimated returns and covariances can then be used to construct an optimal portfolio for a given set of constraints such as risk tolerance and expected return.</p><p>We can estimate the expected asset returns by assuming a normal distribution and using the EM algorithm to estimate the mean and variance of the distribution. We can also estimate the covariance matrix of the asset returns by modeling the returns as a multivariate normal distribution and using the EM algorithm to estimate the covariance matrix.</p><p>The Python code for mean and variance estimation is shown below:</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Batch EM for mean and variance estimation
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate mock data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Initialize mean and variance
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Run EM algorithm for mean and variance estimation
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># E-step
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">data</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">variance</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variance</span><span class="p">)))</span>
    <span class="c1"># Normalize gamma
</span>    <span class="n">gamma</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># M-step
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">data</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="s">'Iteration:'</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">'Mean:'</span><span class="p">,</span> <span class="n">mean</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="s">'Variance:'</span><span class="p">,</span> <span class="n">variance</span><span class="p">)</span>

</code></pre></div></div><h4 id="hidden-markov-models">Hidden Markov models</h4><p>EM can be used for modeling and predicting financial time series data with a Hidden Markov Model (HMM). A HMM is a statistical model that describes a sequence of observations, where each observation is generated from one of several underlying hidden states with some probability. HMMs can be used to model the hidden states of financial markets or assets and to predict future market conditions.</p><p>We can use the EM algorithm to train the HMM parameters by estimating the transition probabilities between the hidden states and the emission probabilities for each observation. The transition probabilities represent the probabilities of moving from one hidden state to another, and the emission probabilities represent the probabilities of observing an observation given a hidden state.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Batch EM for HMM parameter estimation
</span><span class="kn">from</span> <span class="n">hmmlearn</span> <span class="kn">import</span> <span class="n">hmm</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate mock data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Initialize HMM with 2 hidden states
</span><span class="n">model</span> <span class="o">=</span> <span class="n">hmm</span><span class="p">.</span><span class="nc">GaussianHMM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Train HMM with EM algorithm
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Print estimated parameters
</span><span class="nf">print</span><span class="p">(</span><span class="s">'Transition probabilities:'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">transmat_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'Means:'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">means_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="s">'Covariances:'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">covars_</span><span class="p">)</span>
</code></pre></div></div><h3 id="online-mode">Online mode</h3><p>In the online mode, the algorithm processes data point by point or in small batches. The algorithm can be used for various purposes including:</p><h4 id="anomaly-detection">Anomaly detection</h4><p>EM can be used for detecting anomalies in financial data such as fraud detection or detecting unusual market conditions. The algorithm can be used to estimate the normal or expected behavior of the data and identify data points that deviate significantly from the expected behavior.</p><p>We can use the EM algorithm to estimate the parameters of a normal distribution from the historical data and then calculate the probability of a new data point belonging to the same normal distribution. If the probability is lower than a threshold, the data point is considered an anomaly.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Online EM for anomaly detection
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate mock data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Initialize mean and variance
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Run EM algorithm for mean and variance estimation
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># E-step
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">variance</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variance</span><span class="p">)))</span>
    <span class="c1"># Normalize gamma
</span>    <span class="n">gamma</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># M-step
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="c1"># Anomaly detection
</span>    <span class="n">prob</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">variance</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variance</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">prob</span> <span class="o">&lt;</span> <span class="mf">0.001</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'Anomaly detected at index:'</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

</code></pre></div></div><h4 id="short-term-trading">Short-term trading</h4><p>EM can be used for short-term trading strategies by predicting the direction of the market or asset price movements. The algorithm can be used to estimate the parameters of a distribution from the historical data and then predict the next data point using the estimated parameters.</p><p>We can use the EM algorithm to estimate the mean and variance of a normal distribution from the historical data and then predict the next data point as the mean of the distribution.</p><div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Online EM for short-term trading
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate mock data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Initialize mean and variance
</span><span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Run EM algorithm for mean and variance estimation
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># E-step
</span>    <span class="n">gamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="n">variance</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">variance</span><span class="p">)))</span>
    <span class="c1"># Normalize gamma
</span>    <span class="n">gamma</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="c1"># M-step
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>
    <span class="c1"># Short-term trading
</span>    <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">mean</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'Buy'</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">'Sell'</span><span class="p">)</span>

</code></pre></div></div><h2 id="conclusion">Conclusion</h2><p>EM is a powerful algorithm for solving a wide range of statistical problems, particularly in machine learning. The algorithm estimates the parameters of the model, assuming that some of the data is missing or unobserved. EM can be applied in both batch and online modes and has several potential use cases in financial trading analysis, including portfolio optimization, hidden Markov models, anomaly detection, and short-term trading strategies. EM can provide reliable and accurate results for statistical problems in finance, which makes it an important tool for financial analysis and decision-making.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main></body></html>
