<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Expectation Maximization: A Powerful Tool for Trading Analysis" /><meta name="author" content="Pavan Donthireddy" /><meta property="og:locale" content="en_US" /><meta name="description" content="Explore how the Expectation Maximization algorithm can be applied to financial trading analysis in both batch and online mode. A comprehensive mathematical article with Python implementation." /><meta property="og:description" content="Explore how the Expectation Maximization algorithm can be applied to financial trading analysis in both batch and online mode. A comprehensive mathematical article with Python implementation." /><link rel="canonical" href="http://localhost:4000/2023/04/20/expectation-maximization" /><meta property="og:url" content="http://localhost:4000/2023/04/20/expectation-maximization" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-04-20T00:00:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Expectation Maximization: A Powerful Tool for Trading Analysis" /><meta name="twitter:site" content="@" /><meta name="twitter:creator" content="@Pavan Donthireddy" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Pavan Donthireddy"},"dateModified":"2023-04-20T00:00:00+01:00","datePublished":"2023-04-20T00:00:00+01:00","description":"Explore how the Expectation Maximization algorithm can be applied to financial trading analysis in both batch and online mode. A comprehensive mathematical article with Python implementation.","headline":"Expectation Maximization: A Powerful Tool for Trading Analysis","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/04/20/expectation-maximization"},"url":"http://localhost:4000/2023/04/20/expectation-maximization"}</script><title> Expectation Maximization: A Powerful Tool for Trading Analysis - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/04/20/expectation-maximization" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="IntroductionExpectation Maximization, or EM, is a powerful algorithm for solving a wide range of statistical problems, particularly in the field of machine l..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="IntroductionExpectation Maximization, or EM, is a powerful algorithm for solving a wide range of statistical problems, particularly in the field of machine l..."/><meta property="og:title" content="Expectation Maximization: A Powerful Tool for Trading Analysis"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-04-20T00:00:00+01:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/04/20/expectation-maximization" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <!--KaTeX--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.6/dist/katex.min.css" integrity="sha384-mXD7x5S50Ko38scHSnD4egvoExgMPbrseZorkbE49evAfv9nNcbrXJ8LLNsDgh9d" crossorigin="anonymous"> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.6/dist/katex.min.js" integrity="sha384-j/ZricySXBnNMJy9meJCtyXTKMhIJ42heyr7oAdxTDBy/CYA9hzpMo+YTNV5C+1X" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.6/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><section class="post"><h1 style="text-align: center;">Expectation Maximization: A Powerful Tool for Trading Analysis</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-04-20T00:00:00+01:00">April 20, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/Expectation_Maximization">Expectation_Maximization</a></span></div><h2 id="introduction">Introduction</h2><p>Expectation Maximization, or EM, is a powerful algorithm for solving a wide range of statistical problems, particularly in the field of machine learning. The algorithm estimates the parameters of a model, assuming that some of the data is missing or unobserved. EM is an iterative algorithm with two steps: the expectation step, which computes the expected values of the unobserved data or missing values, given the observed data and the estimated parameters, and the maximization step, which maximizes the likelihood of the observed data while updating the parameter estimates. EM can be applied in both batch and online modes, depending on how the data is processed. This article explains the theoretical background and algorithmic approach of EM and demonstrates its potential use cases in financial trading analysis, highlighting both modes.</p><h2 id="theoretical-background">Theoretical background</h2><p>The EM algorithm is based on the maximum likelihood estimation principle, which is used to estimate the parameters of a statistical model that maximizes the likelihood function. The likelihood function calculates the probability of the observed data given the parameters of the model. However, in some cases, part of the data may be unobserved or missing, which makes it impossible to maximize the likelihood function directly. The EM algorithm provides a way to find the maximum likelihood estimates of the parameters in such cases.</p><p>The EM algorithm solves this problem by introducing a latent or unobserved variable, which represents the missing data. The algorithm then iteratively estimates the parameters of the model by computing the expected values of the missing data given the observed data and the current estimate of the parameters, and then computes the maximum likelihood estimate of the parameters using the expected values. This process repeats until convergence is achieved, or a stopping criterion is satisfied.</p><p>EM has several advantages over other parameter estimation algorithms, such as gradient descent or Newton-Raphson, particularly in cases where the likelihood function is not easily differentiable. Furthermore, EM provides a way to estimate the parameters of models with incomplete data or models that depend on unobserved variables.</p><h2 id="the-algorithm">The Algorithm</h2><p>The EM algorithm can be described in two steps: the expectation step (E-step) and the maximization step (M-step).</p><ol><li>E-step: Given the observed data $X$ and the current parameter estimates $\theta^{(t)}$, we compute the expected value of the unobserved or missing data $Z$ using the conditional probability distribution $p(Z|X,\theta^{(t)})$. This distribution is called the posterior distribution of $Z$ given $X$ and $\theta^{(t)}$, and it tells us the probability of the missing data given the observed data and the current parameter estimates.</li></ol><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>γ</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>z</mi><mi>i</mi></msub><mo separator="true">,</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow><mrow><munder><mo>∑</mo><msub><mi>z</mi><mi>i</mi></msub></munder><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>z</mi><mi>i</mi></msub><mo separator="true">,</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\gamma(z_i)=p(z_i|x_i,\theta^{(t)})=\frac{p(x_i|z_i,\theta^{(t)})p(z_i|\theta^{(t)})}{\sum_{z_i}p(x_i|z_i,\theta^{(t)})p(z_i|\theta^{(t)})}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.188em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.6688em;vertical-align:-1.1038em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.565em;"><span style="top:-2.296em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0017em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3998em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.814em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.814em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1038em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span><p>where $z_i$ is the missing data for the $i^{th}$ observation $x_i$, and $\gamma(z_i)$ is the probability of $z_i$ given $x_i$ and the current parameter estimates $\theta^{(t)}$. The denominator in the equation normalizes the probabilities over all possible values of $z_i$.</p><ol><li>M-step: In the M-step, we compute the maximum likelihood estimate of the parameters $\theta$ given the observed data and the expected values of the missing data obtained in the E-step.</li></ol><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>arg</mi><mo>⁡</mo><munder><mrow><mi>max</mi><mo>⁡</mo></mrow><mi>θ</mi></munder><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><munder><mo>∑</mo><msub><mi>z</mi><mi>i</mi></msub></munder><mi>γ</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>z</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta^{(t+1)}=\arg\max_{\theta}\sum_{i=1}^N\sum_{z_i}\gamma(z_i)\log p(x_i,z_i|\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.938em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.938em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.1784em;vertical-align:-1.3501em;"></span><span class="mop">ar<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.4306em;"><span style="top:-2.3479em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7521em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.9em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.044em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.3501em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></span><p>where $N$ is the number of observations, and $\theta^{(t+1)}$ is the updated estimate of the parameters. The expression inside the logarithm is the joint probability of the observed data $x_i$ and its missing data $z_i$ given the parameter estimates $\theta$.</p><p>The EM algorithm iterates between the E-step and the M-step until convergence is achieved. The convergence criterion can be based on the change in the log-likelihood of the observed data or on the change in the parameter estimates.</p><h2 id="potential-use-cases-in-trading-analysis">Potential use cases in trading analysis</h2><p>EM has several potential use cases in financial trading analysis, particularly in cases where the data is incomplete, missing, or noisy. EM can be applied in both batch and online modes depending on the trading strategy and data formats.</p><h3 id="batch-mode">Batch mode</h3><p>In the batch mode, the algorithm processes a fixed set of historical data all at once. The algorithm can be used for various purposes including:</p><h4 id="portfolio-optimization">Portfolio optimization</h4><p>EM can be used for portfolio optimization by estimating the asset returns and covariances from historical data. The estimated returns and covariances can then be used to construct an optimal portfolio for a given set of constraints such as risk tolerance and expected return.</p><p>We can estimate the expected asset returns by assuming a normal distribution and using the EM algorithm to estimate the mean and variance of the distribution. We can also estimate the covariance matrix of the asset returns by modeling the returns as a multivariate normal distribution and using the EM algorithm to estimate the covariance matrix.</p><p>The Python code for mean and variance estimation is shown below:</p><pre><code class="language-python"># Batch EM for mean and variance estimation
import numpy as np

# Generate mock data
np.random.seed(42)
data = np.random.normal(0, 1, size=(10000, 5))

# Initialize mean and variance
mean = np.mean(data, axis=0)
variance = np.var(data, axis=0)

# Run EM algorithm for mean and variance estimation
for i in range(10):
    # E-step
    gamma = np.exp(-0.5 * ((data - mean) ** 2 / variance + np.log(2 * np.pi * variance)))
    # Normalize gamma
    gamma /= np.sum(gamma, axis=0)
    # M-step
    mean = np.sum(gamma * data, axis=0) / np.sum(gamma, axis=0)
    variance = np.sum(gamma * (data - mean) ** 2, axis=0) / np.sum(gamma, axis=0)
    
    print('Iteration:', i+1)
    print('Mean:', mean)
    print('Variance:', variance)

</code></pre><h4 id="hidden-markov-models">Hidden Markov models</h4><p>EM can be used for modeling and predicting financial time series data with a Hidden Markov Model (HMM). A HMM is a statistical model that describes a sequence of observations, where each observation is generated from one of several underlying hidden states with some probability. HMMs can be used to model the hidden states of financial markets or assets and to predict future market conditions.</p><p>We can use the EM algorithm to train the HMM parameters by estimating the transition probabilities between the hidden states and the emission probabilities for each observation. The transition probabilities represent the probabilities of moving from one hidden state to another, and the emission probabilities represent the probabilities of observing an observation given a hidden state.</p><pre><code class="language-python"># Batch EM for HMM parameter estimation
from hmmlearn import hmm
import numpy as np

# Generate mock data
np.random.seed(42)
data = np.random.normal(0, 1, size=(1000, 1))

# Initialize HMM with 2 hidden states
model = hmm.GaussianHMM(n_components=2)

# Train HMM with EM algorithm
model.fit(data)

# Print estimated parameters
print('Transition probabilities:')
print(model.transmat_)
print('Means:')
print(model.means_)
print('Covariances:')
print(model.covars_)
</code></pre><h3 id="online-mode">Online mode</h3><p>In the online mode, the algorithm processes data point by point or in small batches. The algorithm can be used for various purposes including:</p><h4 id="anomaly-detection">Anomaly detection</h4><p>EM can be used for detecting anomalies in financial data such as fraud detection or detecting unusual market conditions. The algorithm can be used to estimate the normal or expected behavior of the data and identify data points that deviate significantly from the expected behavior.</p><p>We can use the EM algorithm to estimate the parameters of a normal distribution from the historical data and then calculate the probability of a new data point belonging to the same normal distribution. If the probability is lower than a threshold, the data point is considered an anomaly.</p><pre><code class="language-python"># Online EM for anomaly detection
import numpy as np

# Generate mock data
np.random.seed(42)
data = np.random.normal(0, 1, size=(10000, 1))

# Initialize mean and variance
mean = np.mean(data[:100], axis=0)
variance = np.var(data[:100], axis=0)

# Run EM algorithm for mean and variance estimation
for i in range(100, 10000):
    # E-step
    gamma = np.exp(-0.5 * ((data[i] - mean) ** 2 / variance + np.log(2 * np.pi * variance)))
    # Normalize gamma
    gamma /= np.sum(gamma, axis=0)
    # M-step
    mean = np.sum(gamma * data[i]) / np.sum(gamma)
    variance = np.sum(gamma * (data[i] - mean) ** 2) / np.sum(gamma)
    # Anomaly detection
    prob = np.exp(-0.5 * ((data[i] - mean) ** 2 / variance + np.log(2 * np.pi * variance)))
    if prob &lt; 0.001:
        print('Anomaly detected at index:', i)

</code></pre><h4 id="short-term-trading">Short-term trading</h4><p>EM can be used for short-term trading strategies by predicting the direction of the market or asset price movements. The algorithm can be used to estimate the parameters of a distribution from the historical data and then predict the next data point using the estimated parameters.</p><p>We can use the EM algorithm to estimate the mean and variance of a normal distribution from the historical data and then predict the next data point as the mean of the distribution.</p><pre><code class="language-python"># Online EM for short-term trading
import numpy as np

# Generate mock data
np.random.seed(42)
data = np.random.normal(0, 1, size=(1000, 1))

# Initialize mean and variance
mean = np.mean(data[:100], axis=0)
variance = np.var(data[:100], axis=0)

# Run EM algorithm for mean and variance estimation
for i in range(100, 1000):
    # E-step
    gamma = np.exp(-0.5 * ((data[i] - mean) ** 2 / variance + np.log(2 * np.pi * variance)))
    # Normalize gamma
    gamma /= np.sum(gamma, axis=0)
    # M-step
    mean = np.sum(gamma * data[i]) / np.sum(gamma)
    variance = np.sum(gamma * (data[i] - mean) ** 2) / np.sum(gamma)
    # Short-term trading
    if data[i] &gt; mean:
        print('Buy')
    else:
        print('Sell')

</code></pre><h2 id="conclusion">Conclusion</h2><p>EM is a powerful algorithm for solving a wide range of statistical problems, particularly in machine learning. The algorithm estimates the parameters of the model, assuming that some of the data is missing or unobserved. EM can be applied in both batch and online modes and has several potential use cases in financial trading analysis, including portfolio optimization, hidden Markov models, anomaly detection, and short-term trading strategies. EM can provide reliable and accurate results for statistical problems in finance, which makes it an important tool for financial analysis and decision-making.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main></body></html>
