<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Numerical differentiation" /><meta property="og:locale" content="en_US" /><meta name="description" content="Numerical differentiation" /><meta property="og:description" content="Numerical differentiation" /><link rel="canonical" href="http://localhost:4000/2023/02/06/numerical-differentiation" /><meta property="og:url" content="http://localhost:4000/2023/02/06/numerical-differentiation" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-06T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Numerical differentiation" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-06T00:00:00+00:00","datePublished":"2023-02-06T00:00:00+00:00","description":"Numerical differentiation","headline":"Numerical differentiation","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/06/numerical-differentiation"},"url":"http://localhost:4000/2023/02/06/numerical-differentiation"}</script><title> Numerical differentiation - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/06/numerical-differentiation" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Numerical differentiation is a problem to determine the derivative of a function from the values on an interval or some scattered points. It arises from many..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Numerical differentiation is a problem to determine the derivative of a function from the values on an interval or some scattered points. It arises from many..."/><meta property="og:title" content="Numerical differentiation"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-06T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/06/numerical-differentiation" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Numerical differentiation</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-06T00:00:00+00:00">February 6, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/numerical_differentiation">numerical_differentiation</a></span></div><p>Numerical differentiation is a problem to determine the derivative of a function from the values on an interval or some scattered points. It arises from many scientific researches and applications, e.g., the identification of the discontinuous points in an image process [1]; the problem of solving the Abel integral equation [2]; the problem of determining the peaks in chemical spectroscopy [3] and some inverse problems in mathematical physical equations [4]. The main difficulty is that it is an ill-posed problem, which means that small errors in the measurement of a function may lead to large errors in its computed derivatives [5,4]. A number of techniques have been developed for numerical differentiation [6-8,4,9]. One type of method is to transform the differentiation problem into an operator equation of the first kind. In fact, for given $g(t) \in H^1[0,1]$, to find $f=D g=g^{\prime}$ is equivalent to solve the Volterra integral equation of the first kind \(\left(K_1 f\right)(s)=\int_0^s f(t) \mathrm{d} t=g(s)-g(0), \quad s \in[0,1] .\) In this paper we will point out the disadvantage of operator $K_1$ and a new operator which is a modified form of $K_1$ will be presented. Since a singular system of the new operator can be obtained easily, it seems natural to use the TSVD method for this problem and good results may be expected. A convergence result, analogous to the literature [4], will be obtained by our method. Comparing with the Tikhonov regularization method used in [4], the regularization parameter can be obtained easily by TSVD method. Moreover, it is well known that the Tikhonov method has a finite saturation index, which means that it is impossible to improve the convergence rates of the regularization solution with increasing smoothness assumption of exact solutions. But for TSVD method this disadvantage will be overcome. Moreover, we will point out that our method calls for a discrete sine transform when the noisy values of the function to be differentiated at the nodes are given, so the method can be implemented easily and fast.</p><h1 id="numerical-differentiation">Numerical Differentiation</h1><p>Numerical differentiation is the process of approximating the derivative of a function using numerical methods. It is an important tool in scientific computing and is used in a variety of fields, including engineering, physics, and economics. There are several approaches to numerical differentiation, each with its own strengths and weaknesses.</p><h2 id="finite-difference-methods">Finite Difference Methods</h2><p>Finite difference methods are the most common approach to numerical differentiation. These methods approximate the derivative of a function by computing the difference quotient:</p>\[f'(x) \approx \frac{f(x+h) - f(x)}{h}\]<p>where $h$ is a small positive number. This is known as the forward difference formula. Other difference formulas include the backward difference formula:</p>\[f'(x) \approx \frac{f(x) - f(x-h)}{h}\]<p>and the central difference formula:</p>\[f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}\]<p>The choice of formula depends on the accuracy and smoothness of the function being approximated.</p><h2 id="lagrange-polynomials">Lagrange Polynomials</h2><p>Lagrange polynomials are another approach to numerical differentiation that involves fitting a polynomial to a set of data points. Given a set of $n+1$ points $(x_i, f(x_i))$, the Lagrange polynomial is given by:</p>\[p_n(x) = \sum_{i=0}^n f(x_i) \prod_{j\ne i} \frac{x-x_j}{x_i-x_j}\]<p>The derivative of the Lagrange polynomial can then be used to approximate the derivative of the original function.</p><h2 id="splines">Splines</h2><p>Splines are a method for approximating a function using piecewise polynomials. The polynomials are typically chosen to be cubic, and are required to be continuous at the points where they meet (known as knots). The second derivative of the polynomials is also required to be continuous, which ensures that the approximation is smooth. The derivatives of the splines can then be used to approximate the derivative of the original function.</p><h2 id="richardson-extrapolation">Richardson Extrapolation</h2><p>Richardson extrapolation is a technique for improving the accuracy of finite difference approximations. It involves using the difference quotient at several different step sizes $h$ to derive a more accurate approximation. The formula for Richardson extrapolation is:</p>\[f'(x) \approx \frac{4f_{h/2}(x) - f_h(x)}{3}\]<p>where $f_h(x)$ and $f_{h/2}(x)$ are the difference quotients at step sizes $h$ and $h/2$, respectively.</p><h2 id="automatic-differentiation">Automatic Differentiation</h2><p>Automatic differentiation is a method for computing derivatives numerically that is more accurate and efficient than finite difference methods. It uses the chain rule of differentiation to compute derivatives at each step of the computation, rather than approximating them numerically. This approach can be especially useful for functions with many variables or complex structures.</p><h2 id="conclusion">Conclusion</h2><p>Numerical differentiation is a powerful tool for approximating the derivatives of functions. Finite difference methods are the most common approach, but other techniques such as Lagrange polynomials, splines, Richardson extrapolation, and automatic differentiation can provide more accurate and efficient approximations in certain situations.</p><h1 id="solving-numerical-differentiation-in-the-presence-of-noise-with-integral-equations">Solving Numerical Differentiation in the Presence of Noise with Integral Equations</h1><p>When performing numerical differentiation on data that is affected by noise, it can be challenging to obtain accurate and reliable results. However, one possible approach to this problem is to use integral equations to formulate the problem and derive a solution that takes into account the noise in the data. In this article, we will explore how integral equations can be used for numerical differentiation in the presence of noise, and we will provide some examples and techniques to illustrate this approach.</p><h2 id="the-problem-of-numerical-differentiation-with-noisy-data">The Problem of Numerical Differentiation with Noisy Data</h2><p>The problem of numerical differentiation consists of estimating the derivative of a function based on a set of discrete data points. When the data is exact and noise-free, this problem can be solved using various numerical methods, such as finite differences, interpolation, or regression. However, in many practical situations, the data is affected by noise, which can cause errors and instability in the differentiation process.</p><p>To illustrate this problem, let’s consider an example of a noisy data set:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x = [-2.5, -2.0, -1.5, -1.0, -0.5, 0.0, 0.5, 1.0, 1.5, 2.0, 2.5] 
y = [-2.635, -1.771, -0.768, -0.316, 0.383, 0.108, 0.704, 0.857, 2.275, 2.975, 4.350]

</code></pre></div></div><p>This data set represents a noisy version of the function <code class="language-plaintext highlighter-rouge">f(x) = x^3 - 2x^2 - 3x + 2</code>, which has the derivative <code class="language-plaintext highlighter-rouge">f'(x) = 3x^2 - 4x - 3</code>. If we try to estimate the derivative using finite differences or other standard methods, we may obtain results that are inaccurate or unreliable due to the noise in the data.</p><h2 id="using-integral-equations-for-numerical-differentiation">Using Integral Equations for Numerical Differentiation</h2><p>One possible approach to solving the problem of numerical differentiation with noisy data is to use integral equations. Integral equations involve expressing the problem as an equation involving an unknown function that needs to be solved for, and then using a known relationship between the function and its derivative to derive an integral equation that can be solved numerically.</p><p>To illustrate this approach, let’s consider the following integral equation:</p>\[\int_{-h}^{h} K(x,t) f'(t) dt = \frac{f(x+h) - f(x-h)}{2h}\]<p>In this equation, <code class="language-plaintext highlighter-rouge">f'(t)</code> represents the derivative of the unknown function <code class="language-plaintext highlighter-rouge">f(x)</code>, <code class="language-plaintext highlighter-rouge">K(x,t)</code> is a kernel function that depends on <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">t</code>, and <code class="language-plaintext highlighter-rouge">h</code> is a small parameter that controls the width of the integral. The right-hand side of the equation represents a finite difference approximation to the derivative of <code class="language-plaintext highlighter-rouge">f(x)</code>.</p><p>To solve this integral equation, we can use a technique known as collocation, which involves discretizing the equation by choosing a set of points <code class="language-plaintext highlighter-rouge">x_i</code> at which the equation is evaluated. We can then approximate the unknown function <code class="language-plaintext highlighter-rouge">f(x)</code> using a set of basis functions <code class="language-plaintext highlighter-rouge">phi_i(x)</code>, and write the integral equation as a system of linear equations:</p>\[\sum_{j=1}^{N} K_{i,j} f'_j = D_i\]<p>In this equation, <code class="language-plaintext highlighter-rouge">N</code> is the number of collocation points, $K_{i,j} = \int_{-h}^{h} K(x_i,t)$</p><h1 id="numerical-differentiation-with-noisy-streaming-data-using-online-algorithms">Numerical Differentiation with Noisy Streaming Data using Online Algorithms</h1><p>Numerical differentiation is a fundamental problem in many fields of science and engineering, where we want to estimate the derivative of a function at a given point based on sampled data. However, when the data is noisy or when it arrives in a streaming fashion, this problem becomes more challenging. In this article, we will discuss how to use online algorithms to solve the problem of numerical differentiation with noisy streaming data.</p><h2 id="the-problem">The Problem</h2><p>Suppose we have a function f(x) that we want to differentiate at a point x0. We can use the following formula to approximate the derivative:</p>\[f'(x0) \approx \frac{(f(x0+h) - f(x0-h))}{(2h)}\]<p>where h is a small step size. However, if the data we have is noisy, this approximation may not be accurate. Moreover, if the data arrives in a streaming fashion, we cannot wait until all the data is available to compute the derivative.</p><h2 id="offline-approaches">Offline Approaches</h2><p>Before we discuss online algorithms, let’s briefly review some of the offline approaches to numerical differentiation with noisy data. One common approach is to use smoothing techniques, such as the Savitzky-Golay filter or the moving average filter, to remove the noise from the data before computing the derivative. Another approach is to use integral equations, such as the Tikhonov regularization or the least-squares method, to estimate the unknown function and its derivative from the noisy data.</p><h2 id="online-algorithms">Online Algorithms</h2><p>While the offline approaches discussed above can be effective for solving numerical differentiation problems with noisy data, they are not well-suited for streaming data. Online algorithms, on the other hand, are designed to continuously update their estimates based on new data as it arrives, and they can be more efficient and accurate in a streaming context.</p><h3 id="recursive-least-squares">Recursive Least Squares</h3><p>One possible online algorithm for numerical differentiation with noisy data is recursive least squares (RLS). RLS is an adaptive algorithm that can continuously update its estimates based on new data as it arrives, and it has been shown to be effective for solving similar problems in a streaming context.</p><p>To apply RLS to numerical differentiation with noisy data, we can first choose a set of basis functions and a kernel function as before, and then use RLS to estimate the unknown function and its derivative based on the incoming data. The algorithm would work by updating the estimate of the function and its derivative at each time step based on the new data, and using a forgetting factor to weight the importance of the old data versus the new data.</p><h3 id="extended-kalman-filter">Extended Kalman Filter</h3><p>Another possible online algorithm for numerical differentiation with noisy data is the extended Kalman filter (EKF). The EKF is a recursive algorithm that can estimate the state of a dynamic system based on noisy measurements, and it has been shown to be effective for solving similar problems in a streaming context.</p><p>To apply the EKF to numerical differentiation with noisy data, we can use the same integral equation and basis functions as before, and then use the EKF to estimate the unknown function and its derivative based on the incoming data. The algorithm would work by updating the estimate of the function and its derivative at each time step based on the new data, and using a Kalman gain to weight the importance of the measurement noise versus the process noise.</p><h2 id="conclusion-1">Conclusion</h2><p>In conclusion, while the specific details of how to apply online algorithms to numerical differentiation with noisy data may vary depending on the specific problem and algorithm, the general approach involves adapting the offline methods discussed above to the streaming context by using adaptive algorithms that can continuously update their estimates based on new data as it arrives. These online algorithms can be more efficient and accurate than their offline counterparts in a streaming context,</p><h1 id="complex-variable-methods-for-numerical-differentiation-with-noisy-data">Complex Variable Methods for Numerical Differentiation with Noisy Data</h1><p>Numerical differentiation is a fundamental problem in many fields of science and engineering, where we want to estimate the derivative of a function at a given point based on sampled data. However, when the data is noisy, this problem becomes more challenging. In this article, we will discuss how to use complex variable methods to solve the problem of numerical differentiation with noisy data.</p><h2 id="the-problem-1">The Problem</h2><p>Suppose we have a function $f(z)$ that we want to differentiate at a point $z_0$. We can use the following formula to approximate the derivative:</p>\[f'(z_0) \approx \frac{f(z_0 + h) - f(z_0 - h)}{2h}\]<p>where $h$ is a small step size. However, if the data we have is noisy, this approximation may not be accurate. Moreover, if the function $f(z)$ is complex-valued, the above formula may not be well-defined.</p><h2 id="complex-variable-methods">Complex Variable Methods</h2><p>Complex variable methods provide an elegant and powerful way to differentiate complex-valued functions, even in the presence of noise. The key idea is to use the Cauchy-Riemann equations to express the derivative of a complex-valued function in terms of its analytic continuation to the complex plane.</p><h3 id="cauchy-riemann-equations">Cauchy-Riemann Equations</h3><p>The Cauchy-Riemann equations are a set of conditions that a complex-valued function $f(z)$ must satisfy in order to be analytic in a region of the complex plane. The equations are:</p>\[\frac{\partial u}{\partial x} = \frac{\partial v}{\partial y} \quad \text{and} \quad \frac{\partial u}{\partial y} = -\frac{\partial v}{\partial x}\]<p>where $u(x,y)$ and $v(x,y)$ are the real and imaginary parts of $f(z) = u(x,y) + iv(x,y)$, respectively. If $f(z)$ is analytic, then it satisfies the Cauchy-Riemann equations, and we can use these equations to express the derivative of $f(z)$ in terms of its analytic continuation.</p><h3 id="wirtinger-calculus">Wirtinger Calculus</h3><p>The Wirtinger calculus provides a convenient way to express the Cauchy-Riemann equations in terms of partial derivatives with respect to $z$ and $z^<em>$, where $z^</em>$ denotes the complex conjugate of $z$. Using the Wirtinger calculus, we can write the Cauchy-Riemann equations as:</p>\[\frac{\partial f}{\partial z^*} = 0 \quad \text{and} \quad \frac{\partial f}{\partial z} = f'(z)\]<p>where $f’(z)$ is the derivative of $f(z)$ with respect to $z$. These equations show that the derivative of $f(z)$ can be expressed in terms of its partial derivatives with respect to $z$ and $z^*$.</p><h3 id="wirtinger-derivatives">Wirtinger Derivatives</h3><p>The Wirtinger derivatives are defined as:</p>\[\frac{\partial}{\partial z} = \frac{1}{2}\left(\frac{\partial}{\partial x} + i\frac{\partial}{\partial y}\right) \quad \text{and} \quad \frac{\partial}{\partial z^*} = \frac{1}{2}\left(\frac{\partial}{\partial x} - i\frac{\partial}{\partial y}\right)\]<hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
