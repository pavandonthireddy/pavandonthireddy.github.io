<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Time Series Classification" /><meta property="og:locale" content="en_US" /><meta name="description" content="Time Series Classification" /><meta property="og:description" content="Time Series Classification" /><link rel="canonical" href="http://localhost:4000/2023/02/04/time-series-classification" /><meta property="og:url" content="http://localhost:4000/2023/02/04/time-series-classification" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-04T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Time Series Classification" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-04T00:00:00+00:00","datePublished":"2023-02-04T00:00:00+00:00","description":"Time Series Classification","headline":"Time Series Classification","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/04/time-series-classification"},"url":"http://localhost:4000/2023/02/04/time-series-classification"}</script><title> Time Series Classification - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/04/time-series-classification" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Typical classification approaches can be categorized asinstance-based (e.g., one nearest neighbor classifier witheuclidean distance (NN-Euclidean), or dynami..."><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Typical classification approaches can be categorized asinstance-based (e.g., one nearest neighbor classifier witheuclidean distance (NN-Euclidean), or dynami..."/><meta property="og:title" content="Time Series Classification"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-04T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/04/time-series-classification" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><section class="post"><h1 style="text-align: center;">Time Series Classification</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-04T00:00:00+00:00">February 4, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/classification">classification</a>, <a href="/tag/timeseries">timeseries</a></span></div><p align="justify">Typical classification approaches can be categorized as instance-based (e.g., one nearest neighbor classifier with euclidean distance (NN-Euclidean), or dynamic time warping distance (NNDTW)), shapelet featurebased and local pattern-frequency histogram based methods Instance-based methods, like NNDTW, have been successfully used for TSC and shown to be very hard to beat but they are usually less interpretable. Shapelet is another promising method for TSC, and it discovers subsequences that are discriminative of class membership and provides more interpretable results, but searching for shapelets on large datasets becomes time-consuming or even intractable. Feature-based methods do show promising classification results, but their capabilities are largely attributed to strong classifiers like SVM, adaboost and random forest, instead of being due to better global/local features and representations.</p><p align="justify">Time series classification methods can be categorized into instance-based, shapelets, feature-based and pattern frequency histogram methods.</p><p align="justify">Instance-based methods predict labels of test time series based on their similarity to the training instances. The most popular similarity metrics include euclidean distance and elastic distances, e.g., the dynamic time warping (DTW) distance. Using a single nearest neighbor, with euclidean distance (NNEuclidean) or DTW distance (NNDTW), has demonstrated successful time series label prediction. DTW allows time series to be locally shifted, contracted and stretched, and lengths of time series hence need not be the same. Therefore, DTW usually gives a better similarity measurement than Euclidean distance, and NNDTW has been shown to be very hard to beat on many datasets. A number of more complex elastic distance measures have been proposed, including longest common subsequences (LCSS), Edit distance with Real Penalty (ERP) and edit distance on Real Sequence (EDR). However, in 30, the authors claimed that no other elastic distance measure outperforms DTW by a statistically significant amount, and DTW is the best measure. Instance-based approaches, like NN-euclidean and NNDTW, are accurate, but they are less interpretable, since they are based on global matching and provide limited insights into the temporal characteristics.</p><p align="justify">Shapelet is a localized time series subsequence, which is discriminative of class membership, and it was first proposed and used by Ye and Keogh [1] for time series classification. The original shapelet algorithm [1] searches for shapelets recursively, and builds a decision tree using different shapelets as splitting criteria. However, the expressiveness of shapelets is limited to binary decision questions. In [4], the authors proposed logical-shapelets, specifically conjunctions or disjunctions of shapelets, which are shown to be more expressive than a single shapelet, and toexperimentally outperform the original shapelet algorithm.</p><p align="justify">The above two algorithms embed shapelet discovery in a decision tree, while in [2], the authors separate shapelet discovery from classifier by finding the best k shapelets in a single scan of all time series. The shapelets are used to transform the data, where each attribute in the new dataset represents the distance of a time series to one of the k shapelets. Hills et al. demonstrate that the transformed data, in conjunction with more complex classifiers, produces better accuracies than the embedded shapelet tree. Since shapelets are localized class-discriminative subsequences, shapeletsbased methods have increased interpretability than global instance-based matching. The main drawback is the time complexity of searching for shapelets, and subsequent research, e.g., focuses on developing efficient shapeletsearching algorithms.</p><p align="justify">Feature-based methods generally consist of two sequential steps: extract features and train a classifier based on features. Typical global features include statistical features, like variance and mean, PCA coefficients, DFT coefficients, zerocrossing rate. These features are extracted either from time domain or from transformed domains, like frequency domain and principal component space. Afterwards, the extracted features either go through feature selection procedures to prune less significant ones [5], or are fed directly into complex classifiers, like multi-layer neural network [31]. Global features lose temporal information, although it is potentially informative for classification.</p><p align="justify">In [8], the authors extracted features from intervals of time series, constructed and then boosted binary stumps on these interval features, and trained an SVM on outputs of the boosted binary stumps. In [6], the authors extracted simple interval features as well, including mean, variance and slope, trained a random forest ensemble classifier, and showed better performance than NNDTW. Although feature-based methods have shown promising classification results, their capabilities are largely attributed to strong classifiers such as SVM, adaboost and random forest, instead of being due to better global/local features and representations.</p><p align="justify">Another popular method is based on pattern frequency histograms, widely known as bag of words. The BoW approach incorporates word frequencies but ignores their locations. In time series applications, several recent papers adopted BoW ideas. Lin et al. [25] first symbolize time series by SAX, then slide a fixed-sized window to extract a contiguous set of SAX words, and at last use the frequency distribution of SAX words as a representation for the time series. Baydogan et al. [9] propose a similar bag-of-features framework.</p><p align="justify">They sample subsequences with varying lengths randomly, use mean, variance, slope and temporal location t to represent each subsequence, afterwards utilize random forest classification to estimate class probabilities of each subsequence, and finally represent the raw time series by summarizing the subsequence class-probability distribution information. They showed superior or comparable results to competing methods like NNDTW on UCR datasets [4]. Wang et al. [10] adopted a typical bag of words framework to classify biomedical time series data, and they sample subsequences uniformly and represent them by DWT. Grabocka and Schmidt-Thieme [32] introduce a similar BoW pipeline to classify time series: they sample subsequences from time series instances uniformly, learn latent patterns and membership assignments of each subsequence to those patterns, and sum up membership assignments of subsequences on a time series as the representation of that time series.</p><p align="justify">Time series representations are then classified by polynomial kernel SVM. Our work belongs to this category, but emphasizes detecting better local feature points and developing better local subsequence representations. There are two recent papers using local descriptors as well [33], [34]. In [33], the authors attempt to improve efficiency of traditional DTW computation, to be concrete, they extract local feature points, match them by their descriptors and compute the local band constraints (based on matched pairs) applicable during the execution of the DTW algorithm. In this way, they only have to compute the accumulative distances within the band, and the DTW computation efficiency is improved.</p><p align="justify">Our work is different from [33] in that: our use local descriptors to improve classification accuracies, while [33] use local descriptors to improve DTW computation efficiency. In [34], the authors extract local features from multi-variate time series by leveraging metadata, and their method for local feature extraction is only applicable for multi-variate time series data with known correlations and dependencies among different dimensions. UCR datasets are univariate time series datasets, and their method cannot be used here.</p><h2 id="background">background</h2><h3 id="nn-based-time-series-classification">NN-Based Time Series Classification</h3><p align="justify">As mentioned in Sect. 1, most studies have been directed at finding techniques that can compensate for small misalignments between time series. Two main elastic distance measures, DTW and edit distance, have been widely studied.</p><h3 id="dtw-based-elastic-distance-measures">DTW-Based Elastic Distance Measures.</h3><p align="justify">DTW is considered as the standard benchmark elastic distance measure to find an optimal alignment between two given sequences [16]. The standard DTW utilizes a pointwise distance matrix to record the cumulative distance from the start point pair to current point pair and employs the dynamic programming method to complete the process. Two aspects of DTW have been studied in recent years. One is speedup technique because the standard DTW has a quadratic time and space complexity. Some works have reduced it to nearly linear time complexity [25]. Another improvement is to change the calculation way of cumulative distance. A weighted form of DTW (WDTW) [14] is proposed to reduce warping by adding a multiplicative weight item to penalize points with higher phase difference between a test point and a reference point. In standard DTW, there is a scenario where a single point on one time series may map onto a large number of points on the other time series, which lead to pathological results. To avoid these singularities, a modification of DTW, called DDTW, is proposed by transforming the time series into a series of first-order difference [17]. On the basis of this idea, G´orecki et al. [11] use a weighted combination of DTW on raw time series and DDTW on first order differences for NN classification. An extension of DDTW that uses DTW in conjunction with transforms and derivatives is proposed by G´orecki and Luczak [12]. They propose a new distance function by combining three distances: DTW distance between time series, DTW distance between derivatives of time series, and DTW distance between transforms of time series.</p><h3 id="edit-distance-based-elastic-distance-measures">Edit Distance-Based Elastic Distance Measures.</h3><p align="justify">The initial edit distancetechnique is longest common subsequence (LCSS) distance which is extended to handle the real-valued time series from discrete series by using a distance threshold. A point pair from two time series can be considered as a match if their distance is less than the predefined threshold. Like LCSS, edit distance on real sequences (EDR) [5] also use a distance threshold to define a series match, but the difference is EDR employs a constant penalty to deal with the scenario of non-matching point pair. The drawback of EDR is it does not satisfy triangular inequality. Chen et al. [6] revise the weakness of EDR by utilizing the distance between point pairs when there is no gap and a constant when gaps occur. TWE [22] and MSM [27] are two effective edit distance-based approaches proposed in recent years. TWE makes full use of the characteristics of LCSS and DTW, which allows warping in the time axis and combines the edit distance with Lpnorms. In MSM the similarity of two different time series is calculated by using a series of operations to transform a given time series into the target time series.</p><h3 id="other-elastic-distance-measures">Other Elastic Distance Measures.</h3><p align="justify">Batista et al. consider the complexityinvariance problem for time series similarity measures and propose a parameterfree method, complexity invariant distance (CID) [13], to solve this problem. They describe a method for weighting a distance measure to compensate for the differences in the complexity when two time series are compared. The sum of squares of the first differences is used to measure the complexity. Except using individual elastic distance measure to calculate the similarity of two time series, Lines and Bagnall [20] combine 11 elastic distance measures through simple ensemble schemes and get significantly better classification accuracy.</p><h3 id="basic-nearest-subspace-algorithm">Basic Nearest Subspace Algorithm</h3><p>The NN classifier may be the simplest supervised method to predict the label of a test instance. Essentially, it seeks the best representation of a test instance in term of one training sample. Unlike NN algorithm, the nearest subspace (NS) classifier (e.g. $[18,21])$ takes all training samples of each class into consideration and tries to find the best representation by fitting the test instance. Formally, we assign a test instance $y$ to class $i$ if the distance from $y$ to the subspace spanned by all samples $\boldsymbol{X}<em>i=\left[\boldsymbol{x}</em>{i, 1}, \ldots, \boldsymbol{x}_{i, n_4}\right]$ of class $i$ is the smallest one among all classes, i.e., \(r_i(\boldsymbol{y})=\min _{\boldsymbol{\alpha}_i \in \mathbb{R}^{n_1, i \in\{1, \ldots, K\}}}\left\|\boldsymbol{y}-\boldsymbol{X}_i \boldsymbol{\alpha}_i\right\|_2\) where $\boldsymbol{\alpha}_i$ is a fitting coefficient vector. One may notice that the Eq. (1) is easily overfitting which makes the problem ill-posed when we attempt to get the best solution. In general, we can introduce an additional regularization item to prevent overfitting. An alternative is to restrict the variation of $\boldsymbol{\alpha}$ by adding an $L_2$-regularization term: \(\widetilde{\boldsymbol{\alpha}}_i=\underset{\alpha_i \in \mathbb{R}^{n_i}}{\arg \min }\left\|\boldsymbol{y}-\boldsymbol{X}_i \boldsymbol{\alpha}_i\right\|_2^2+\lambda\left\|\boldsymbol{\alpha}_i\right\|_2^2\)</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main></body></html>
