<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Principal Component Analysis" /><meta property="og:locale" content="en_US" /><meta name="description" content="Principal Component Analysis" /><meta property="og:description" content="Principal Component Analysis" /><link rel="canonical" href="http://localhost:4000/2023/02/16/pca" /><meta property="og:url" content="http://localhost:4000/2023/02/16/pca" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-16T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Principal Component Analysis" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-16T00:00:00+00:00","datePublished":"2023-02-16T00:00:00+00:00","description":"Principal Component Analysis","headline":"Principal Component Analysis","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/16/pca"},"url":"http://localhost:4000/2023/02/16/pca"}</script><title> Principal Component Analysis - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/16/pca" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Principal Component Analysis (PCA) and its Variants"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Principal Component Analysis (PCA) and its Variants"/><meta property="og:title" content="Principal Component Analysis"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-16T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/16/pca" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Principal Component Analysis</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-16T00:00:00+00:00">February 16, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/PCA">PCA</a>, <a href="/tag/SVD">SVD</a></span></div><h1 id="principal-component-analysis-pca-and-its-variants">Principal Component Analysis (PCA) and its Variants</h1><p>Principal Component Analysis (PCA) is a widely used technique for dimensionality reduction and feature extraction. PCA finds a lower-dimensional representation of the data that retains as much of the original information as possible. In this article, we will explore the basic PCA algorithm and its variants.</p><h2 id="basic-pca-algorithm">Basic PCA Algorithm</h2><p>Given a dataset $\mathbf{X} = {\mathbf{x}_1, \mathbf{x}_2, …, \mathbf{x}_n}$ consisting of $n$ data points, each with $d$ dimensions, the goal of PCA is to find a set of $k$ orthonormal basis vectors ${\mathbf{v}_1, \mathbf{v}_2, …, \mathbf{v}_k}$ that capture the most important information in the data.</p><p>The first principal component $\mathbf{v}_1$ is the direction that maximizes the variance of the projected data. This can be found by solving the optimization problem:</p>\[\mathbf{v}_1 = \operatorname*{argmax}_{\|\mathbf{v}\|=1} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{v}^T \mathbf{x}_i)^2\]<p>The second principal component $\mathbf{v}_2$ is the direction that maximizes the variance of the projected data, subject to the constraint that it is orthogonal to $\mathbf{v}_1$. This can be found by solving the optimization problem:</p>\[\mathbf{v}_2 = \operatorname*{argmax}_{\|\mathbf{v}\|=1, \mathbf{v} \perp \mathbf{v}_1} \frac{1}{n} \sum_{i=1}^{n} (\mathbf{v}^T \mathbf{x}_i)^2\]<p>This process is repeated to find the remaining principal components ${\mathbf{v}_3, \mathbf{v}_4, …, \mathbf{v}_k}$.</p><p>The projection of the data onto the $k$-dimensional subspace spanned by ${\mathbf{v}_1, \mathbf{v}_2, …, \mathbf{v}_k}$ is given by:</p>\[\mathbf{Z} = \begin{bmatrix} \mathbf{v}_1^T \mathbf{x}_1 &amp; \mathbf{v}_2^T \mathbf{x}_1 &amp; \cdots &amp; \mathbf{v}_k^T \mathbf{x}_1 \\ \mathbf{v}_1^T \mathbf{x}_2 &amp; \mathbf{v}_2^T \mathbf{x}_2 &amp; \cdots &amp; \mathbf{v}_k^T \mathbf{x}_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{v}_1^T \mathbf{x}_n &amp; \mathbf{v}_2^T \mathbf{x}_n &amp; \cdots &amp; \mathbf{v}_k^T \mathbf{x}_n \end{bmatrix}\]<p>where each row of $\mathbf{Z}$ corresponds to a projected data point.</p><h1 id="sparse-pca-and-its-variants">Sparse PCA and its Variants</h1><p>Sparse PCA is a variant of Principal Component Analysis (PCA) that aims to find sparse representations of the data. It is useful for feature selection and dimensionality reduction in high-dimensional datasets where only a few features are relevant.</p><h2 id="basic-sparse-pca-algorithm">Basic Sparse PCA Algorithm</h2><p>Given an $n \times p$ data matrix $\mathbf{X}$, the goal of Sparse PCA is to find a sparse $p \times 1$ loading vector $\mathbf{w}$ that maximizes the variance of the projected data.</p><p>The Sparse PCA optimization problem can be formulated as:</p>\[\max_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{X}^T \mathbf{X} \mathbf{w}}{\|\mathbf{w}\|^2}\]<p>subject to the constraint $|\mathbf{w}|_0 \leq k$, where $k$ is a sparsity parameter that controls the number of non-zero entries in $\mathbf{w}$.</p><p>This problem is non-convex and NP-hard, but can be approximately solved using various algorithms, such as the Orthogonal Matching Pursuit (OMP), the Iterative Soft Thresholding (IST), and the Iterative Hard Thresholding (IHT).</p><p>The solution to the Sparse PCA problem is not unique, and depends on the choice of the sparsity parameter $k$ and the algorithm used.</p><h2 id="variants-of-sparse-pca">Variants of Sparse PCA</h2><h3 id="group-sparse-pca">Group Sparse PCA</h3><p>Group Sparse PCA is a variant of Sparse PCA that takes into account the group structure of the features. It involves partitioning the features into groups and enforcing sparsity within each group.</p><p>The Group Sparse PCA optimization problem can be formulated as:</p>\[\max_{\mathbf{w}_1, \dots, \mathbf{w}_G} \sum_{g=1}^G \frac{\mathbf{w}_g^T \mathbf{X}_g^T \mathbf{X}_g \mathbf{w}_g}{\|\mathbf{w}_g\|^2}\]<p>subject to the constraint $|\mathbf{w}_g|_2 \leq c_g$ for each group $g$, where $c_g$ is a regularization parameter that controls the sparsity within the group.</p><p>The Group Sparse PCA problem can be solved using various algorithms, such as the Group Iterative Soft Thresholding (GIST), the Group Iterative Hard Thresholding (GIHT), and the Group Orthogonal Matching Pursuit (GOMP).</p><h3 id="sparse-pca-with-orthogonal-constraint">Sparse PCA with Orthogonal Constraint</h3><p>Sparse PCA with Orthogonal Constraint is a variant of Sparse PCA that enforces orthogonality between the loading vectors. It involves solving the Sparse PCA problem subject to the constraint $\mathbf{W}^T \mathbf{W} = \mathbf{I}$, where $\mathbf{W}$ is the $p \times k$ loading matrix.</p><p>The Sparse PCA with Orthogonal Constraint problem can be formulated as:</p>\[\max_{\mathbf{W}} \frac{\text{tr}(\mathbf{W}^T \mathbf{X}^T \mathbf{X} \mathbf{W})}{\|\mathbf{W}\|^2}\]<p>subject to the constraint $|\mathbf{w}_i|_0 \leq k$ for each column of $\mathbf{W}$.</p><p>The Sparse PCA with Orthogonal Constraint problem can be solved using various algorithms, such as the Sparse Orthogonal Iteration (SOI)</p><h1 id="robust-principal-component-analysis-pca">Robust Principal Component Analysis (PCA)</h1><p>Principal Component Analysis (PCA) is a popular statistical technique used for data analysis and dimensionality reduction. It is often used to identify patterns and relationships within datasets by reducing the number of dimensions and extracting a set of uncorrelated variables, known as principal components. However, traditional PCA assumes that the data is well-behaved and has no outliers.</p><p>Robust PCA is a modification of traditional PCA that can handle outliers in the data. It is particularly useful when dealing with datasets that are corrupted by noise or contain anomalies. The goal of robust PCA is to separate the data into a low-rank component (representing the underlying structure of the data) and a sparse component (representing the outliers or anomalies in the data).</p><p>The mathematical formulation of robust PCA involves solving the following optimization problem:</p>\[\begin{equation} \min_{L,S} ||L||_* + \lambda ||S||_1 \quad \text{subject to} \quad X = L + S \end{equation}\]<table><tbody><tr><td>where $X$ is the data matrix, $L$ is the low-rank component, $S$ is the sparse component, $</td><td> </td><td>.</td><td> </td><td>_*$ is the nuclear norm (sum of singular values), $</td><td> </td><td>.</td><td> </td><td>_1$ is the l1 norm (sum of absolute values), and $\lambda$ is a regularization parameter that controls the trade-off between low-rankness and sparsity.</td></tr></tbody></table><p>To solve this optimization problem, various algorithms have been proposed, including Principal Component Pursuit (PCP), Accelerated Proximal Gradient (APG), and Alternating Direction Method of Multipliers (ADMM). These algorithms are iterative and require solving sub-problems at each iteration.</p><p>In this example, the original data matrix (top left) contains outliers (red points). Robust PCA separates the data into a low-rank component (bottom left) and a sparse component (bottom right). The low-rank component represents the underlying structure of the data, while the sparse component represents the outliers.</p><p>Robust PCA has many applications, including image and video processing, recommender systems, and anomaly detection. It is a powerful tool for data analysis when dealing with noisy or corrupted data.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
