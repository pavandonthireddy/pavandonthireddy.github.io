<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Singular value decomposition" /><meta property="og:locale" content="en_US" /><meta name="description" content="Singular value decomposition" /><meta property="og:description" content="Singular value decomposition" /><link rel="canonical" href="http://localhost:4000/2023/02/17/svd" /><meta property="og:url" content="http://localhost:4000/2023/02/17/svd" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-17T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Singular value decomposition" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-17T00:00:00+00:00","datePublished":"2023-02-17T00:00:00+00:00","description":"Singular value decomposition","headline":"Singular value decomposition","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/17/svd"},"url":"http://localhost:4000/2023/02/17/svd"}</script><title> Singular value decomposition - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/17/svd" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Singular Value Decomposition (SVD) and its Variants"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Singular Value Decomposition (SVD) and its Variants"/><meta property="og:title" content="Singular value decomposition"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-17T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/17/svd" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Singular value decomposition</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-17T00:00:00+00:00">February 17, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/PCA">PCA</a>, <a href="/tag/SVD">SVD</a>, <a href="/tag/linear_algebra">linear_algebra</a></span></div><h1 id="singular-value-decomposition-svd-and-its-variants">Singular Value Decomposition (SVD) and its Variants</h1><p>Singular Value Decomposition (SVD) is a matrix factorization technique that has many applications in signal processing, image compression, and machine learning. In this article, we will explore the basic SVD algorithm and its variants.</p><h2 id="basic-svd-algorithm">Basic SVD Algorithm</h2><p>Given an $m \times n$ matrix $\mathbf{A}$, the goal of SVD is to find matrices $\mathbf{U}$, $\mathbf{D}$, and $\mathbf{V}$ such that:</p>\[\mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{V}^T\]<p>where $\mathbf{U}$ is an $m \times m$ orthogonal matrix, $\mathbf{D}$ is an $m \times n$ diagonal matrix with non-negative entries called singular values, and $\mathbf{V}$ is an $n \times n$ orthogonal matrix.</p><p>The singular values in $\mathbf{D}$ are arranged in descending order along the diagonal. The first $k$ singular values and their corresponding columns in $\mathbf{U}$ and $\mathbf{V}$ capture the most important information in the matrix.</p><p>The truncated SVD approximation of rank $k$ is given by:</p>\[\mathbf{A}_k = \mathbf{U}_k \mathbf{D}_k \mathbf{V}_k^T\]<p>where $\mathbf{U}_k$ is the first $k$ columns of $\mathbf{U}$, $\mathbf{D}_k$ is the first $k$ rows and columns of $\mathbf{D}$, and $\mathbf{V}_k$ is the first $k$ columns of $\mathbf{V}$.</p><p>The SVD can be computed using various algorithms, such as the Golub-Kahan bidiagonalization algorithm, the Jacobi method, and the power iteration method.</p><h2 id="variants-of-svd">Variants of SVD</h2><h3 id="truncated-svd">Truncated SVD</h3><p>Truncated SVD is a variant of SVD that is useful for dimensionality reduction and feature extraction. It involves computing the truncated SVD approximation of rank $k$, as described above.</p><h3 id="randomized-svd">Randomized SVD</h3><p>Randomized SVD is a variant of SVD that is useful for large datasets that do not fit into memory. It involves approximating the SVD using a randomized algorithm that computes a low-rank approximation of the matrix.</p><p>The algorithm consists of the following steps:</p><ol><li>Generate a random $n \times k$ matrix $\mathbf{R}$.</li><li>Compute the matrix $\mathbf{Y} = \mathbf{A} \mathbf{R}$.</li><li>Compute the QR decomposition $\mathbf{Y} = \mathbf{Q} \mathbf{B}$.</li><li>Compute the SVD of the matrix $\mathbf{B} = \mathbf{W} \mathbf{\Sigma} \mathbf{V}^T$.</li><li>Compute the matrix $\mathbf{U} = \mathbf{Q} \mathbf{W}$.</li></ol><p>The randomized SVD algorithm is faster than the basic SVD algorithm for large datasets, and can be used to compute a low-rank approximation of the matrix in a reasonable amount of time.</p><h1 id="sparse-svd-and-kernel-svd">Sparse SVD and Kernel SVD</h1><h2 id="sparse-svd">Sparse SVD</h2><p>Sparse SVD is a variant of traditional SVD that can handle sparse data. It is often used in text mining and natural language processing, where the data is represented as a sparse matrix. The goal of sparse SVD is to decompose the data into a low-rank component and a sparse component, while preserving the original sparsity pattern.</p><p>The mathematical formulation of sparse SVD involves solving the following optimization problem:</p>\[\begin{equation} \min_{L,S} ||L||_* + \lambda ||S||_1 \quad \text{subject to} \quad X = L + S \end{equation}\]<table><tbody><tr><td>where $X$ is the data matrix, $L$ is the low-rank component, $S$ is the sparse component, $</td><td> </td><td>.</td><td> </td><td>_*$ is the nuclear norm (sum of singular values), $</td><td> </td><td>.</td><td> </td><td>_1$ is the l1 norm (sum of absolute values), and $\lambda$ is a regularization parameter that controls the trade-off between low-rankness and sparsity.</td></tr></tbody></table><p>To solve this optimization problem, various algorithms have been proposed, including Principal Component Pursuit (PCP), Accelerated Proximal Gradient (APG), and Alternating Direction Method of Multipliers (ADMM). These algorithms are iterative and require solving sub-problems at each iteration.</p><h2 id="kernel-svd">Kernel SVD</h2><p>Kernel SVD is a variant of traditional SVD that can handle nonlinear data. It is often used in machine learning, where the data is represented as a kernel matrix. The goal of kernel SVD is to decompose the data into a low-dimensional linear subspace and a nonlinear component, while preserving the original kernel similarity structure.</p><p>The mathematical formulation of kernel SVD involves solving the following optimization problem:</p>\[\begin{equation} \min_{L,S} ||L||_* + \lambda ||S||_1 \quad \text{subject to} \quad K = LL^T + S \end{equation}\]<table><tbody><tr><td>where $K$ is the kernel matrix, $L$ is the low-dimensional subspace, $S$ is the nonlinear component, $</td><td> </td><td>.</td><td> </td><td>_*$ is the nuclear norm (sum of singular values), $</td><td> </td><td>.</td><td> </td><td>_1$ is the l1 norm (sum of absolute values), and $\lambda$ is a regularization parameter that controls the trade-off between low-rankness and sparsity.</td></tr></tbody></table><p>To solve this optimization problem, various algorithms have been proposed, including Kernel PCA, Kernel Ridge Regression, and Kernel Ridge Regression with Iterative Hard Thresholding (KRR-IHT).</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div style="width:100%;text-align:center;"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
