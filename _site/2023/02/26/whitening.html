<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Whitening and Quasi Whitening" /><meta property="og:locale" content="en_US" /><meta name="description" content="Whitening and Quasi Whitening" /><meta property="og:description" content="Whitening and Quasi Whitening" /><link rel="canonical" href="http://localhost:4000/2023/02/26/whitening" /><meta property="og:url" content="http://localhost:4000/2023/02/26/whitening" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-26T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Whitening and Quasi Whitening" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-26T00:00:00+00:00","datePublished":"2023-02-26T00:00:00+00:00","description":"Whitening and Quasi Whitening","headline":"Whitening and Quasi Whitening","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/26/whitening"},"url":"http://localhost:4000/2023/02/26/whitening"}</script><title> Whitening and Quasi Whitening - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/26/whitening" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Quasi-Whitening of Sensor Matrices"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Quasi-Whitening of Sensor Matrices"/><meta property="og:title" content="Whitening and Quasi Whitening"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-26T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/26/whitening" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Whitening and Quasi Whitening</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-26T00:00:00+00:00">February 26, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/BlindSourceSeperation">BlindSourceSeperation</a>, <a href="/tag/whitening">whitening</a>, <a href="/tag/preprocessing">preprocessing</a></span></div><h2 id="quasi-whitening-of-sensor-matrices">Quasi-Whitening of Sensor Matrices</h2><p>In signal processing and machine learning, sensor matrices are often used to represent high-dimensional data collected from sensors. These matrices can be analyzed using techniques such as independent component analysis (ICA) or principal component analysis (PCA), but these methods often assume that the samples are uncorrelated and have unit variance. Quasi-whitening is a preprocessing technique that can transform a given sensor matrix into a form that has these desirable statistical properties.</p><p>The goal of quasi-whitening is to transform the sensor matrix in such a way that the samples become uncorrelated and have unit variance. This can help improve the performance of algorithms that rely on these statistical properties. The term “quasi” refers to the fact that the transformed matrix is not perfectly whitened, meaning that the samples are not completely uncorrelated and do not have exactly unit variance. However, the transformation is still useful because it can significantly reduce the correlation among the sensor samples, making them more suitable for subsequent analysis.</p><p>One common technique for quasi-whitening a sensor matrix is to use a preprocessing step called “decorrelation,” which involves applying a matrix transformation to the original sensor matrix to eliminate correlations among the samples. After decorrelation, the transformed matrix can be further scaled so that each sample has unit variance.</p><p>The decorrelation transformation can be achieved using the following equation:</p>\[X' = DX\]<p>where $X$ is the original sensor matrix, $D$ is a decorrelation matrix, and $X’$ is the transformed matrix. The decorrelation matrix $D$ is typically computed using the eigenvalue decomposition of the sample covariance matrix of $X$. Specifically, if the sample covariance matrix of $X$ is denoted by $C$, then $D$ can be computed as:</p>\[D = C^{-1/2}\]<p>where $C^{-1/2}$ is the matrix inverse square root of $C$. This transformation has the property that the transformed matrix $X’$ has a sample covariance matrix that is approximately equal to the identity matrix.</p><p>After decorrelation, the transformed matrix $X’$ can be further scaled so that each sample has unit variance. This is achieved using the following equation:</p>\[X'' = \frac{X'}{\sqrt{\mathrm{diag}(X'^TX')}}\]<p>where $\mathrm{diag}(A)$ denotes the diagonal elements of matrix $A$. This transformation scales each sample so that it has unit variance, while preserving the decorrelation achieved by the earlier transformation.</p><p>In summary, quasi-whitening is a useful preprocessing technique for improving the performance of signal processing and machine learning algorithms that rely on uncorrelated and unit variance samples. By decorrelating and scaling the sensor matrix, quasi-whitening can reduce the impact of correlations among the samples and make them more suitable for subsequent analysis.</p><h1 id="whitening-for-blind-source-signal-separation">Whitening for Blind Source Signal Separation</h1><p>In many applications such as speech recognition and biomedical signal processing, it is common to encounter signals that are mixtures of multiple sources. Blind source separation (BSS) is the problem of separating these sources from their mixture without any prior knowledge of the sources or the mixing process. One technique used for BSS is whitening, which transforms the mixture to a new space where the sources are uncorrelated and have unit variance.</p><h2 id="whitening">Whitening</h2><p>Whitening is a linear transformation that maps a random vector $\mathbf{x} \in \mathbb{R}^n$ with covariance matrix $\mathbf{\Sigma}$ to a new vector $\mathbf{y} \in \mathbb{R}^n$ with identity covariance matrix, i.e.,</p>\[\mathbf{y} = \mathbf{W} \mathbf{x},\]<p>where $\mathbf{W}$ is a square matrix such that $\mathbf{W}\mathbf{\Sigma}\mathbf{W}^T = \mathbf{I}$, where $\mathbf{I}$ is the identity matrix.</p><p>To see why this is useful for BSS, consider a mixture of $m$ sources:</p>\[\mathbf{x}(t) = \sum_{i=1}^m s_i(t) \mathbf{a}_i,\]<p>where $\mathbf{a}_i$ is the mixing matrix for the $i$th source, and $s_i(t)$ is the signal of the $i$th source at time $t$. We assume that the sources are statistically independent and have zero mean and unit variance. The covariance matrix of $\mathbf{x}$ is then</p>\[\mathbf{\Sigma}_x = \mathbb{E}[\mathbf{xx}^T] = \sum_{i=1}^m \mathbf{a}_i \mathbf{a}_i^T.\]<p>To separate the sources, we want to find a matrix $\mathbf{B}$ such that</p>\[\mathbf{y}(t) = \mathbf{B} \mathbf{x}(t) = \sum_{i=1}^m b_i s_i(t) \mathbf{e}_i,\]<p>where $\mathbf{e}_i$ is the $i$th canonical basis vector. The covariance matrix of $\mathbf{y}$ is then</p>\[\mathbf{\Sigma}_y = \mathbb{E}[\mathbf{yy}^T] = \mathbf{B} \mathbf{\Sigma}_x \mathbf{B}^T.\]<p>If we choose $\mathbf{B} = \mathbf{W}^{-1}$, then we have</p>\[\mathbf{\Sigma}_y = \mathbf{W}^{-1} \mathbf{\Sigma}_x (\mathbf{W}^{-1})^T = \mathbf{W}^{-1} \mathbf{W} \mathbf{\Sigma}_s \mathbf{W}^T (\mathbf{W}^{-1})^T = \mathbf{I},\]<p>where $\mathbf{\Sigma}_s$ is the diagonal matrix of the variances of the sources.</p><p>Thus, by whitening the mixture with $\mathbf{W}$, we transform the problem of BSS into a problem of finding a linear transformation that makes the sources uncorrelated.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main></body></html>
