<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Classifying Time Series Using Local Descriptors with Hybrid Sampling" /><meta property="og:locale" content="en_US" /><meta name="description" content="Classifying Time Series Using Local Descriptors with Hybrid Sampling" /><meta property="og:description" content="Classifying Time Series Using Local Descriptors with Hybrid Sampling" /><link rel="canonical" href="http://localhost:4000/2023/02/03/classifying-time-series-using-local-descriptors-with-hybrid-sampling" /><meta property="og:url" content="http://localhost:4000/2023/02/03/classifying-time-series-using-local-descriptors-with-hybrid-sampling" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-03T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Classifying Time Series Using Local Descriptors with Hybrid Sampling" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-03T00:00:00+00:00","datePublished":"2023-02-03T00:00:00+00:00","description":"Classifying Time Series Using Local Descriptors with Hybrid Sampling","headline":"Classifying Time Series Using Local Descriptors with Hybrid Sampling","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/03/classifying-time-series-using-local-descriptors-with-hybrid-sampling"},"url":"http://localhost:4000/2023/02/03/classifying-time-series-using-local-descriptors-with-hybrid-sampling"}</script><title> Classifying Time Series Using Local Descriptors with Hybrid Sampling - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/03/classifying-time-series-using-local-descriptors-with-hybrid-sampling" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Abstract"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Abstract"/><meta property="og:title" content="Classifying Time Series Using Local Descriptors with Hybrid Sampling"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-03T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/03/classifying-time-series-using-local-descriptors-with-hybrid-sampling" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Classifying Time Series Using Local Descriptors with Hybrid Sampling</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-03T00:00:00+00:00">February 3, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/classification">classification</a>, <a href="/tag/timeseries">timeseries</a>, <a href="/tag/GMM">GMM</a>, <a href="/tag/representation">representation</a></span></div><h2 id="abstract">Abstract</h2><p align="justify">Time series classification (TSC) arises in many fields and has a wide range of applications. Here, we adopt the bag-ofwords (BoW) framework to classify time series. Our algorithm first samples local subsequences from time series at feature-point locations when available. It then builds local descriptors, and models their distribution by Gaussian mixture models (GMM), and at last it computes a Fisher Vector (FV) to encode each time series. The encoded FV representations of time series are readily used by existing classifiers, e.g., SVM, for training and prediction. In our work, we focus on detecting better feature points and crafting better local representations, while using existing techniques to learn codebook and encode time series. Specifically, we develop an efficient and effective peak and valley detection algorithm from real-case time series data. Subsequences are sampled from these peaks and valleys, instead of sampled randomly or uniformly as was done previously. Then, two local descriptors, Histogram of Oriented Gradients (HOG-1D) and Dynamic time warping-Multidimensional scaling (DTW-MDS), are designed to represent sampled subsequences. Both descriptors complement each other, and their fused representation is shown to be more descriptive than individual ones. We test our approach extensively on 43 UCR time series datasets, and obtain significantly improved classification accuracies over existing approaches, including NNDTW and shapelet transform.</p><h2 id="introduction">Introduction</h2><p>A typical BoW framework consists of three major steps:</p><ol><li>local feature points detection and description,</li><li>codebook generation and</li><li>signal encoding.</li></ol><p>Afterwards, any classifier can be trained on signal encodings to do the final classification.</p><p>The performance of a BoW framework implementation depends on all steps. In the computer vision community, many efforts have been made to improve each step.</p><p>Regarding local feature detection and description, successful feature extractors (e.g., SIFT, Space Time Interest Points (STIPs)) have been developed to detect local feature points, and manually-crafted descriptors (e.g., Histogram of Gradients, Motion Boundary Histogram (MBH)) have been invented to represent local 2D image patches and 3D visual cuboid patterns around feature points.</p><p>However, as reviewed below, fewer developments have been made with 1D time series descriptors. The next step, codebook generation, attempts to model the local descriptor space and to provide a partition in that space. Two typical ways are K-means and Gaussian Mixture Models (GMM). For the last step, encoding, there is a large family of research studies; several representative encoding methods are vector quantization (hard voting) , sparse coding and Fisher Vector encoding .</p><p>In this work, we adopt the BoW pipeline: we focus on improving the first step: designing better local feature extractors and descriptors, while using existing techniques for the second and third steps; specifically, GMM is used to produce the codebook and Fisher Vector is employed to encode the time series.</p><p>While local feature extractors are well studied in the computer vision community, in the time series community, no widely used extractors exist yet, such that most methods sample feature points either uniformly or randomly. In this paper, we introduce an efficient and effective feature point extractor, which detects all peaks and valleys, termed as landmarks, from time series. Afterwards, subsequences centered on landmarks are sampled. Landmark-based sampling gives deterministic and phase-invariant subsequences, while uniformor random sampling are affected by the phase of the time series.</p><p align="justify">Due to the observation that dense sampling outperforms sparse interest-points sampling in image classification and activity recognition, in experiments, we adopt a hybrid sampling strategy: first sample subsequences from landmarks, then sample uniformly in “flat” featureless regions of the signal. In this way, information from both feature- rich and feature-less intervals is incorporated in the sampled subsequences. We show experimentally that this new hybrid sampling strategy outperforms both uniform and random sampling significantly.</p><p align="justify">To the best of our knowledge, little recent literature on time series classification is focused on developing better local descriptors for local time series subsequences. Commonly used local features are often simple, including mean, variance and slope. However, statistical features like mean and variance cannot characterize local segment shapes well.</p><p align="justify">Although slope incorporates shape information, it will underfit the shape of local subsequences if the interval (here, a subsequence is divided into equal-length non-overlapping temporal intervals and represented as a sequence of slopes of intervals) is too long, and becomes sensitive to noise if the interval is too short. Symbolic Aggregate approXimation (SAX) is shown be a good representation for time series, however, its usage in a BoW framework creates a large codebook, resulting in high-dimensional encoding vectors for time series, which inevitably enburdens downstream classifier training and prediction.</p><p align="justify"> Other widely used and somewhat older time series representations include Discrete Fourier Transform (DFT) coefficients, Discrete Wavelet Transform (DWT), piecewise linear approximation (PLA), etc. It is important to clarify that SAX, DFT, DWT and PLA have been used in general to represent the whole time series, instead of local subsequences.</p><p align="justify">In our work, we propose two new local descriptors, namely Histogram of Oriented Gradients (HOG) of 1D time series (HOG-1D) and Dynamic time warping-multidimensional scaling (DTW-MDS), which are shown experimentally to be quite descriptive of local subsequence shapes. These two descriptors have individual advantages: HOG- 1D consists of statistical histograms, therefore is robust to noise. Moreover, HOG-1D is invariant to y-axis magnitude shift. While DTW-MDS is sensitive to noise and magnitude shift, it is more invariant to stretching, contraction and temporal shifting. Two descriptors thus complement each other. By fusing them into a single descriptor, the fused one, HOG-1D+DTW-MDS, combines the benefits of both descriptors, becomes more descriptive of subsequences, and thus is more discriminative for classification tasks.</p><p align="justify">Experimental results show that our fused descriptor outperforms existing descriptors, such as DFT, DWT and Zhang’s significantly on 43 UCR datasets for time series classification. Here DFT, DWT and Zhang’s are used to represent local subsequences, instead of the whole time series. All local descriptors, including our fused one, work under the same classification pipeline: (1) feature points extraction, (2) local subsequence representation, (3) time series encoding by Fisher Vector, (4) linear kernel SVM classification. In addition, we compare TSC performance of our fused descriptor with two state-of-the-art algorithms, NNDTW and shapelet transform, on 41 UCR datasets, and ours achieves the best performance on 22 of them (including ties).</p><p align="justify">Wilcoxon signed rank test on relative accuracy boost shows our fused descriptor improves relative classification accuracies significantly compared to NNDTW (p &lt; 0.0017) and shapelet transform (p &lt; 0.0452). Our algorithm performs well on UCR datasets, which have fixed length time series instances, however, our algorithm is also applicable to datasets with variable length time series instances, since Fisher Vector is essentially a normalized encoder, making encodings largely invariant to time series length.</p><p align="justify">Our contributions are several fold: (1) we introduce a simple but effective feature point extractor, which detects a set of landmarks from time series; (2) we explicitly design two local subsequence descriptors, namely HOG-1D and DTW-MDS, which are descriptive of local shapes and complement each other; (3) we obtain significantly improved classification accuracies using our fused descriptor when compared with two competing state-of-the-art TSC algorithms, NNDTW and shapelet transform, and three existing descriptors, DFT, DWT and Zhang’s, on 43 UCR datasets.</p><p>2 PREVIOUS WORK Time series classification methods can be categorized into instance-based, shapelets, feature-based and pattern frequency histogram methods. Instance-based methods predict labels of test time series based on their similarity to the training instances. The most popular similarity metrics include euclidean distance and elastic distances, e.g., the dynamic time warping (DTW) distance. Using a single nearest neighbor, with euclidean distance (NNEuclidean) or DTW distance (NNDTW), has demonstrated successful time series label prediction. DTW allows time series to be locally shifted, contracted and stretched, and lengths of time series hence need not be the same. Therefore, DTW usually gives a better similarity measurement than Euclidean distance, and NNDTW has been shown to be very hard to beat on many datasets [11]. A number of more complex elastic distance measures have been proposed, including longest common subsequences (LCSS) [27], Edit distance with Real Penalty (ERP) [28] and edit distance on Real Sequence (EDR) [29]. However, in [30], the authors claimed that no other elastic distance measure outperforms DTW by a statistically significant amount, and DTW is the best measure. Instance-based approaches, like NN-euclidean and NNDTW, are accurate, but they are less interpretable, since they are based on global matching and provide limited insights into the temporal characteristics.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div style="width:100%;text-align:center;"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
