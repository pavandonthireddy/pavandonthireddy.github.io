<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Nearest Subspace Classification" /><meta property="og:locale" content="en_US" /><meta name="description" content="Nearest Subspace Classification" /><meta property="og:description" content="Nearest Subspace Classification" /><link rel="canonical" href="http://localhost:4000/2023/02/20/nearest-subspace-classification" /><meta property="og:url" content="http://localhost:4000/2023/02/20/nearest-subspace-classification" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-20T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Nearest Subspace Classification" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-20T00:00:00+00:00","datePublished":"2023-02-20T00:00:00+00:00","description":"Nearest Subspace Classification","headline":"Nearest Subspace Classification","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/20/nearest-subspace-classification"},"url":"http://localhost:4000/2023/02/20/nearest-subspace-classification"}</script><title> Nearest Subspace Classification - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/20/nearest-subspace-classification" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Nearest Subspace Classification"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Nearest Subspace Classification"/><meta property="og:title" content="Nearest Subspace Classification"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-20T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/20/nearest-subspace-classification" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Nearest Subspace Classification</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-20T00:00:00+00:00">February 20, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/classification">classification</a>, <a href="/tag/timeseries">timeseries</a>, <a href="/tag/subspace_methods">subspace_methods</a></span></div><h1 id="nearest-subspace-classification">Nearest Subspace Classification</h1><p>Nearest Subspace Classification (NSC) is a classification algorithm that is used to classify high-dimensional data into different categories. It is based on the idea that the data can be represented as a subspace in a higher-dimensional feature space, and the nearest subspace to a new observation can be used to classify it into one of the predefined categories.</p><h2 id="subspace-representation">Subspace Representation</h2><p>Let $X$ be a matrix of size $n \times p$, where $n$ is the number of data points and $p$ is the number of features. Let $Y$ be a vector of size $n \times 1$, where each element $y_i$ is the label of the corresponding data point $x_i$.</p><p>We can represent the data points in $X$ as subspaces in a higher-dimensional feature space by using a linear projection. Let $V$ be a matrix of size $p \times k$, where $k &lt; p$, that represents the projection matrix. The subspace representation of a data point $x_i$ is then given by:</p>\[\hat{x_i} = VV^T x_i\]<p>The subspace representation of the entire dataset can be obtained by computing the projection of $X$ onto the subspace spanned by $V$:</p>\[\hat{X} = XV(XV)^+\]<p>where (+) denotes the Moore-Penrose pseudoinverse. The subspace spanned by $V$ is also called the column space of $V$.</p><h2 id="nearest-subspace-classification-1">Nearest Subspace Classification</h2><p>Given a new data point $x$, the goal of NSC is to classify it into one of the predefined categories based on its subspace representation. The basic idea is to compute the distance between the subspace representation of $x$ and the subspace representations of each category, and classify $x$ into the category with the smallest distance.</p><p>Let $C_i$ be the set of data points in class $i$, and let $V_i$ be the matrix that represents the subspace spanned by the data points in $C_i$. The distance between the subspace representation of $x$ and the subspace spanned by $V_i$ is given by:</p>\[d_i = \|x - V_iV_i^Tx\|\]<p>The classification rule of NSC is then to classify $x$ into the category with the smallest distance:</p>\[\hat{y} = \operatorname*{argmin}_{i=1}^k d_i\]<p>where $k$ is the number of categories.</p><h2 id="algorithm">Algorithm</h2><p>The algorithm for NSC can be summarized as follows:</p><ol><li>Given a labeled training dataset $X$ and $Y$, compute the subspace representations of each class by computing the projection matrix $V$ for each class.</li><li>Given a new data point $x$, compute its subspace representation using the projection matrix $V$.</li><li>For each class $i$, compute the distance between the subspace representation of $x$ and the subspace spanned by the data points in class $i$.</li><li>Classify $x$ into the category with the smallest distance.</li></ol><h2 id="advantages-and-disadvantages">Advantages and Disadvantages</h2><p>NSC has several advantages over other classification algorithms, including:</p><ul><li>It can handle high-dimensional data by representing the data as subspaces in a higher-dimensional feature space.</li><li>It is robust to noise and outliers, since the subspace representation of the data is obtained by a linear projection.</li><li>It can handle non-linear decision boundaries by using a non-linear projection.</li></ul><p>However, NSC also has some disadvantages:</p><ul><li>It is sensitive to the choice of projection matrix $V$, which can affect the performance of the classifier. In practice, multiple projection matrices can be used and the best one can be selected using cross-validation or other methods.</li><li>It assumes that the data points in each class are drawn from a single subspace, which may not always be true in practice.</li><li>It may not perform well on imbalanced datasets, where some classes have significantly fewer data points than others. In this case, it may be necessary to use techniques such as oversampling or undersampling to balance the dataset.</li></ul><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div class="center"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
