<!DOCTYPE html><html lang="en" ><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Jump process for the trend estimation of time series" /><meta property="og:locale" content="en_US" /><meta name="description" content="Jump process for the trend estimation of time series" /><meta property="og:description" content="Jump process for the trend estimation of time series" /><link rel="canonical" href="http://localhost:4000/2023/02/14/jump-process-for-trend-estimation-time-series" /><meta property="og:url" content="http://localhost:4000/2023/02/14/jump-process-for-trend-estimation-time-series" /><meta property="og:site_name" content="Pavan Donthireddy" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-02-14T00:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Jump process for the trend estimation of time series" /><meta name="twitter:site" content="@" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-02-14T00:00:00+00:00","datePublished":"2023-02-14T00:00:00+00:00","description":"Jump process for the trend estimation of time series","headline":"Jump process for the trend estimation of time series","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2023/02/14/jump-process-for-trend-estimation-time-series"},"url":"http://localhost:4000/2023/02/14/jump-process-for-trend-estimation-time-series"}</script><title> Jump process for the trend estimation of time series - Pavan Donthireddy</title><meta charset="UTF-8"><link rel="canonical" href="http://localhost:4000/2023/02/14/jump-process-for-trend-estimation-time-series" /><meta name="viewport" content="width=device-width, initial-scale=1.0"><meta name="description" content="Abstract"><meta property="og:site_name" content="Pavan Donthireddy"><meta property="og:description" content="Abstract"/><meta property="og:title" content="Jump process for the trend estimation of time series"><meta property="og:type" content="article"><meta property="article:published_time" content="2023-02-14T00:00:00+00:00"><meta property="article:author" content="http://localhost:4000/"><meta property="og:url" content="http://localhost:4000/2023/02/14/jump-process-for-trend-estimation-time-series" /><link rel="shortcut icon" href="/favicon.png"><link rel="alternate" type="application/atom+xml" title="Pavan Donthireddy" href="/atom.xml"><link rel="alternate" type="application/json" title="Pavan Donthireddy" href="http://localhost:4000/feed.json" /><link rel="sitemap" type="application/xml" title="sitemap" href="/sitemap.xml" /> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ jax: ["input/TeX","input/MathML","output/SVG", "output/CommonHTML"], extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "CHTML-preview.js"], TeX: { extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"] }, tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true, processEnvironments: true, skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }, messageStyle: "none", "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }, TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js","AMScd.js"], TagSide: "left", Macros: { field: ['\\mathbb{#1}', 1], C: ['\\field{C}'], E: ['\\field{E}'], F: ['\\field{F}'], N: ['\\field{N}'], P: ['\\field{P}'], Q: ['\\field{Q}'], R: ['\\field{R}'], Z: ['\\field{Z}'], ha : ['\\hat{#1}',1], Re: ['\\mathop{\\mathrm{Re}}'], Im: ['\\mathop{\\mathrm{Im}}'], Res: ['\\mathop{\\mathrm{Res}}'], } } }); </script> <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full"></script><style> *,:after,:before{box-sizing:border-box;background-color:inherit;color:inherit;margin:0;padding:0}body{font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;line-height:1.5;font-size:1rem;color:#16171a}nav ul{border-right:1px solid #edf2f7}a{color:#000;text-decoration-skip-ink:auto;text-decoration:underline}pre{margin:.5rem 0;padding:.5rem}.post p{margin:.5rem 0}.post h1,.post h2,.post h3,.post h4{margin:1rem 0}.post h2:first-child,.project h2:first-child,.photo h2:first-child{margin-top:0}.meta{margin:2rem 0}code,pre{background:#ecedee}code{padding:.1rem}pre code{border:none}pre{padding:1rem;overflow-x:auto}img{max-width:100%}hr{background:#000;height:1px;border:0}header{flex-basis:10rem;flex-grow:1;position:relative}header a{text-decoration:none}header li{margin-bottom:.2rem;text-align:right;margin-right:2rem}header a.active{font-weight:bold}header,section{padding:1rem}blockquote{font-style:italic;border-left:5px solid #ececec;padding-left:1rem}h1,h2,h3,h4,h5{line-height:1;margin:1rem 0;font-weight:600}section h1:first-child{margin-top:0}strong,b{font-weight:bold}.photos ul{list-style:none}.photos li{margin-bottom:1.5rem}.photo picture,.project picture{margin-bottom:.5rem}.posts ul,header ul{list-style:none}.posts li{align-items:center;display:flex;justify-content:space-between;margin-bottom:.5rem}.posts li a,.posts li div,.projects li a{white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.posts li time,.projects li time{padding-left:1rem;white-space:nowrap;font-variant-numeric:tabular-nums}main{display:flex;flex-wrap:wrap;max-width:60rem;margin:2rem auto;padding:1rem}@media screen and (max-width: 45rem){header li{display:inline;margin-right:1rem}.logo{padding-bottom:1rem}header ul{border-bottom:1px solid #edf2f7;padding-bottom:2rem}nav ul{border-right:0px}.photos ul{margin-top:.5rem}}section{flex-basis:0;flex-grow:999;min-width:70%;display:flex;flex-direction:column}figcaption{font-size:smaller}.post-archive{font-size:15px;line-height:2;padding-bottom:.8em}.post-archive .date{padding-right:.7em}.pagination{width:100%;padding:0 20px;text-align:center}.pagination ul{list-style:none}.pagination ul li{display:inline;margin:0 5px}.pagination ul li a{color:#000;text-decoration:none}.pagination ul li a:hover{color:gray;text-decoration:underline}.pagination ul li.current-page a{background:#ddd;color:#fff}.post-link{position:relative;display:inline-block}.post-excerpt{display:none;position:absolute;z-index:1;top:100%;left:0;width:100%;padding:10px;background-color:#fff;box-shadow:0 2px 5px rgba(0,0,0,.1)}.post-link:hover .post-excerpt{display:block}.img-padding{width:200px;height:200px;padding:20px 20px 20px 20px}</style></head><body><main role="main"><header role="banner"> <!--<h1 class="logo">Pavan Donthireddy</h1>--><nav role="navigation"><ul><li><a href="/" >Notes</a></li><li><a href="/tags" >Tags</a></li><li><a href="/search" >Search</a></li><li><a href="/about" >About</a></li></ul></nav></header><link rel="stylesheet" href="/assets/js/prism.css"><link rel="stylesheet" href="/assets/js/prism-line-numbers.css"><section class="post"> <script src="/assets/js/prism.js"></script> <script src="/assets/js/prism-line-numbers.js"></script><h1 style="text-align: center;">Jump process for the trend estimation of time series</h1><div style="text-align: center;"><span class="meta"><time datetime="2023-02-14T00:00:00+00:00">February 14, 2023</time> </span></div><div style="text-align: center;"><span class="meta"> <a href="/tag/trend_estimation">trend_estimation</a>, <a href="/tag/timeseries">timeseries</a></span></div><h2 id="abstract">Abstract</h2><p>A jump process approach is proposed for the trend estimation of time series. The proposed jump process estimator can locally minimize two important features of a trend, the smoothness and 0delity, and explicitly balance the fundamental tradeoff between them. A weighted average form of the jump process estimator is derived. The connection of the proposed approach to the Hanning 0lter, Gaussian kernel regression, the heat equation and the Wiener process is discussed. It is found that the weight function of the jump process approaches the Gaussian kernel, as the smoothing parameter increases.</p><h2 id="introduction">Introduction</h2><p>The main objective of this paper is to present a new nonparametric approach, jump process, to estimate the unknown deterministic trend function of a time series. In conrast to the global implicit minimization approach, a localized approach is developed for trend estimation. The proposed jump process can locally minimize both characteristics and explicitly balance the fundamental tradeoff between them. A weighted average form of the jump process estimate is derived in the present paper, so that the implementation of jump process becomes extremely simple. The connection between the present jump process approach and the traditional trend estimation methods, as well as the DSP is discussed. It is found that the weight function of the jump process filter approaches the normal kernel, as the smoothing parameter increases.</p><h2 id="theory-and-algorithm">Theory and Algorithm</h2><h3 id="local-measurement-of-smoothness-and-fidelity">Local measurement of smoothness and fidelity</h3><p>A common feature of optimization schemes WH, HP, SS and MR is to minimize the linear combination of the global measure of fidelity and smoothness, while using a “smoothing parameter” to balance the tradeoff between fidelity and smoothness. There are three key aspects in the design of these nonparametric trend estimation approaches: (1) Define measures for fidelity and smoothness, (2) balance the tradeoff by employing a smoothing parameter, (3) minimize the linear combination of two measures to achieve an estimate which is optimal in the sense of the given measures.</p><p>In the present study, an iterative jump process will be considered, which actually admits a simpler optimization approach for trend estimation \(\begin{aligned} &amp; T_t^{M+1}=T_t^M+R\left(T_{t-1}^M-2 T_t^M+T_{t+1}^M\right) \\ &amp; \quad=T_t^M+R \Delta^2 T_t^M \\ &amp; T_t^0=y_t, \quad t=1, \ldots, N, \end{aligned}\) where ratio $R(R&gt;0)$ and iteration parameter $M$ are user-specified constants. The second term on the right-hand side of iterative process (7), $T_{t-1}^M-2 T_t^M+T_{t+1}^M$, is the pointwise measure of smoothness. To have a better understanding of this iterative jump process, $T_t^M$ of Eq. (7) is rewritten in terms of $y_t$ : \(T_t^M=y_t+R \sum_{k=0}^{M-1} \Delta^2 T_t^k, \quad t=1, \ldots, N .\) Further denote $v_t^{M-1}=\sum_{k=0}^{M-1} \Delta^2 T_t^k$, this yields \(\left(y_t-T_t^M\right)+R v_t^{M-1}=0, \quad t=1, \ldots, N .\) It is obvious that the first term on the left-hand side of (9), $\left(y_t-T_t^M\right)$, is the local measure of the fidelity, while the second term, $v_t^{M-1}$, is the accumulative local measure of smoothness. At each step of the iteration, this process guarantees that the sum of the linear combination of the local deviation from $y_t$ and the accumulative local measure of smoothness equals to zero. In the sense of such local minimization, the optimal estimated trend is the output of iterative jump process (7) by using $y_t$ as input. Such a trend estimation method will be referred as a jump process estimator.</p><p>In terms of minimization, the relationship between the jump process and the optimization schemes WH, HP, SS, and MR is somewhat analogous to the relationship between the collocation and Galerkin approximation schemes well known in numerical analysis (see for example, Wei, 2000). The previous optimization schemes minimize the criterion function over the entire domain to obtain optimal estimates, while jump process forces the criterion function to pass through zero at each step of the iteration to give an optimal trend.</p><p>Besides the minimization of two properties, another important aspect of the construction of nonparametric trend estimation approach is the tradeoff balance. To illustrate how jump process (7) balances the tradeoff between smoothness and fidelity, the stability of jump process (7) is considered first. For this purpose, Eq. (7) is rewritten in a matrix form, \(\mathbf{T}^{M+1}=\mathbf{A T}^M\) where $\mathbf{T}^M=\left(T_1^M, T_2^M, \ldots, T_N^M\right)^{\prime}$, and the tridiagonal matrix $\mathbf{A}$ has coefficients: $a_{t, t-1}=$ $a_{t, t+1}=R$ and $a_{t, t}=1-2 R$, for $t=1,2, \ldots, N$. If all of the eigenvalues of $\mathbf{A}$ are smaller than unity, the iterative correction $\varepsilon^{M+1}=\left|\mathbf{T}^{M+1}-\mathbf{T}^M\right|$ will decay, then the process is stable. Since each diagonal term of the matrix is a constant, the eigenvectors of $\mathbf{A}$ can be represented in terms of a complex exponential form, \(\mathbf{T}_t^M=q^M \mathrm{e}^{\mathrm{i} \gamma t}\) where $\mathrm{i}=\sqrt{-1}$ and $\gamma$ is a wavenumber that can be chosen arbitrarily. Substituting Eq. (11) into Eq. (10) and removing the common term $\mathrm{e}^{\mathrm{i} \gamma t}$, an explicit expression for the eigenvalue $q$ is obtained \(q=1+2 R(\cos \gamma-1)\) For a stable process, the magnitude of this quantity is required to be smaller than unity, \(q^2=[1+2 R(\cos \gamma-1)]^2&lt;1\) and $q$ is maximum when $\cos \gamma=-1$. Therefore, the iterative process is stable provided $R&lt;\frac{1}{2}$</p><p>Thus, when $0&lt;R&lt;\frac{1}{2}$, one has $\varepsilon^{M+1} \leqslant \varepsilon^M$, for any $M \in \mathbb{Z}^{+}$. Due to $\varepsilon^{M+1}=| \mathbf{T}^{M+1}-$ $\mathbf{T}^M|=| \Delta^2 \mathbf{T}^M |$, is actually the global smoothness measure of estimated trend at the $M$ th iteration step, one can argue that as the iterative process is carried out longer and longer, the estimated trend becomes smoother and smoother, while the deviation of $\mathbf{T}^M$ from $\mathbf{Y}=\left(y_1, y_2, \ldots, y_N\right)^{\prime}$ becomes larger and larger. Two smoothing parameters, $R$ and $M$, govern the fundamental tradeoff between the smoothness and fidelity. In practice, $R$ can be pre-fixed in the iteration process and only $M$ is used to achieve the desired tradeoff.</p><h3 id="weighted-average-form-of-jump-process">Weighted average form of jump process</h3><p>The advantage of the proposed approach is its simplicity, robustness and efficiency. However, the relationship between final estimates and original time series needs to be clarified for jump process (7). Fortunately, like most of the other nonparametric approaches, the estimated trend of the jump process also permits a weighted average representation in terms of the original series $y_t$. If $M$ equals to 1 , the estimates are \(T_t^1=R y_{t-1}+(1-2 R) y_t+R y_{t+1}, \quad t=1, \ldots, N,\) which is clearly a local weighted average form for $y_t$. In general, after $M$ iterations, the estimated trend can be represented as \(T_t^M=\sum_{k=t-M}^{t+M} W(k, M) y_k,\) where weight function $W(k, M)$ has the general form of and \(g(k, M, h)=\frac{M ! R^{M-h}(1-2 R)^h}{((M+k-h) / 2) !((M-k-h) / 2) ! h !} .\) It can be easily verified that, \(\sum_{k=t-M}^{t+M} W(k, M)=1\) and \(W(-k, M)=W(k, M) \quad \forall k=1, \ldots, M .\) Eq. (15) indicates that the jump process estimator can be viewed as a kernel smoother with (16) as a jump process kernel. From the point of view of the DSP, the weight function $W(k, M)$ is a low-pass filter. The implementation of the jump process becomes extremely simple due to the existence of (15). Therefore, the weighted average form (15) is very useful numerically.</p><p>The weight assignment of the jump process filter is analogous to that of other kernel regression methods. When $M$ is not too small, and for any reasonable choice of $R$, such as $0.1 \leqslant R&lt;0.5$, the greater of the weights is assigned to the points close $y_i$, the smaller weight will be assigned to the points far away from $y_i$, see Table 1 and Fig. 1. It can also be seen from the figure that, when $M$ is large, although the involved neighborhood is large, the effective window size of significant nonzero filter coefficients is smaller than $2 M+1$.</p><p>A simple moving average filter can be formed by convolving $\left(\frac{1}{2}, \frac{1}{2}\right)$ with itself $2 M$ times. When $M=1$, such a filter is the so-called Hanning filter (see Goodall, 1990) \(\left(W_{-1}, W_0, W_1\right)=\left(\frac{1}{4}, \frac{1}{2}, \frac{1}{4}\right)\) By setting $R=\frac{1}{4}$ in Eq. (14), it is clear that the one step jump process filter has the same coefficients as those of the Hanning filter and the $M$ step jump process filter is identical to the digital filter obtained by convolving Hanning filter with itself M times. Thus, the proposed jump process 0lter can be viewed as a generalization of the Hanning 0lter.</p><table><thead><tr><th style="text-align: left">k</th><th style="text-align: left">W_(k,6)</th><th style="text-align: left"> </th><th style="text-align: left"> </th></tr></thead><tbody><tr><td style="text-align: left"> </td><td style="text-align: left">General</td><td style="text-align: left">R=0.4</td><td style="text-align: left">R=0.1</td></tr><tr><td style="text-align: left">0</td><td style="text-align: left">924R^(6)-1512R^(5)+1050R^(4)-400R^(3)+90R^(2)-12 R+1</td><td style="text-align: left">0.181824</td><td style="text-align: left">0.390804</td></tr><tr><td style="text-align: left">1</td><td style="text-align: left">-792R^(6)+1260R^(5)-840R^(4)+300R^(3)-60R^(2)+6R</td><td style="text-align: left">0.154368</td><td style="text-align: left">0.227808</td></tr><tr><td style="text-align: left">2</td><td style="text-align: left">495R^(6)-720R^(5)+420R^(4)-120R^(3)+15R^(2)</td><td style="text-align: left">0.12672</td><td style="text-align: left">0.065295</td></tr><tr><td style="text-align: left">3</td><td style="text-align: left">-220R^(6)+270R^(5)-120R^(4)+20R^(3)</td><td style="text-align: left">0.07168</td><td style="text-align: left">0.01048</td></tr><tr><td style="text-align: left">4</td><td style="text-align: left">66R^(6)-60R^(5)+15R^(4)</td><td style="text-align: left">0.039936</td><td style="text-align: left">0.000966</td></tr><tr><td style="text-align: left">5</td><td style="text-align: left">-12R^(6)+6R^(5)</td><td style="text-align: left">0.012288</td><td style="text-align: left">0.000048</td></tr><tr><td style="text-align: left">6</td><td style="text-align: left">R^(6)</td><td style="text-align: left">0.004096</td><td style="text-align: left">0.000001</td></tr></tbody></table><h3 id="generalization-of-jump-process-estimator">Generalization of jump process estimator</h3><p>For simplicity, only the uniformly spaced data have been considered for trend estimation so far. However, the framework of jump process estimator can be extended easily to randomly distributed data (i.e. nonuniformly spaced observations), \(\begin{aligned} &amp; T_t^{M+1}=T_t^M+\bar{R} \frac{T_{t-1}^M\left(x_t-x_{t-1}\right)-T_t^M\left(x_{t+1}-x_{t-1}\right)+T_{t+1}^M\left(x_{t+1}-x_t\right)}{\frac{1}{2}\left(x_{t+1}-x_{t-1}\right)\left(x_{t+1}-x_t\right)\left(x_t-x_{t-1}\right)} \\ &amp; T_t^0=y_t, \quad t=1, \ldots, N . \end{aligned}\) This iteration process is stable provided $\bar{R}&lt;(\Delta x)^2 / 2$, where $\Delta x=\min <em>t\left(x_t-x</em>{t-1}\right)$. The weighted average form of $(21)$ depends on the design. It will be quite complicated to obtain the explicit expression of the weighted function similar to (16). Fortunately, the S. Zhao, G.W. Wei/ Computational Statistics \&amp; Data Analysis 42 (2003) 219-241 227 regression estimates based on the iterative jump process (21) can be easily obtained. Therefore, the trend estimation based on jump process (21) will be useful even if either some data points are missing or a cross validation method is employed.</p><p>It is well known that the continuous counterpart of the jump process in stochastic processes is the diffusion process, which is usually represented in the form of a partial differential equation, \(\begin{aligned} &amp; \frac{\partial T(x, \tau)}{\partial \tau}=\nabla^2 T(x, \tau), \\ &amp; T(x, 0)=y(x) . \end{aligned}\) Here, the temporal variable $\tau$ is the continuous time, rather than the time variable of the time series. To numerically simulate the diffusion process on uniformly spaced data, the second-order central difference and explicit Euler scheme may be employed for spatial and temporal discretizations \(\begin{aligned} &amp; T_t^{M+1}=T_t^M+\frac{\Delta \tau}{(\Delta x)^2}\left(T_{t-1}^M-2 T_t^M+T_{t+1}^M\right), \\ &amp; T_t^0=y_t, \quad t=1, \ldots, N . \end{aligned}\) It is interesting to note that if one sets $R=\Delta \tau /(\Delta x)^2$, discretized approximation scheme (23) is the same as the iterative jump process (7) and the stability of explicit Euler scheme also requires mesh ratio $\Delta \tau /\left(\Delta x^2\right)&lt;1 / 2$. Thus, the diffusion Eq. (22) is capable of providing alternative perspective for the understanding of the jump process estimator. For example, the nonuniform jump process (21) can be easily derived from the heat Eq. (22). Obviously, high-order spatiotemporal discretization of Eq. (22) can also be used to construct a family of jump processes.</p><h3 id="jump-process-and-wiener-process">Jump process and Wiener process</h3><p>For an appropriate range of $R(0&lt;R&lt;1 / 2)$, the coefficients of (14), i.e. $(R, 1-$ $2 R, R)$, are nonnegative and $R+(1-2 R)+R=1$. Therefore, these coefficients may be interpreted as probabilities. Consider a particle at position $k$ on the $x$-axis at time $\tau=M \Delta \tau$, in the next $\Delta \tau$ time period, the particle can have only three possible states: forward $\Delta k$, backward $\Delta k$, no change in position, with probabilities of $P^{+}, P^{-}$and $P$, respectively, \(\Delta k= \begin{cases}\Delta x &amp; \text { with probability } P^{+}=R, \\ 0 &amp; \text { with probability } P=1-2 R, \\ -\Delta x &amp; \text { with probability } P^{-}=R,\end{cases}\) where $k+\Delta k$ is the particle position after $\Delta \tau$. The process (24) is usually called a jump process in the stochastic process analysis, see for example Cox and Ross (1976). If a particle follows the jump process (24) and starts at the origin of the $x$-axis at $\tau=0$, after $M$ steps, it is easy to prove that the probability of this particle at position $k$ is exactly the weight $W(k, M)$ given by (16). Hence, further investigation of jump process (24) will provide considerable insight into the proposed jump process filter.</p><p>It is obvious that the local mean and variance of $\Delta k$ in (24) are \(\begin{aligned} E\{\Delta k\} &amp; =\Delta x\left(P^{+}-P^{-}\right)=0 \\ \operatorname{Var}\{\Delta k\} &amp; =\Delta x^2\left(P^{+}+P^{-}\right)-(E\{\Delta k\})^2 \\ &amp; =2 \Delta x^2 R \\ &amp; =2 \Delta \tau . \end{aligned}\) In the continuum limit of an infinitesimally small step size, the discrete model (24) yields \(\mathrm{d} k=\sqrt{2} \mathrm{~d} Z\) where $\mathrm{d} Z$ is a standard Wiener process with $E{\mathrm{~d} Z}=0, \operatorname{Var}\left{\mathrm{d} Z^2\right}=\mathrm{d} \tau$. This implies that the $\mathrm{d} k$ is also a one-dimensional Wiener process (Brownian motion without the drift). Hence, the increase of particle movement during a relatively long period of time $\tau$ is given by \(k(\tau)-k(0)=\sum_{t=1}^M \varepsilon_t \sqrt{2 \mathrm{~d} \tau},\) where the $\varepsilon_t(t=1,2, \ldots, M)$ are random numbers drawn from a standardized normal distribution. Consequently, it can be shown that $k(\tau)-k(0)$ is normally distributed with (Hull, 1999, Section 10.2) \(\begin{aligned} &amp; E\{k(\tau)-k(0)\}=0, \\ &amp; \operatorname{Var}\{k(\tau)-k(0)\}=\sqrt{2 \tau} . \end{aligned}\) Here $k(0)=0$ and $k(\tau)=k$. This means that under the jump process (24), the particle movement will follow the normal distribution in the continuum limit $\Delta \tau \rightarrow 0$. Since $\Delta \tau \rightarrow 0$ is equivalent to $M \rightarrow \infty$ when $\tau$ is fixed, and the particle movement probability function is exactly the weight function $W(k, M)$. One can conclude that the weight function $W(k, M)$ of the proposed jump process filter will approach the normal kernel at the limit of $M \rightarrow \infty$.</p><p>It is well known that the filter coefficients generated by convolving Hanning moving average filter approximate the Gaussian kernel as $M \rightarrow \infty$. The above finding indicates that, the present generalized Hanning filter, the jump process filter, shares the same property. Such property endows the jump process weight function to be a good kernel function for kernel regression, for which a widely used kernel is the Gaussian density. The proposed jump process weight function provides a discrete approximation to the Gaussian kernel. On the other hand, as pointed out in the DSP literature (such as Koenderink, 1984; Hummel, 1987), the solution of the heat diffusion Eq. (22) may equivalently be viewed as the result obtained by convolving original signal with the Gaussian kernel. This again agrees with the present finding about the jump process estimator.</p><h3 id="implementation-particulars">Implementation particulars</h3><p>It follows from above discussion that there are two simple and controllable ways to implement the jump process trend estimation. One way is based on iterative jump process (7). The time series $y_{t}$ is used as the input data. The iteration number M is used to control the final estimates.</p><p>The other way is to use the weighted average form (15). The trend is estimated by convolving $y_{t}$ and the jump process kernel once. In both ways, the R can be fixed and only M needs to be adjusted. Theoretically, the estimated trends from two ways are the same, however, there are some minor differences due to possible different boundary treatment and applicability. Generally speaking, for uniform spacing data trend estimation problems, the convolution implementation is more efficient than iterative implementation. However, the iterative implementation can be easily done for randomly spaced data regression, for which the weight kernel of the form of (15) is difficult to be constructed.</p><p>Another difference of these two implementations is the different possible modification in dealing with boundary effect. The boundary effect is a common thorny problem for linear filtering and kernel smoothing, i.e., linear symmetric filters fail to provide estimates for the initial or=and end terms of the series (Kendall et al., 1983). The problem seems to be more serious in a convolution, since a larger computational support will locate outside the boundaries in this case, while there is only one point outside each boundary for the iterative way at each iteration step.</p><p>In the literature, there are some alternative approaches for dealing with boundary effect (Goodall, 1990).</p><p>One approach uses progressively more asymmetric versions of filters at the end points, it will result in more biased estimates. Such techniques were widely used in moving average filtering and kernel smoothing, see for example Gasser and MNuller (1979), and can be directly adopted by the convolution implementation of the jump process estimation. The counterpart of such a technique in an iterative implementation is the well-known upwind difference approximation scheme in numerical analysis. However, though based on the same motivation, these two modifications along with two implementations generally yield different estimates.</p><p>Artificially “padding data” or extrapolating the series is another approach for generating necessary support for the symmetric filter. According to the observed characteristic of trend component, repeating the latest observation, symmetric or antisymmetric extension may be used.</p><p>The same extension technique, such as symmetric or antisymmetric, can be adopted by both convolution and iterative implementations. Furthermore, it can be proved that by using the same symmetric=antisymmetric extension technique, the final estimates of two different implementations of the jump process are identical. Therefore, we limit our attention in the present study to the implementation of convolution with boundary extensions.</p><h2 id="conclusion">Conclusion</h2><p>The characteristic of the jump process estimator is its local optimization. In terms of minimization, the relationship between the jump process approach and traditional methods is analogous to the relationship between the collocation and Galerkin approximation in numerical analysis. The numerical strengths of the jump process are simplicity and robustness.</p><p>Like many nonparametric approaches, the estimate of the jump process also permits a weighted average form, which is very useful numerically. The weight shape of the jump process filter is also analogous to that of other kernel regression schemes. The present study reveals that the jump process filter can be viewed as a generalization of the Hanning filter. A jump process trend estimation scheme is developed for nonuniformly spaced data. Such a scheme is useful in case where some data points are missing or cross validation method is used. Furthermore, it is shown that the proposed jump process is equivalent to a discretized form of the heat diffusion equation. Hence the continuous discusion process can be used as the other convenient starting point for theoretical analysis.</p><p>By examining the jump process from the point of view of stochastic process analysis, it is shown that such a jump process, in an appropriate limit, is a Wiener process. Hence, the weight function of jump process Filter approaches the normal curve when smoothing parameter tends to infinity. This agrees with the relevant 0ndings of the convolution Hanning Filter, as well as the findings of the heat equation in the DSP literature.</p><hr> <side style="font-size: 0.9em"><h3 style="margin-bottom: 1em">Notes mentioning this note</h3><div style="font-size: 0.9em"><p> There are no notes linking to this note.</p></div></side> <a href="" class="post-link"><h2></h2><p class="post-excerpt"></p></a> <script src="https://giscus.app/client.js" data-repo="pavandonthireddy/pavandonthireddy.github.io" data-repo-id="[ENTER REPO ID HERE]" data-category="[ENTER CATEGORY NAME HERE]" data-category-id="[ENTER CATEGORY ID HERE]" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light_high_contrast" data-lang="en" data-loading="lazy" crossorigin="anonymous" async> </script></section></main><div style="width:100%;text-align:center;"> <a href='https://ko-fi.com/G2G3KNTHM' target='_blank'><img height='36' style='border:0px;height:36px;' src='https://storage.ko-fi.com/cdn/kofi5.png?v=3' border='0' alt='Buy Me a Coffee at ko-fi.com' /></a></div></body></html>
