---
layout: post
title: Hilbert Huang Transform
description: Hilbert Huang Transform
summary: Hilbert Huang Transform
author:
- Pavan Donthireddy
usemathjax: true
tags: [trend_extraction, EMD, hilbert_spectral_analysis, HHT, EEMD]
---


The EMD method is necessary to reduce any data from non-stationary and nonlinear processes into simple oscillatory function that will yield meaningful instantaneous frequency through the Hilbert transform. Contrary to almost all the previous decomposing methods, EMD is empirical, intuitive, direct, and adaptive, with the a posteriori defined basis derived from the data. The decomposition is designed to seek the different simple intrinsic modes of oscillations in any data based on the principle of scale separation. The data, depending on it complexity, may have many different coexisting modes of oscillation at the same time. Each of these oscillatory modes is represented by an Intrinsic Mode Function (IMF) with the following definitions:

(a) in the whole data set, the number of extrema and the number of zero-crossings must either equal or differ at most by one, and

(b) at any point, the mean value of the envelope defined by the local maxima and the envelope defined by the local minima is zero.

The IMF is a counter part to the simple harmonic function, but it is much more general: instead of constant amplitude and frequency, IMF can have both variable amplitude and frequency as functions of time. This definition is inspired by the simple example of constant plus sinusoidal function given above. The total number of the IMF components is limited to $\ln _{2} N$, where $\boldsymbol{N}$ is the total number of data points. It satisfies all the requirements for a meaningful instantaneous frequency through Hilbert transform.

Pursuant to the above definition for IMF, one can implement the needed decomposition of any function, known as sifting, as follows: Take the test data; identify all the local extrema; divide the extrema into two sets: the maxima and the minima. Then connect all the local maxima by a cubic spline line to form an upper envelope. Repeat the procedure for the local minima to form a lower envelope. The upper and lower envelopes should encompass all the data between them. Their mean is designated as $m_1$, and the difference between the data and $m_1$ is designated as, $h_1$, a proto-IMF:

$$
X(t)-m_{1}=h_{1}
$$

Ideally, $h_1$ should satisfy the definition of an IMF by construction of $h_1$ described above, which should have made it symmetric and having all maxima positive and all minima negative. Yet, in changing the local zero from a rectangular to a curvilinear coordinate system some inflection points could become additional extrema. New extrema generated this way actually reveal the hidden modes missed in the initial treatment. The sifting process sometimes can recover signals representing low amplitude riding waves with repeated siftings.

The sifting process serves two purposes: to eliminate riding waves and to make the wave profiles more symmetric. While the first condition is absolute necessary for Hilbert transform to give a meaningful instantaneous frequency, the second condition is also necessary in case the neighboring wave amplitudes having too large a disparity. As a result, the sifting process has to be repeated many times to reduce the extracted signal an IMF. In the subsequent sifting process, $h_1$ is treated as the data for the next round of sifting; therefore,

$$
h_{1}-m_{11}=h_{11}
$$

After repeated sifting, up to $\mathrm{k}$ times, $h_{1k}$ :

$$
h_{1(k-1)}-m_{1 k}=h_{1 k} \text {. }
$$

If $\boldsymbol{h}_{\boldsymbol{1} \boldsymbol{k}}$ becomes an IMF, it is designated as 
$C_1$ :

$$
C_{1}=h_{1 k}
$$

the first IMF component from the data. Here one has a critical decision to make: when to stop. Too many rounds of sifting will reduce the IMF to FM page criterion; too few rounds of sifting will not have a valid IMF. In the past, different criteria have been used, including Cauchy type criterion (Huang et al. 19980), $\boldsymbol{S}$-number criterion (Huang et al. 2003), fixed-number criterion (Wu and Huang 2004), and etc.

With any stoppage criterion, the, $c_1$ should contain the finest scale or the shortest period component of the signal. one can, then, remove $c_1$ from the rest of the data by

$$
X(t)-C_{1}=r_{1}
$$

Since the residue, $r_1$, contains all longer period variations in the data, it is treated as the new data and subjected to the same sifting process as described above. This procedure can be repeated to all the subsequent $r_j$ 's, and the result is

$$
\begin{gathered}
r_{1}-C_{2}=r_{2}, \\
\cdots \\
r_{n-1}-C_{n}=r_{n}
\end{gathered}
$$

The sifting process should stop when the residue, $\boldsymbol{r}_{n}$, becomes a constant, a monotonic function, or a function contains only a single extrema, from which no more IMF can be extracted. By summing up Equations (16) and (17), we finally obtain

$$
X(t)=\sum_{j=1}^{n} C_{j}+r_{n}
$$

Thus, sifting process produces a decomposition of the data into $\boldsymbol{n}$-intrinsic modes, and a residue, $\boldsymbol{r}_{\boldsymbol{n}}$. When apply the EMD method, a mean or zero reference is not required; EMD needs only the locations of the local extrema. The sifting process generates the zero reference for each component. Without the need of the zero reference, EMD avoids the troublesome step of removing the mean values for the large non-zero mean.

Two special notes here deserve our attention. First, the sifting process offered a way to circumvent the difficulty of define the local mean in a nonstationary time series, where no length scale exists for one to implement the traditional mean operation. The envelope mean employed here does not involve time scale; however, it is local. Second, the sifting process is a Reynolds-type decomposition: separating variations from the mean, except that the mean is a local instantaneous mean, so that the different modes are almost orthogonal to each other, except for the nonlinearity in the data.

Recent studies by Flandrin et al. (2004) and Wu and Huang (2004) established that the EMD is equivalent to a dyadic filter bank, and it is also equivalent to an adaptive wavelet. Being adaptive, we have avoided the shortcomings of using any a priori-defined wavelet basis, and also avoided the spurious harmonics that would have resulted. The components of the EMD are usually physically meaningful, for the characteristic scales are defined by the physical data.

Having established the decomposition, we can also identify a new use of the IMF components as filtering. Traditionally, filtering is carried out in frequency space only. But there is a great difficult in applying the frequency filtering when the data is either nonlinear or non-stationary or both, for both nonlinear and nonstationary data generate harmonics of all ranges. Therefore, any filtering will eliminate some of the harmonics, which will cause deformation of the data filtered. Using IMF, however, we can devise a time space filtering. 

For example, a low pass filtered results of a signal having $\boldsymbol{n}$-IMF components can be simply expressed as

$$
X_{l k}(t)=\sum_{k}^{n} C_{j}+r_{n}
$$

a high pass results can be expressed as

$$
X_{h k}(t)=\sum_{1}^{k} C_{j}
$$

and a band pass result can be expressed as

$$
X_{b k}(t)=\sum_{b}^{k} C_{j}
$$

The advantage of this time space filtering is that the results preserve the full nonlinearity and nonstationarity in the physical space.

Having obtained the Intrinsic Mode Function components, one can compute the instantaneous frequency for each IMF component as the derivative of the phase function. And one can also designate the instantaneous amplitude from the Hilbert transform to each IMF component. Finally, the original data can be expressed as the real part, RP, of the sum of the data in terms of time, frequency and energy as:

$$
X(t)=R P \sum_{j=1}^{n} a_{j}(t) e^{i \int \omega_{j}(t) d t}
$$

Above equation gives both amplitude and frequency of each component as a function of time. The same data, if expanded in a Fourier representation, would have a constant amplitude and frequency for each component. 

The contrast between EMD and Fourier decomposition is clear: The IMF represents a generalized Fourier expansion with a time varying function for amplitude and frequency. This frequency-time distribution of the amplitude is designated as the Hilbert Amplitude Spectrum, $\boldsymbol{H}(\boldsymbol{\omega}, \boldsymbol{t})$, or simply the Hilbert spectrum.

From the Hilbert spectrum, we can also define the marginal spectrum, $\boldsymbol{h}(\boldsymbol{\omega})$, as

$$
h(\omega)=\int_{0}^{T} H(\omega, t) d t
$$

The marginal spectrum offers a measure of total amplitude (or energy) contribution from each frequency value. It represents the cumulated amplitude over the entire data span in a probabilistic sense.

The combination of the Empirical Mode Decomposition and the Hilbert Spectral Analysis is designated by NASA as the Hilbert-Huang Transform (HHT) for short. Recent studies by various investigators indicate that HHT is a super tool for time-frequency analysis of nonlinear and nonstationary data (Huang and Attoh-Okine, 2005, Huang and Shen, 2005). It is based on an adaptive basis, and the frequency is defined through the Hilbert transform. Consequently, there is no need for the spurious harmonics to represent nonlinear waveform deformations as in any of the a priori basis methods, and there is no uncertainty principle limitation on time or frequency resolution from the convolution pairs based also on a priori bases. A summary of the comparison between Fourier, Wavelet and HHT analyses is given in Table 1.

Table 1. Comparisons between Fourier, Wavelet and HilbertHuang Transform in Data analysis.

|      Transform     |              Fourier             |               Wavelet              |              Hilbert              |
|:------------------:|:--------------------------------:|:----------------------------------:|:---------------------------------:|
| Basis              | a priori                         | a priori                           | adaptive                          |
| Frequency          | convolution: global, uncertainty | convolution: regional, uncertainty | differentiation: local, certainty |
| Presentation       | energy-frequency                 | energy-time-frequency              | energy-time-frequency             |
| Nonlinear          | no                               | no                                 | yes                               |
| Non-stationary     | no                               | yes                                | yes                               |
| Feature Extraction | no                               | discrete: no, continuous: yes      | yes                               |
| Theoretical Base   | theory complete                  | theory complete                    | empirical                         |

After this basic development of the HHT method, there are some recent developments, which have either added insight to the results, enhanced the statistical significance of the results, and fixed some shortcomings in the HHT. 

## Recent developments

### The Normalized Hilbert Transform and the direct quadrature

It is well known that, although the Hilbert transform exists for any function of $\boldsymbol{L}^{p}$ class, the phase function of the transformed function will not always yield physically meaningful instantaneous frequencies. The limitations have been summarized succinctly in two theorems.

First, in order to separate the contribution of the phase variation into the phase and amplitude parts, the function have to satisfy the limitation stipulated in the Bedrosian theorem (1963), which states that the Hilbert transform for the product of two functions, $\boldsymbol{f}(\boldsymbol{t})$ and $\boldsymbol{h}(\boldsymbol{t})$, can be written as

$$
H[f(t) h(t)]=f(t) H[h(t)]
$$

only if the Fourier spectra for $\boldsymbol{f}(\boldsymbol{t})$ and $\boldsymbol{h}(\boldsymbol{t})$ are totally disjoint in frequency space, and the frequency content of the spectrum for $\boldsymbol{h}(\boldsymbol{t})$ is higher than that of $\boldsymbol{f}(\boldsymbol{t})$. This limitation is critical, for we need to have

$H[a(t) \cos \theta(t)]=a(t) H[\cos \theta(t)]$, (28) otherwise, one cannot use Equation (6) to define the phase function, for the amplitude variation would mix with the phase function. Bedrosian theorem requires that the amplitude is varying be so slowly that the frequency spectra of the envelope and the carrier waves are disjoint. This is possible only for trivial cases, for unless the amplitude is constant, any local deviation can be considered as a sum of delta-functions, which has a wide white spectrum. Therefore, the spectrum for varying amplitude would never be totally separate from that of the carrier. This limitation has made the application of the Hilbert transform even to IMFs problematic. To satisfy this requirement, Huang and Long (2003) have proposed the normalization of the IMFs in the following steps: Starting from an IMF, they first find all the maxima of the IMFs, defining the envelope by spline through all the maxima, and designating the envelope as $\boldsymbol{E}(\boldsymbol{t})$. Now, normalize the IMF by dividing the IMF by $\boldsymbol{E}(\boldsymbol{t})$. Thus, they have the normalized function having amplitude always equal to unity, and have circumvented the limitation of Bedrosian theorem.

Second, there is the new restriction given by the Nuttall theorem (1966), which stipulates that the Hilbert transform of cosine is not necessarily the sine with the same phase function for a cosine with an arbitrary phase function. Nuttall gave an energy based error bound, $\boldsymbol{\Delta E}$, defined as the difference between $y(t)$, the Hilbert transform of the data, and $\boldsymbol{Q}(\boldsymbol{t})$, the quadrature (with phase shift of exactly $90^{\circ}$ ) of the function as

$$
\Delta E=\int_{t=0}^{T}|y(t)-Q(t)|^{2} d t=\int_{-\infty}^{0} S_{q}(\omega) d \omega,
$$

in which $\boldsymbol{S}_{\boldsymbol{q}}$ is Fourier spectrum of the quadrature function. Though the proof of this theorem is rigorous, the result is hardly useful, for it gives a constant error bound over the whole data range. With the normalized IMF, Huang and Long (2003) have proposed a variable error bound based on a simple argument, which goes as follows: compute the difference between squared amplitude of the normalized IMF and unity. If the Hilbert transform is exactly the quadrature, the difference between it and unity should be zero; otherwise, the Hilbert transform cannot be exactly the quadrature. Consequently, the error can be measured simply by the difference between the squared normalized IMF and unity, which is a function of time. Huang and Long (2003) and Huang et al. (2006) have conducted detailed comparisons and found the result quite satisfactory.

Even with the error indicator, we can only know that the Hilbert transform is not exactly the quadrature; we still do not have the correct answer. This prompts a drastic alternative, eschewing the Hilbert transform totally. An exact direct quadrature has been found (Huang et al., 2006), and it would resolve the difficulties associated with the instantaneous frequency computation.


### The Confidence Limit 

The confidence limit for the Fourier spectral analysis is based on the ergodic theory, where the temporal average is treated as the ensemble average. This approach is only valid if the processes are stationary. Huang et al. (2003) has proposed a different approach by utilizing the fact that there are infinite many ways to decompose one given function into difference components. Using EMD, one can still obtain many different sets of IMFs by changing the stoppage criteria. The confidence limit so derived does not depend on the ergodic theory From the confidence limit study, Huang et al. (2003) also found the optimal $\boldsymbol{S}$-number, when the differences reach a local minimum. Based on their experience from different data sets, they concluded that an $\boldsymbol{S}$-number in the range of 4 to 8 performed well. Logic also dictates that the $\boldsymbol{S}$-number should not be too high (which would drain all the physical meaning out of the IMF), nor too low (which would leave some riding waves remaining in the resulting IMFs).

### The Statistical Significance of IMFs

The EMD is a method to separate the data into different components by their scales. There is always the question: On what is the statistical significance of the IMFs based? In data containing noise, how can we separate the noise from information with confidence? This question was addressed by both Flandrin et al. (2004) and Wu and Huang (2004) through the study of signals consisting of noise only. Using white noise, Wu and Huang (2004) found the relationship between the mean period and RMS values of the IMFs. Furthermore, from the statistical properties of the scattering of the data, they found the bounds of the data distribution analytically. They concluded that when a data set is analyzed with EMD, if the mean period-RMS values exist outside the noise bounds, the components most likely contains signal, otherwise, a component could be resulted only from noise. Therefore, the components with their mean period-RMS values exceeding the noise bounds are statistically significant.

### Ensemble EMD (EEMD)

One of the major problems existed in EMD is scale mixing: an IMF often contains local oscillations with dramatically different frequencies/scales (Huang et al 1999). Previous solution to that was introducing the intermittency check in which the frequency/scale range is subjectively determined. While such an approach works well in many cases, it also has side effect such as reducing adaptation of the EMD method.

Recently, a new Ensemble Empirical Mode Decomposition (EEMD) method is presented. This new approach consists of an ensemble of decompositions of data with added white noise, and then treats the resultant mean as the final true result. Finite, not infinitesimal, amplitude white noise is necessary to force the ensemble to exhaust all possible solutions in the sifting process, thus requiring the different scale signals to collate in the proper intrinsic mode functions (IMF) dictated by the dyadic filter banks. The effect of the added white noise is to present a uniform reference frame in the timefrequency and time-scale space; and, therefore, the added noise provides a natural reference for the signals of comparable scale to collate in one IMF. With this ensemble mean, the scale can be clearly and naturally separated without any a priori subjective criterion selection, such as in the intermittence test for the original EMD algorithm. This new approach fully utilizes the statistical characteristics of white noise to perturb the data in its true solution neighborhood, and then cancel itself out (via ensemble averaging) after serving its purpose; therefore, it represents a substantial improvement over the original EMD and qualifies for a truly noise-assisted data analysis (NADA) method.