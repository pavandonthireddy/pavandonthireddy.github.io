---
layout: post
title: Low rank plus sparse decomposition
description:  Low rank plus sparse decomposition
summary:  Low rank plus sparse decomposition
author:
- Pavan Donthireddy
usemathjax: true
tags: [signal_extraction, subspace_methods]
original: new
---

Low-rank and sparse decomposition is a technique used in signal processing, image processing, and machine learning to decompose a matrix into two components: a low-rank component and a sparse component.

A matrix is said to be low-rank if it can be approximated by a matrix of much lower rank. For example, a rank-10 matrix can be approximated by a rank-3 matrix if the approximation is good enough. A matrix is said to be sparse if most of its elements are zero.

The low-rank and sparse decomposition technique seeks to find a low-rank matrix L and a sparse matrix S such that the sum of L and S is equal to the original matrix M. This can be expressed as:

M = L + S

The low-rank component L represents the underlying structure or pattern in the data, while the sparse component S represents the noise or outliers in the data.

This technique has many applications, such as in image denoising, video compression, and anomaly detection. It is often used in combination with other techniques such as principal component analysis (PCA) and singular value decomposition (SVD) to extract meaningful information from complex data sets.

## Introduction

Let us consider the problem of the enhancement of a speech signal contaminated by an independent additive noise. Let $x(t)$ and $d(t)$ denote the sampled clean speech and noise signal, respectively. The observed noisy speech signal $y(t)$ is
$$
y(t)=x(t)+d(t)
$$
Suppose $y(t)$ was framed with the length $N$. Arranging the $N$-dimensional vectors into a $(M-1+l) \times l$ Toeplitz structure matrix, we can get
$$
Y=X+D
$$
Assuming that the rank of matrix $Y$ is $r$, the optimal enhanced speech matrix $\hat{X}$ can be estimated according to the following least-square criterion
$$
\min _{\hat{X}}\|Y-\hat{X}\|_F^2, \operatorname{rank}(\hat{X}) \leq r
$$
where symbol $\|_F$ denotes the Frobenius norm of a matrix and $\|X\|_F=\sqrt{X_{i j}^2}$

If $d(t)$ is a white Gaussian noise, it satisfies the conditions $D^T D=\sigma_d^2 I$ and $X^T D=0$. Where $\sigma_d^2$ is the variance of noise. The optimal solution of (4) can be obtained by applying singular value decomposition (SVD) of $Y$.
$$
\begin{aligned}
& Y=U \Sigma V^T \\
& \hat{X}=\sum_{i=1}^r \lambda_i U_i V_i^T .
\end{aligned}
$$
Here, $U$ and $V$ are two orthogonal matrices holding the left and right (approximate) singular vectors of given matrix, and $\Lambda$ is a diagonal matrix holding the singular values: $\lambda_1 \geq \lambda_2 \geq \cdots \lambda_{r-1} \geq \lambda_r$.

The above low-rank matrix $\hat{X}$ represents the original speech matrix $\mathrm{X}$ in the sense of least-square minimization. This may get the optimal estimate when the noise is small, independent, and identically distributed Gaussian.

However, PCA is highly sensitive to the presence of large corruptions. Even a single outlier in the data matrix can render the estimation of the low-rank component arbitrarily far from the true model. In [16], a new theory called Robust PCA was developed for this shortcoming. The basic idea of Robust PCA is to decompose the data matrix $M$ as $M=L+S$, where $S \hat{\mathrm{I}} \mathrm{i}^{N K}$ is a sparse matrix with a sparse number of non-zero coefficients with arbitrarily large magnitude. RPCA can be solved by minimizing the following convex program
$$
\min \|L\|_*+\lambda\|S\|_1 \text {, s.t. } M=L+S
$$
where $\|-\|_*$ denotes the matrix nuclear norm, which is defined as the sum of all singular values and is suggested as a convex surrogate to the rank function [18]. \|\|$_1$ denotes the $l_1$-norm of a matrix, which is defined as the sum of the absolute values of matrix elements. This problem is known to have a stable solution provided $L$ and $S$ are sufficiently incoherent [19], i. e., the low-rank matrix is not sparse and the sparse matrix is not low-rank. More recently, RPCA theory was introduced into the s peech enhancement task in [20], where a constrained low-rank and sparse matrix decomposition (CLSMD) algorithm is designed for noise reduction.

### LSD based speech denoising method

In this work, we propose a new subspace decomposition algorithm based on the LSD, which is less sensitive to the large noise interferences.

Firstly, we formulate the speech enhancement problem as the following optimization problem,
$$
\begin{aligned}
& \min _{\mathrm{L}, \mathrm{S}}\|Y-L-S\|_F^2, \\
& \text { s.t. } \operatorname{rank}(L) \leq r,|S|_0 \leq h .
\end{aligned}
$$
The above formula can be solved by alternatively solving the following two formulas until convergence
$$
\left\{\begin{array}{l}
L_i=\underset{\operatorname{rank}(L) \leq r}{\arg \min }\left\|Y-L-S_{i-1}\right\|_F^2 \\
S_i=\underset{\mid S b \leq h}{\arg \min }\left\|Y-L_i-S\right\|_F^2
\end{array}\right.
$$
Given an estimate of sparse matrix $S_{i-1}$, the minimization in (7-a) over $L$ is to learn a rank- $r$ low-rank matrix from partial observations. This is a fixed-rank approximation problem, we can solve it use bilateral random projections (BRP) based fast low-rank matrix approximation.
$$
L_t=M_1\left(A_2^T M_1\right)^{-1} M_2^T
$$
Where $M_1=Y A_1, M_2=Y^T A_2$. Both $A_1 \in R^{n \times r}$ and $A_2 \in R^{m \times r}$ are Gaussian random matrices.

The minimization in (7-b) over $S$ is to learn a sparse matrix from partial observations. This can be computed via entry-wise hard thresholding function [21], $\varphi_T(x)=x \cdot 1(|x|>u)$
which keeps the input if it is larger than the threshold; otherwise, it is set to zero. In summary, we have following optimization algorithm for LSD.


