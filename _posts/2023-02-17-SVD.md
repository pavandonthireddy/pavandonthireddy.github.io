---
layout: post
title: Singular value decomposition
description: Singular value decomposition
summary: Singular value decomposition
author:
- Pavan Donthireddy
usemathjax: true
tags: [PCA, SVD, linear_algebra]
original: new
---

# Singular Value Decomposition (SVD) and its Variants

Singular Value Decomposition (SVD) is a matrix factorization technique that has many applications in signal processing, image compression, and machine learning. In this article, we will explore the basic SVD algorithm and its variants.

## Basic SVD Algorithm

Given an $m \times n$ matrix $\mathbf{A}$, the goal of SVD is to find matrices $\mathbf{U}$, $\mathbf{D}$, and $\mathbf{V}$ such that:

$$\mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{V}^T$$

where $\mathbf{U}$ is an $m \times m$ orthogonal matrix, $\mathbf{D}$ is an $m \times n$ diagonal matrix with non-negative entries called singular values, and $\mathbf{V}$ is an $n \times n$ orthogonal matrix.

The singular values in $\mathbf{D}$ are arranged in descending order along the diagonal. The first $k$ singular values and their corresponding columns in $\mathbf{U}$ and $\mathbf{V}$ capture the most important information in the matrix.

The truncated SVD approximation of rank $k$ is given by:

$$\mathbf{A}_k = \mathbf{U}_k \mathbf{D}_k \mathbf{V}_k^T$$

where $\mathbf{U}_k$ is the first $k$ columns of $\mathbf{U}$, $\mathbf{D}_k$ is the first $k$ rows and columns of $\mathbf{D}$, and $\mathbf{V}_k$ is the first $k$ columns of $\mathbf{V}$.

The SVD can be computed using various algorithms, such as the Golub-Kahan bidiagonalization algorithm, the Jacobi method, and the power iteration method.

## Variants of SVD

### Truncated SVD

Truncated SVD is a variant of SVD that is useful for dimensionality reduction and feature extraction. It involves computing the truncated SVD approximation of rank $k$, as described above.

### Randomized SVD

Randomized SVD is a variant of SVD that is useful for large datasets that do not fit into memory. It involves approximating the SVD using a randomized algorithm that computes a low-rank approximation of the matrix.

The algorithm consists of the following steps:

1. Generate a random $n \times k$ matrix $\mathbf{R}$.
2. Compute the matrix $\mathbf{Y} = \mathbf{A} \mathbf{R}$.
3. Compute the QR decomposition $\mathbf{Y} = \mathbf{Q} \mathbf{B}$.
4. Compute the SVD of the matrix $\mathbf{B} = \mathbf{W} \mathbf{\Sigma} \mathbf{V}^T$.
5. Compute the matrix $\mathbf{U} = \mathbf{Q} \mathbf{W}$.

The randomized SVD algorithm is faster than the basic SVD algorithm for large datasets, and can be used to compute a low-rank approximation of the matrix in a reasonable amount of time.

# Sparse SVD and Kernel SVD

## Sparse SVD

Sparse SVD is a variant of traditional SVD that can handle sparse data. It is often used in text mining and natural language processing, where the data is represented as a sparse matrix. The goal of sparse SVD is to decompose the data into a low-rank component and a sparse component, while preserving the original sparsity pattern.

The mathematical formulation of sparse SVD involves solving the following optimization problem:

$$\begin{equation}
\min_{L,S} ||L||_* + \lambda ||S||_1 \quad \text{subject to} \quad X = L + S
\end{equation}$$

where $X$ is the data matrix, $L$ is the low-rank component, $S$ is the sparse component, $||.||_*$ is the nuclear norm (sum of singular values), $||.||_1$ is the l1 norm (sum of absolute values), and $\lambda$ is a regularization parameter that controls the trade-off between low-rankness and sparsity.

To solve this optimization problem, various algorithms have been proposed, including Principal Component Pursuit (PCP), Accelerated Proximal Gradient (APG), and Alternating Direction Method of Multipliers (ADMM). These algorithms are iterative and require solving sub-problems at each iteration.

## Kernel SVD

Kernel SVD is a variant of traditional SVD that can handle nonlinear data. It is often used in machine learning, where the data is represented as a kernel matrix. The goal of kernel SVD is to decompose the data into a low-dimensional linear subspace and a nonlinear component, while preserving the original kernel similarity structure.

The mathematical formulation of kernel SVD involves solving the following optimization problem:

$$\begin{equation}
\min_{L,S} ||L||_* + \lambda ||S||_1 \quad \text{subject to} \quad K = LL^T + S
\end{equation}$$


where $K$ is the kernel matrix, $L$ is the low-dimensional subspace, $S$ is the nonlinear component, $||.||_*$ is the nuclear norm (sum of singular values), $||.||_1$ is the l1 norm (sum of absolute values), and $\lambda$ is a regularization parameter that controls the trade-off between low-rankness and sparsity.

To solve this optimization problem, various algorithms have been proposed, including Kernel PCA, Kernel Ridge Regression, and Kernel Ridge Regression with Iterative Hard Thresholding (KRR-IHT).




